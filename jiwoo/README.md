# LLM을 활용한 실전 AI 애플리케이션 개발 | 1부 LLM의 기초 뼈대 세우기

# 1. LLM 지도

## 1.1 딥러닝과 언어 모델링

**딥러닝**: 머신러닝의 한 분야이며 인간의 두뇌에 영감을 받아 만들어진 신경망

**언어 모델**: 다음에 올 단어를 예측하는 모델

**LLM**: 딥러닝 기반의 언어 모델

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝

> 딥러닝과 머신러닝의 차이점
- 머신러닝: 데이터의 특징을 연구자 혹은 개발자가 직접 찾고 모델에 입력함
- 딥러닝: 모델이 스스로 데이터의 특징을 찾음

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

**임베딩**: 데이터의 의미와 특징을 포착해 여러 개의 숫자 집합으로 표현한 것

- 임베딩이 필요한 이유: 컴퓨터는 숫자만 처리할 수 있으므로 데이터의 의미를 딥러닝 모델이 이해할 수 있게 함
- 임베딩의 장점: 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있게 함

> 임베딩의 활용
- 검색 및 추천
- 클러스터링 및 분류
- 이상치 탐지

### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

**언어 모델링**: 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식
> **전이학습**: 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
- **사전 학습**: 대량의 데이터로 모델을 학습시키는 과정, 자연어 처리 분야에서 언어 모델링이 사용됨
- **미세 조정**: 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 과정
    - 다운스트림: 미세 조정해 풀고자 하는 과제

## 1.2 언어 모델이 챗GPT가 되기까지

### 1.2.1 RNN에서 트랜스포머 아키텍처로

**RNN**: 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축하는 순환 신경망

> RNN의 장단점
- 장점
  - 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용
  - 다음 단어를 예측할 때, 이미 계산된 잠재 상태와 입력 단어만 있으면 되므로 다음 단어를 빠르게 생성
- 단점
  - 먼저 입력된 단어의 의미가 희석
  - 입력이 길어지는 경우, 의미를 충분히 담지 못함

**트랜스포머 아키텍처**: 맥락 데이터를 그대로 모두 활용하여 다음 단어를 예측, RNN의 문제를 해결

> 트랜스포머 아키텍처의 장단점
- 장점
  - **어텐션 연산**을 사용하여 맥락을 압축하지 않고 모두 활용하므로 고성능
  - 병렬 처리를 통해 학습 속도를 높일 수 있음
- 단점
  - 메모리 사용량 증가
  - 입력이 길어지면 예측에 걸리는 시간도 증가

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계

- 모델이 커지면 모델의 성능이 높아지나 학습 데이터의 크기가 최대 모델 크기의 상한

### 1.2.3 챗GPT의 등장

> GPT-3에 비해 챗GPT의 발전된 점
- 지도 미세 조정: 정렬을 위해 온어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것
  - 지시 데이터셋: 사용자의 요청 혹은 지시와 그에 대한 적절한 응답을 정리한 데이터셋
- RLHF: 사람의 피드백을 활용한 강화 학습
  - 선호 데이터셋: 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋
  - 리워드 모델: 선호 데이터셋으로 LLM의 답변을 평가하는 모델, LLM이 더 높은 점수를 받도록 추가 학습

## 1.3 LLM 애플리케이션의 시대가 열리다

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM

- LLM의 다재다능함으로 인해 생성 모델과 이해 모델을 필요로 했던 기존 모델과 달리, 하나의 모델로 작업 가능

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기

- sLLM: 오픈 소스 LLM을 이용해 원하는 데이터를 추가 학습시켜, 크기가 작으면서 특정 작업을 높은 성능으로 수행하는 모델

### 1.3.3 더 효율적인 학습과 추론을 위한 기술

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 기술(RAG) 기술

- 환각 현상: LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상
- RAG: LLM이 답변할 때 필요한 정보를 미리 추가시켜 환각 현상을 감소시키는 기술

## 1.4 LLM의 미래: 인식과 행동의 확장

> LLM의 미래
- 멀티 모달 LLM: 더 다양한 형태의 데이터를 입력 혹은 출력하는 LLM
- 에이전트: LLM이 텍스트 생성 능력을 사용하여 계획을 세우거나 의사결정을 내리고 필요한 행동까지 수행하는 것
- 새로운 아키텍처: 트랜스포머 아키텍처보다 더 효율적인 아키텍처

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기

## 2.1 트랜스포머 아키텍처란

> RNN의 단점
- 병렬적으로 처리하지 못하기 때문에 학습 속도가 느림
- 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되어 성능이 떨어짐
- 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실 혹은 그레이디언트 증폭이 발생하여 학습이 불안정

> 해결책: 트랜스포머 아키텍처
- **셀프 어텐션**: 입력된 문장의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정
- 확장성: 더 깊은 모델을 만들어도 학습이 안정, 확장에 용이
- 효율성: 학습할 때 병렬 연산이 가능하므로 학습 시간 단축
- 더 긴 입력 처리: 입력이 길어져서 성능 변화 거의 없음

## 2.2 텍스트를 임베딩으로 변환하기

> 텍스트를 임베딩으로 변환하는 과정
1. **토큰화**: 텍스트를 적절히 잘라 숫자형 ID 부여
2. 토큰 임베딩: 토큰 아이디를 토큰 임베딩 층을 통해 토큰 임베딩으로 변환
3. 위치 임베딩: 위치 인코딩 층을 통해 토큰의 위치 정보가 담긴 위치 임베딩을 추가하여 최종 임베딩 생성

### 2.2.1 토큰화

> 큰 단위로 토큰화 (단어)
- 장점: 텍스트의 의미가 잘 유지
- 단점: 사전의 크기가 커짐, OOV 문제 발생

> 작은 단위로 토큰화 (자모)
- 장점: 사전의 크기가 작고, OOV 문제 발생 감소
- 단점: 텍스트의 의미가 유지되지 못함

서브워드 토큰화: 자주 나오는 단어는 단어 단위, 가끔 나오는 단어는 더 작은 단위로 토큰화

### 2.2.2 토큰 임베딩 변환하기

Pytorch의 nn.Embedding 클래스를 사용하여 토큰 ID를 토큰 임베딩으로 변환

딥러닝 모델은 학습의 유용성을 위해 데이터(토큰)의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습

### 2.2.3 위치 인코딩

위치 인코딩: 트랜스포머 아키텍처는 순차적 방식이 아니므로 입력의 순서 정보가 사라지므로 위치 정보를 추가

**절대적 위치 인코딩**: 입력 토큰의 위치에 따라 고정된 임베딩을 추가

**상대적 위치 인코딩**: 토큰과 토큰 사이의 상대적인 위치 정보도 활용

토큰 임베딩과 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩을 생성

## 2.3 어텐션 이해하기

### 2.3.1 사람이 글을 읽는 방법과 어텐션

사람은 특정 단어와 다른 단어 사이의 관계를 통해 맥락을 유추

어텐션 연산: 단어와 단어 사이의 관계를 계산하여 관련이 깊은 단어와 관련이 적은 단어를 구분하고, 각 단어의 맥락 반영 비율을 달리 함

### 2.3.2 쿼리, 키, 값 이해하기

**쿼리**: 문장 내의 특정 토큰

**키**: 문장 내의 다른 토큰

**값**: 각 키의 임베딩

> 임베딩을 직접 활용해 관련도를 계산하는 방식의 문제점
- 같은 단어는 관련도가 크게 계산되어 주변 맥락을 충분히 반영하지 못함
- 문법과 같이 간접적인 관련성은 반영되기 어려움

> 트랜스포머 아키텍처에서의 해결책
- 각 토큰 임베딩에 가중치 도입
- 값에도 가중치 도입
- 가중치로 변환한 값을 가중합하면 특정 토큰을 재해석한 결과 도출

### 2.3.3 코드로 보는 어텐션

> 스케일 점곱 방식
1. 쿼리와 키를 곱하기
2. 이를 임베딩 차원 수의 제곱근으로 나누기 (분산 커지는 것을 방지)
3. 쿼리와 키를 곱한 값의 합이 1이 되도록 softmax를 취한 것을 가중치로 설정
4. 가중치와 값을 곱해 출력을 반환
5. 이를 더해서 주변 단어의 맥락을 반영한 하나의 값 임베딩을 산출

### 2.3.4 멀티 헤드 어텐션

멀티 헤드 어텐션: 여러 어텐션 연산을 동시에 적용하여 성능을 높인 것

헤드의 수만큼 각각의 어텐션 연산을 수행

## 2.4 정규화와 피드 포워드 층

### 2.4.1 층 정규화 이해하기

> 정규화의 필요성
- 정규화를 하지 않으면 각 입력 변수의 범위와 분포가 달라져 중요성이 적절히 반영되지 못함

norm_x = (x - 평균) / 표준편차

> 배치 정규화: 모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행
- 자연어 처리 분야에서는 입력으로 들어가는 문장의 길이가 다양하므로 정규화에 포함되는 데이터의 수가 각자 달라 정규화를 보장할 수 없음

> 층 정규화: 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행
- 정규화에 포함되는 데이터의 수가 각자 다르더라도 정규화에는 지장 없음

### 2.4.2 피드 포워드 층

**피드 포워드 층**: 데이터의 특징을 학습하는 완전 연결 층, 입력 텍스트 전체를 이해하는 역할

> 피드 포워드 층의 구성
- 선형 층
- 드롭아웃 층
- 층 정규화
- 활성 함수

## 2.5 인코더

> 인코더의 구조
- 층 정규화
- 멀티 헤드 어텐션
- 층 정규화
- 피드 포워드 층
- 위의 구조를 반복 수행

**잔차 연결**: 안정적인 학습을 위해 입력을 그대로 다시 더해주는 과정

## 2.6 디코더

> 인코더와 디코더의 차이
- 마스크 멀티 헤드 어텐션
  - 어텐션을 그대로 활용할 경우, 미래에 작성할 텍스트를 미리 확인하는 문제 발생
  - 특정 시점 이전에 생성된 토큰만 확인할 수 있도록 마스크를 추가
- 크로스 어텐션
  - 쿼리는 인코더의 잠재 상태를 사용
  - 키와 값은 인코더의 결과를 사용

## 2.7 BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
> 트랜스포머 아키텍처를 활용한 모델의 구분
- 인코더만 활용한 자연어 이해 특화 그룹
- 디코더만 활용한 자연어 생성 특화 그룹
- 인코더와 디코더 모두 활용한 그룹

### 2.7.1 인코더를 활용한 BERT

사전 학습으로 입력 토큰의 일부를 마스크 토큰으로 대체하여 그것을 맞추는 마스크 언어 모델링 과제를 활용

양방향 문맥을 이해할 수 있다는 특징

### 2.7.2 디코더를 활용한 GPT

사전 학습으로 입력 토큰이나 이전까지 생성한 토큰만을 사용하여 다음 토큰을 예측하는 인과적 언어 모델링 과제를 활용

단방향 방식

### 2.7.3 인코더와 디코더를 무도 사용하는 BART, T5

> BART
- 사전 학습으로 노이즈가 추가된 입력 테스트를 사용하여 노이즈가 제거된 결과를 생성하는 과제를 활용
- 양방향 인코더, 단방향 디코더

> T5
- 입력의 시작에 과제의 종류를 지정해서 특정 작업을 수행하도록 학습

## 2.8 주요 사전 학습 메커니즘

### 2.8.1 인과적 언어 모델링

문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식

디코더 모델을 학습시키는 과제

단방향 예측

### 2.8.2 마스크 언어 모델링

입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 방식

인코더 모델을 학습시키는 과제

양방향 예측

# 3. 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리

## 3.1 허깅페이스 트랜스포머란

**허깅페이스 트랜스포머**: 다양한 트랜스포머 모델을 통일된 인터페이스로 사용할 수 있도록 지원하는 오픈소스 라이브러리

transformers 라이브러리: 트랜스포머 모델과 토크나이저를 활용할 때 사용하는 라이브러리

datasets 라이브러리: 데이터셋을 공개하고 쉽게 사용할 수 있도록 지원하는 라이브러리

## 3.2 허깅페이스 허브 탐색하기

### 3.2.1 모델 허브

다양한 작업 분야의 모델이 분류되어 있음

### 3.2.2 데이터셋 허브

다양한 데이터셋이 분류되어 있음

### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스

사용자가 자신의 모델 데모를 공개할 수 있는 기능

## 3.3 허깅페이스 라이브러리 사용법 익히기

### 3.3.1 모델 활용하기

> 모델
- 바디
- 헤드: 사용하려는 작업에 따라 같은 바디이지만 서로 다른 헤드를 사용할 수 있음

> 모델의 바디만 불러오기
- AutoModel 클래스 이용
- 모델의 config.json 파일에 의해 AutoModel 클래스가 특정 모델 파악 가능

> 모델의 바디와 헤드 함께 불러오기
- AutoModelfor~~ 클래스 이용
- 모델의 config.json 파일에 의해 AutoModelfor~~ 클래스가 헤드의 기능 파악 가능

> 헤드가 함께 있는 모델의 바디만 불러오기
- 모델의 바디는 사전 학습된 파라미터
- 모델의 헤드는 랜덤으로 초기화, 추가 학습이 필요

### 3.3.2 토크나이저 활용하기

토크나이저: 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환하는 역할

허깅페이스 라이브러리에서 모델과 토크나이저를 함께 불러올 경우, 동일한 모델 아이디로 맞추어야 함

AutoTokenizer 클래스 이용

토크나이저에 대한 정보는 모델의 tokenizer_config.json 파일과 tokenizer.json 파일에 저장

### 3.3.3 데이터셋 활용하기

load_dataset 함수를 이용하여 데이터셋을 불러옴

## 3.4 모델 학습시키기

### 3.4.1 데이터 준비

### 3.4.2 트레이너 API를 사용해 학습하기

트레이너 API: 허깅페이스에서 학습에 필요한 다양한 기능을 추상화를 통해 학습 인자만으로 쉽게 활용할 수 있는 기능

### 3.4.3 트레이너 API를 사용하지 않고 학습하기

> 트레이너 API의 장단점
- 장점: 추상화를 통해 간편하게 사용할 수 있음
- 단점: 내부 동작을 파악하기 어려움

### 3.4.4 학습한 모델 업로드하기

huggingface_hub 라이브러리 사용

트레이너 API를 사용한 경우, push_to_hub() 메서드 사용하면 모델과 토크나이저를 함께 업로드 됨

트레이너 API를 사용하지 않은 경우, push_to_hub() 메서드 사용하면 모델과 토크나이저를 각각 업로드해야 함

## 3.5 모델 추론하기

### 3.5.1 파이프라인을 활용한 추론

파이프라인: 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 기능

파이프라인의 입력: 작업 종류, 모델, 설정

파이프라인의 반환: 예측 확률이 가장 높은 레이블과 그 확률

### 3.5.2 직접 추론하기

직접 모델과 토크나이저를 불러와 파이프라인과 유사하게 추론을 구현

# 4. 말 잘 듣는 모델 만들기

## 4.1 코딩 테스트 통과하기: 사전 학습과 지도 미세 조정

### 4.1.1 코딩 개념 익히기: LLM의 사전 학습

LLM은 인터넷 상에 있는 다양한, 대용량의 텍스트 데이터를 이용해 언어 모델링 방식으로 사전 학습

학습 데이터의 일부를 입력하고, 다음에 오는 정답 토큰을 맞추는 방식으로 언어 모델을 학습

### 4.1.2 연습문제 풀어보기: 지도 미세 조정

**지도 미세 조정**: 사용자의 요청에 적절히 응답하기 위해서 요청의 형식을 적절히 해석하고 응답의 형태를 적절히 작성하며 요청과 응답이 잘 연결되도록 추가 학습하는 과정

**지시 데이터셋**: 지도 미세 조정에 사용하는 데이터셋, 사용자의 요청을 형식에 맞춰 작성하고 그에 대한 적절한 응답을 하는 형태

지도 미세 조정은 사전 학습과 학습하는 데이터셋의 차이가 있을 뿐, 학습하는 방식은 사전 학습의 방식인 인과적 언어 모델링과 동일

### 4.1.3 좋은 지시 데이터셋이 갖춰야 할 조건

1. 작은 규모라도 상관 없음
2. 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 모델의 답변 품질이 높아짐
3. 모델의 학습에 도움이 되는, 교육적 가치가 높은 데이터를 선별
4. 고품질 데이터를 학습 데이터에 추가하면 성능의 비약적 상승이 발생

## 4.2 채점 모델로 코드 가독성 높이기

### 4.2.1 선호 데이터셋을 사용한 채점 모델 만들기

1. 선호 데이터셋을 구축하여 채점 모델 학습
2. 선호 데이터셋을 사용해 채점 모델이 지도 미세 조정을 마친 LLM이 생성한 여러 응답의 선호 순위를 학습

### 4.2.2 강화 학습: 높은 코드 가독성 점수를 향해

**강화 학습**: 에이전트의 **행동**에 따라 환경의 **상태**가 변하고 그에 따른 보상이 주어짐, 에이전트는 가능한 많은 보상을 받기 위해 행동을 수정

토큰 생성을 하나의 행동으로 간주

언어 모델은 생성한 문장의 점수가 높아지는 방향으로 학습

### 4.2.3 PPO: 보상 해킹 피하기

**보상 해킹**: 점수를 높게 받는 것에만 집중

**PPO**: 지도 미세 조정 모델을 기준으로, 가까운 범위 내의 학습 모델(참고 모델) 중 가장 높은 점수를 받은 모델을 찾는 강화 학습 방법

### 4.2.4 RLHF: 멋지지만 피할 수 있다면...

> RLHF의 문제점
- 리워드 모델의 성능이 좋지 않으면 LLM이 제대로 학습되지 못함
- 참고 모델, 학습 모델, 리워드 모델이 필요하므로 더 많은 리소스 필요
- 하이퍼파라미터에 민감하고 학습이 불안정

## 4.3 강화 학습이 꼭 필요할까?

### 4.3.1 기각 샘플링: 단순히 가장 점수가 높은 데이터를 사용한다면?

**기각 샘플링**: 지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고 그 중 리워드 모델이 가장 높은 점수를 부여한 응답만 모아 다시 지도 미세 조정을 수행

### 4.3.2 DPO: 선호 데이터셋을 직접 학습하기

**DPO**: 리워드 모델을 거치지 않고 선호 데이터셋을 언어 모델에 직접 학습

### 4.3.3 DPO를 사용해 학습한 모델들

> DPO를 사용한 모델
- 제퍼-7B-베타: 지식 증류를 활용
- 뉴럴-챗-7B: 더 성능이 뛰어난 모델의 생성 결과를 선호 데이터로 선택하여 학습 과정 단순화
- 튈루-2: 파라미터가 매우 많은 모델이 학습되며 DPO의 확장성을 어느 정도 확인

# LLM을 활용한 실전 AI 애플리케이션 개발 | 2부 LLM 길들이기

# 5. GPU의 효율적인 학습

## 5.1 GPU에 올라가는 데이터 살펴보기

OOM 에러: 한정된 GPU 메모리에 데이터가 가득 차 더 이상 새로운 데이터를 추가하지 못해 발생하는 에러

### 5.1.1 딥러닝 모델의 데이터 타입

> 딥러닝 모델의 데이터 타입
- fp32: 지수 8비트, 가수 23비트
- fp16: 지수 5비트, 가수 10비트 (표현 가능한 수의 범위가 좁음)
- bf16: 지수 8비트, 가수 7비트 (fp16의 문제를 해소하기 위해 지수의 비트 수를 fp32와 동일한 8비트로 증가)

> 딥러닝 모델의 GPU 메모리 사용량
- 파라미터 수 * 파라미터 당 비트 혹은 바이트 수
- ex) 파라미터 10억 개, fp16 형식: 10억 * 2 byte = 20억 byte

### 5.1.2 양자화로 모델 용량 줄이기

**양자화**: 더 적은 비트로 모델을 표현하는 방식

> 양자화 방식
- 서로 다른 두 데이터 형식의 최댓값과 최솟값을 각각 대응시키는 방식
  - 사용하는 데이터가 없이 낭비되는 문제
- 데이터의 절대 최댓값을 찾아 데이터의 최댓값 범위로 양자화하는 방식
  - 이상치가 있는 경우 취약
- 전체 데이터를 K개로 묶은 블록 별로 양자화를 수행
  - 이상치가 영향을 미치는 데이터가 줄어들어 대부분의 데이터가 양자화 수행
- 퀀타일 방식
  - 입력 데이터를 크기 순으로 등수를 매겨 작은 데이터 타입에 동일한 개수의 큰 데이터 타입의 값이 대응되도록 배치
  - 계산량이 많고 별도의 메모리를 사용해야 한다는 단점

### 5.1.3 GPU 메모리 분해하기

> GPU 메모리에 저장되는 데이터
- 모델 파라미터
- 그레이디언트
- 옵티마이저 상태
- 순전파 상태

## 5.2 단일 GPU 효율적으로 활용하기

그레이디언트 누적: 모델을 학습시킬 때 각 배치마다 모델을 업데이트하지 않고 여러 배치의 학습 데이터를 연산한 후 모델을 업데이트하여 더 큰 배치 크기를 사용하는 것과 같은 효과를 내는 방법

그레이디언트 체크포인팅: 순전파의 계산 결과를 모두 저장하지 않고 일부만 저장하여 학습 중 GPU 메모리 사용량을 줄이는 학습 방법

### 5.2.1 그레이디언트 누적

장점: 손실을 그레이디언트 누적 횟수로 나눠서 역전파를 수행하고 각 스텝마다 모델을 업데이트하여 마치 배치 크기가 그레이디언트 누적 횟수배만큼 증가한 것과 동일한 효과를 창출

단점: 추가적인 순전파 및 역전파 연산을 수행해야 하므로 학습 시간이 증가

### 5.2.2 그레이디언트 체크포인팅

1. 역전파를 진행하면서 메모리 절약을 위해 사용이 끝난 데이터는 삭제
2. 역전파를 계산할 때 필요한 최소 데이터만 저장하고 나머지는 필요할 때 다시 계산

절충안(**그레이디언트 체크포인팅**): 중간중간 값을 저장해서 메모리 사용량 줄이고 필요한 경우 체크포인트부터 다시 계산하여 순전파 계산량도 감소

## 5.3 분산 학습과 ZeRO

**분산 학습**: 2개 이상의 GPU를 사용하여 모델을 학습시키는 방법

### 5.3.1 분산 학습

> 분산 학습의 목적
- **데이터 병렬화**: 여러 GPU에 각각 모델을 올리고 병렬 처리하여 학습 속도 증가
  - 동일한 모델을 여러 GPU에 올리므로 데이터 낭비 발생
- **모델 병렬화**: 하나의 GPU에 올리기 힘든, 큰 모델의 경우 모델을 여러 GPU에 나눠서 올리는 방식
  - **파이프라인 병렬화**: 딥러닝 모델의 층 별로 나눠 GPU에 올리는 방식
  - **텐서 병렬화**: 한 층의 모델도 나눠서 GPU에 올리는 방식

### 5.3.2 데이터 병렬화에서 중복 저장 줄이기(ZeRO)

하나의 모델을 나눠 각 GPU에 올리고 각 GPU는 자신의 모델 부분의 연산만 수행하여 메모리를 효율적으로 사용

## 5.4 효율적인 학습 방법(PEFT): LoRA

PEFT 방법: 일부 파라미터만 학습

LoRA 학습 방식: 모델에 일부 파라미터를 추가하고 그 부분만 학습하는 방식

### 5.4.1 모델 파라미터의 일부만 재구성해 학습하는 LoRA

(d * d 차원)의 파라미터를 (d * r 차원), (r * d 차원)의 파라미터로 재구성하여 (d * d) 개의 파라미터를 학습해야 할 것을 (2 * d * r) 개의 파라미터만 학습하면 되므로 더 적은 GPU 메모리 사용 (d > r)

파라미터의 수가 줄어들면 모델 업데이트에 사용하는 옵티마이저 상태의 데이터가 줄어드므로 그레이디언트와 옵티마이저 상태를 저장하는 데에 필요한 메모리가 줄어듦

### 5.4.2 LoRA 설정 살펴보기

1. 5.4.1의 r을 어느 정도로 할 것인가?
   - r을 너무 작게 하면 모델이 학습할 수 있는 용량이 줄어드는 문제 발생
2. 추가한 파라미터를 기존 파아미터에 얼마나 반영할 것인가?
   - 재구성한 파라미터들을 (알파/r)만큼 비중으로 기존 파라미터에 더해줌
   - 알파가 커질수록 재구성한 파라미터가 더 큰 비중으로 기존 파라미터에 영향을 줌
3. 모델에 있는 많은 파라미터 중 어떤 파라미터를 재구성할 것인가?
   - 선형 연산의 가중치를 재구성

### 5.4.3 코드로 LoRA 학습 사용하기

허깅페이스는 peft 라이브러리를 통해 LoRA와 같은 효율적인 학습 방식을 쉽게 활용할 수 있는 기능 제공

peft 라이브러리의 LoraConfig 클래스를 사용하면 LoRA 설정 정의 가능

## 5.5 효율적인 학습 방법(PEFT): QLoRA

QLoRA: LoRA에 양자화를 추가해 메모리 효율성을 한 번 더 높인 학습 방법

페이지 옵티마이저: 학습 도중 OOM 에러가 발생하지 않고 안정적으로 진행할 수 있는 기능

### 5.5.1 4비트 양자화와 2차 양자화

NF4: 학습된 모델의 파라미터가 정규 분포를 따른다고 가정하고 4비트 부동소수점 데이터 형식으로 양자화 수행

**2차 양자화**: 64개의 파라미터를 하나의 블록으로 묶어 양자화를 수행하고, 다시 256개의 양자화 상수를 하나의 블록으로 묶어 8비트 양자화 수행

### 5.5.2 페이지 옵티마이저

엔비디아의 통합 메모리를 통해 GPU가 CPU 메모리를 공유하는 것

**페이징**: CPU 메모리가 가득 차면 일부 데이터를 디스크로 옮기고 필요할 때 다시 CPU 메모리로 불러오는 과정

### 5.5.3 코드로 QLoRA 모델 활용하기

bitsandbytes 라이브러리: 4비트 양자화를 간단히 수행

BitsAndBytesConfig 클래스를 이용해 양자화 설정 정의

# 6. sLLM 학습하기

SQL: 관계형 데이터베이스에서 데이터를 생성, 조회, 업데이트, 삭재하기 위해 사용하는 언어

Text2SQL: 사용자의 요청을 자연어로 작성하면 LLM이 요청에 맞는 SQL을 생성하는 작업

## 6.1 Text2SQL 데이터셋

### 6.1.1 대표적인 Text2SQL 데이터셋

> 대표적인 Text2SQL 데이터셋
- WikiSQL: 하나의 테이블만 사용, 비교적 쉬운 SQL 문
- Spider: 비교적 복잡한 SQL 문

> SQL 생성을 위해 필요한 데이터
- 데이터 베이스 정보: 테이블 이름, 컬럼 이름, 컬럼 형식
- 요청사항: 어떤 데이터를 추출하고 싶은지
- 정답 SQL 데이터

### 6.1.2 한국어 데이터셋

자연어 기반 질의 검색 생성 데이터

### 6.1.3 합성 데이터 활용

## 6.2 성능 평가 파이프라인 준비하기

성능 평가: 모신러닝 모델을 학습시킬 때 학습이 잘 진행된 것인지 판단하는 과정

### 6.2.1 Text2SQL 평가 방식

> Text2SQL 평가 방식
- EM 방식: 생성한 SQL이 문자열 그대로 동일한지 확인하는 방식
  - SQL 쿼리가 의미상으로 동일하나 문자열이 동일하지 않아 다르다고 판단하는 문제
- 실행 정확도 방식: 쿼리를 수행할 수 있는 데이터베이스를 만들고 프로그래밍 방식으로 SQL 쿼리를 수행해 정답과 일치하는지 확인하는 방식
  - 쿼리를 실행할 수 있는 데이터베이스를 추가로 준비해야 함
 
> GPT를 활용한 성능 평가 파이프라인을 준비하기 위해 필요한 것
- 평가 데이터셋 구축
- 프롬프트
- 프롬프트와 요청을 빠르게 수행할 수 있는 코드

### 6.2.2 평가 데이터셋 구축

### 6.2.3 SQL 생성 프롬프트

프롬프트: LLM이 SQL을 생성하도록 하기 위해서는 지시사항과 데이터를 포함한 것

쿼리를 입력해 정답 SQL 문이 추가된 형태는 학습에 사용하는 형태

쿼리를 입력하지 않아 정답 SQL이 비어 있는 형태는 SQL 생성에 사용하는 형태

### 6.2.4 GPT-4 평가 프롬프트와 코드 준비

평가 데이터셋이 많다면, OpenAI가 제공하는 코드를 활용하여 요청 보낼 내용을 저장한 jsonl 파일을 읽어 순차적으로 요청을 전송

## 6.3 실습: 미세 조정 수행하기

### 6.3.1 기초 모델 평가하기

1. 예시 데이터를 입력해 기초 모델이 생성한 결과를 확인
2. 평가 데이터셋에 대한 SQL 생성을 수행하고 GPT-4를 사용해 평가

### 6.3.2 미세 조정 수행

1. autotrain-advanced 라이브러리를 사용하여 준비한 학습 데이터로 미세 조정 수행
2. 앞서 사용한 예ㅒ시 데이터에 대해 다시 SQL을 생성하고 기초 모델의 출력과 비교

### 6.3.3 학습 데이터 정제와 미세 조정

정제된 데이터셋을 사용할 경우, 더 적은 데이터셋 크기로도 정제 전의 데이터셋으로 학습했을 때와 동일한 성능 달성

### 6.3.4 기초 모델 변경

### 6.3.5 모델 성능 비교
