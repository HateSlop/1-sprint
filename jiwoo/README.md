# LLM을 활용한 실전 AI 애플리케이션 개발 | 1부 LLM의 기초 뼈대 세우기

# 1. LLM 지도

## 1.1 딥러닝과 언어 모델링

**딥러닝**: 머신러닝의 한 분야이며 인간의 두뇌에 영감을 받아 만들어진 신경망

**언어 모델**: 다음에 올 단어를 예측하는 모델

**LLM**: 딥러닝 기반의 언어 모델

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝

> 딥러닝과 머신러닝의 차이점

- 머신러닝: 데이터의 특징을 연구자 혹은 개발자가 직접 찾고 모델에 입력함
- 딥러닝: 모델이 스스로 데이터의 특징을 찾음

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

**임베딩**: 데이터의 의미와 특징을 포착해 여러 개의 숫자 집합으로 표현한 것

- 임베딩이 필요한 이유: 컴퓨터는 숫자만 처리할 수 있으므로 데이터의 의미를 딥러닝 모델이 이해할 수 있게 함
- 임베딩의 장점: 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있게 함
> 임베딩의 활용
- 검색 및 추천
- 클러스터링 및 분류
- 이상치 탐지

### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

**언어 모델링**: 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식
> **전이학습**: 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식

- **사전 학습**: 대량의 데이터로 모델을 학습시키는 과정, 자연어 처리 분야에서 언어 모델링이 사용됨
- **미세 조정**: 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 과정
    - 다운스트림: 미세 조정해 풀고자 하는 과제

## 1.2 언어 모델이 챗GPT가 되기까지

### 1.2.1 RNN에서 트랜스포머 아키텍처로

**RNN**: 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축하는 순환 신경망

> RNN의 장단점
- 장점
  - 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용
  - 다음 단어를 예측할 때, 이미 계산된 잠재 상태와 입력 단어만 있으면 되므로 다음 단어를 빠르게 생성
- 단점
  - 먼저 입력된 단어의 의미가 희석
  - 입력이 길어지는 경우, 의미를 충분히 담지 못함

**트랜스포머 아키텍처**: 맥락 데이터를 그대로 모두 활용하여 다음 단어를 예측, RNN의 문제를 해결

> 트랜스포머 아키텍처의 장단점
- 장점
  - **어텐션 연산**을 사용하여 맥락을 압축하지 않고 모두 활용하므로 고성능
  - 병렬 처리를 통해 학습 속도를 높일 수 있음
- 단점
  - 메모리 사용량 증가
  - 입력이 길어지면 예측에 걸리는 시간도 증가

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계

- 모델이 커지면 모델의 성능이 높아지나 학습 데이터의 크기가 최대 모델 크기의 상한

### 1.2.3 챗GPT의 등장

> GPT-3에 비해 챗GPT의 발전된 점
- 지도 미세 조정: 정렬을 위해 온어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것
  - 지시 데이터셋: 사용자의 요청 혹은 지시와 그에 대한 적절한 응답을 정리한 데이터셋
- RLHF: 사람의 피드백을 활용한 강화 학습
  - 선호 데이터셋: 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋
  - 리워드 모델: 선호 데이터셋으로 LLM의 답변을 평가하는 모델, LLM이 더 높은 점수를 받도록 추가 학습

## 1.3 LLM 애플리케이션의 시대가 열리다

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기

### 1.3.3 더 효율적인 학습과 추론을 위한 기술

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 기술(RAG) 기술

## 1.4 LLM의 미래: 인색과 행동의 확장

## 1.5 정리

