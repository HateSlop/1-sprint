# LLM을 활용한 실전 AI 애플리케이션 개발 | 1부 LLM의 기초 뼈대 세우기

# 1. LLM 지도

## 1.1 딥러닝과 언어 모델링

**딥러닝**: 머신러닝의 한 분야이며 인간의 두뇌에 영감을 받아 만들어진 신경망

**언어 모델**: 다음에 올 단어를 예측하는 모델

**LLM**: 딥러닝 기반의 언어 모델

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝

> 딥러닝과 머신러닝의 차이점
- 머신러닝: 데이터의 특징을 연구자 혹은 개발자가 직접 찾고 모델에 입력함
- 딥러닝: 모델이 스스로 데이터의 특징을 찾음

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

**임베딩**: 데이터의 의미와 특징을 포착해 여러 개의 숫자 집합으로 표현한 것

- 임베딩이 필요한 이유: 컴퓨터는 숫자만 처리할 수 있으므로 데이터의 의미를 딥러닝 모델이 이해할 수 있게 함
- 임베딩의 장점: 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있게 함

> 임베딩의 활용
- 검색 및 추천
- 클러스터링 및 분류
- 이상치 탐지

### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

**언어 모델링**: 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식
> **전이학습**: 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
- **사전 학습**: 대량의 데이터로 모델을 학습시키는 과정, 자연어 처리 분야에서 언어 모델링이 사용됨
- **미세 조정**: 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 과정
    - 다운스트림: 미세 조정해 풀고자 하는 과제

## 1.2 언어 모델이 챗GPT가 되기까지

### 1.2.1 RNN에서 트랜스포머 아키텍처로

**RNN**: 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축하는 순환 신경망

> RNN의 장단점
- 장점
  - 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용
  - 다음 단어를 예측할 때, 이미 계산된 잠재 상태와 입력 단어만 있으면 되므로 다음 단어를 빠르게 생성
- 단점
  - 먼저 입력된 단어의 의미가 희석
  - 입력이 길어지는 경우, 의미를 충분히 담지 못함

**트랜스포머 아키텍처**: 맥락 데이터를 그대로 모두 활용하여 다음 단어를 예측, RNN의 문제를 해결

> 트랜스포머 아키텍처의 장단점
- 장점
  - **어텐션 연산**을 사용하여 맥락을 압축하지 않고 모두 활용하므로 고성능
  - 병렬 처리를 통해 학습 속도를 높일 수 있음
- 단점
  - 메모리 사용량 증가
  - 입력이 길어지면 예측에 걸리는 시간도 증가

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계

- 모델이 커지면 모델의 성능이 높아지나 학습 데이터의 크기가 최대 모델 크기의 상한

### 1.2.3 챗GPT의 등장

> GPT-3에 비해 챗GPT의 발전된 점
- 지도 미세 조정: 정렬을 위해 온어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것
  - 지시 데이터셋: 사용자의 요청 혹은 지시와 그에 대한 적절한 응답을 정리한 데이터셋
- RLHF: 사람의 피드백을 활용한 강화 학습
  - 선호 데이터셋: 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋
  - 리워드 모델: 선호 데이터셋으로 LLM의 답변을 평가하는 모델, LLM이 더 높은 점수를 받도록 추가 학습

## 1.3 LLM 애플리케이션의 시대가 열리다

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM

- LLM의 다재다능함으로 인해 생성 모델과 이해 모델을 필요로 했던 기존 모델과 달리, 하나의 모델로 작업 가능

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기

- sLLM: 오픈 소스 LLM을 이용해 원하는 데이터를 추가 학습시켜, 크기가 작으면서 특정 작업을 높은 성능으로 수행하는 모델

### 1.3.3 더 효율적인 학습과 추론을 위한 기술

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 기술(RAG) 기술

- 환각 현상: LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상
- RAG: LLM이 답변할 때 필요한 정보를 미리 추가시켜 환각 현상을 감소시키는 기술

## 1.4 LLM의 미래: 인식과 행동의 확장

> LLM의 미래
- 멀티 모달 LLM: 더 다양한 형태의 데이터를 입력 혹은 출력하는 LLM
- 에이전트: LLM이 텍스트 생성 능력을 사용하여 계획을 세우거나 의사결정을 내리고 필요한 행동까지 수행하는 것
- 새로운 아키텍처: 트랜스포머 아키텍처보다 더 효율적인 아키텍처

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기

## 2.1 트랜스포머 아키텍처란

> RNN의 단점
- 병렬적으로 처리하지 못하기 때문에 학습 속도가 느림
- 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되어 성능이 떨어짐
- 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실 혹은 그레이디언트 증폭이 발생하여 학습이 불안정

> 해결책: 트랜스포머 아키텍처
- **셀프 어텐션**: 입력된 문장의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정
- 확장성: 더 깊은 모델을 만들어도 학습이 안정, 확장에 용이
- 효율성: 학습할 때 병렬 연산이 가능하므로 학습 시간 단축
- 더 긴 입력 처리: 입력이 길어져서 성능 변화 거의 없음

## 2.2 텍스트를 임베딩으로 변환하기

> 텍스트를 임베딩으로 변환하는 과정
1. **토큰화**: 텍스트를 적절히 잘라 숫자형 ID 부여
2. 토큰 임베딩: 토큰 아이디를 토큰 임베딩 층을 통해 토큰 임베딩으로 변환
3. 위치 임베딩: 위치 인코딩 층을 통해 토큰의 위치 정보가 담긴 위치 임베딩을 추가하여 최종 임베딩 생성

### 2.2.1 토큰화

> 큰 단위로 토큰화 (단어)
- 장점: 텍스트의 의미가 잘 유지
- 단점: 사전의 크기가 커짐, OOV 문제 발생

> 작은 단위로 토큰화 (자모)
- 장점: 사전의 크기가 작고, OOV 문제 발생 감소
- 단점: 텍스트의 의미가 유지되지 못함

서브워드 토큰화: 자주 나오는 단어는 단어 단위, 가끔 나오는 단어는 더 작은 단위로 토큰화

### 2.2.2 토큰 임베딩 변환하기

Pytorch의 nn.Embedding 클래스를 사용하여 토큰 ID를 토큰 임베딩으로 변환

딥러닝 모델은 학습의 유용성을 위해 데이터(토큰)의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습

### 2.2.3 위치 인코딩

위치 인코딩: 트랜스포머 아키텍처는 순차적 방식이 아니므로 입력의 순서 정보가 사라지므로 위치 정보를 추가

**절대적 위치 인코딩**: 입력 토큰의 위치에 따라 고정된 임베딩을 추가
**상대적 위치 인코딩**: 토큰과 토큰 사이의 상대적인 위치 정보도 활용

토큰 임베딩과 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩을 생성

## 2.3 어텐션 이해하기

### 2.3.1 사람이 글을 읽는 방법과 어텐션

사람은 특정 단어와 다른 단어 사이의 관계를 통해 맥락을 유추

어텐션 연산: 단어와 단어 사이의 관계를 계산하여 관련이 깊은 단어와 관련이 적은 단어를 구분하고, 각 단어의 맥락 반영 비율을 달리 함

### 2.3.2 쿼리, 키, 값 이해하기

**쿼리**: 문장 내의 특정 토큰

**키**: 문장 내의 다른 토큰

**값**: 각 키의 임베딩

### 2.3.3 코드로 보는 어텐션

### 2.3.4 멀티 헤드 어텐션

## 2.4 정규화와 피드 포워드 층

### 2.4.1 층 정규화 이해하기

### 2.4.2 피드 포워드 층

## 2.5 인코더

## 2.6 디코더

## 2.7 BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처

### 2.7.1 인코더를 활용한 BERT

### 2.7.2 디코더를 활용한 GPT

### 2.7.3 인코더와 디코더를 무도 사용하는 BART, T5

## 2.8 주요 사전 학습 메커니즘

### 2.8.1 인과적 언어 모델링

### 2.8.2 마스크 언어 모델링

