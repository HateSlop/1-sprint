# LLM을 활용한 실전 AI 애플리케이션 개발
# 1. LLM 지도
## 딥러닝과 언어 모델링
- 딥러닝: 인간의 두뇌에 영감을 받아 만들어진 신경망으로, 데이터의 패턴을 학습하는 머신러닝의 한 분야
- 언어모델: 다음에 올 단어를 예측하는 모델  
- 딥러닝과 머신러닝의 가장 큰 차이는, 데이터의 특징을 누가 뽑는가이다. 기존 머신러닝에서는 데이터의 특징을 연구자가 찾고 모델에 입력으로 넣었으면, 딥러닝에서는 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.
- 사진(그림 1.3)
- 임베딩: 데이터의 의미와 특징을 포착해 숫자로 표현한 것
- 임베딩 작업을 통해 거리를 계산할 수 있으므로 검색 및 추천, 클러스터링 및 분류, 이상치 탐지에 활용 가능
- 전이 학습
  +  하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
  + 전이 학습은 사전 학습과 미세 조정으로 진행
    + 사전학습은 대량의 데이터로 모델을 학습하는 과정
    +  미세 조정은 특정 문제를 해결하기 위해 데이터를 추가로 학습하는 과정
- 사진(그림 1.8)
## 언어 모델이 챗GPT가 되기까지
-  RNN(순환 신경망): 트랜스포머가 개발되기 전 사용했던 방식
    + 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축하는 방식
    +  빠르다는 장점이 있지만 입력이 길어지는 경우 먼저 이력된 단어의 의미가 점차 희석되며, 입력이 길어지는 경우 성능이 떨어진다는 단점 존재
- 사진 (그림 1.12)
-  이러한 RNN의 단점을 극복하기 위해, 맥락을 모두 참조하는 어텐션 연산 개발
    + RNN과 달리 맥락을 모두 참조하기 때문에, 예측에 걸리는 시간 증가
    + 무겁고 비효율적인 연산
- 사진(그림 1.15)
- 사진(그림 1.16)
## LLM 어플리케이션의 시대가 열리다
- LLM은 기존 자연어 처리 접근 방식과 달리 언어 이해 모델과 생성 모델이 하나의 LLM으로 수행이 가능하기 때문에 더 다양한 작업에 AI를 활용할 수 있다.
- 사진(그림 1.22 1.23)
-  sLLM: 모델의 크기가 작으면서 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델
    + 라마-3, 젬마-2, Phi-3 등이 존재
- LLM의 학습과 추론에 필요한 연산량을 줄이기 위해서 양자화(quantization)과 LoRA(Low Rank Adaption) 방식으로 연산량을 효율적으로 줄임
-  환각 현상: LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어내는 현상
    + 이를 해결하기 위해 프롬프트에 LLM이 답변할 때 필요한 정보를 미리 추가하는 기술인 RAG 기술 이용 
## LLM의 미래: 인식과 행동의 확장
-  멀티 모달: LLM이 더 다양한 형식(이미지, 비디오, 오디오 등)으로 입력을 받을 수 있고 다양한 형태의 출력이 가능하도록 발전시킨 모델
-  에이전트 : 특정한 작업을 수행하기 위해 LLM을 활용하여 작동하는 프로그램이나 시스템으로 자율성, 상호작용성, 학습 능력 보유
# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기
## 트랜스포머 아키텍처란
- RNN: 기존의 자연어 처리 문제에서 사용하던 방식
  +  텍스트를 순차적으로 처리하기 때문에 학습 속도가 느림
  +  입력이 길어지면 먼저 입력된 토큰의 정보가 희석
  +  그레이디언트 소실이나 그레이디언트 증폭 발생
- 사진(그림 2.1)
- 트랜스포머
  +  셀프 어텐션 사용
  +  작동 방식
      1. 입력을 임베딩으로 변환
      2. 위치 인코딩을 문장의 위치 정보 더하기
      3. 인코더에서 층 정규화, 멀티 헤드 어텐션, 피드 포워드 층을 거치며 디코더로 전달
      4. 디코더에서 층 정규화, 멀티 헤드 어텐션, 피드 포워드 층을 거치며 한국어 번역 결과 생성 
- 사진(그림 2.2)
## 텍스트를 임베딩으로 변환하기
-  토큰화: 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여하는 과정
    +  사전: 어떤 토큰이 어떤 숫자 아이디에 연결됐는지 기록해둔 것
    +  서브워드 토큰화: 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 방식
-  임베딩: 데이터의 의미를 담아 숫자 집합으로 변환하는 것
    + 파이토치의 nn.Embedding 클래스를 이용
-  위치 인코딩
    + 기존 RNN 방식과 트랜스포머의 큰 차이점은 입력을 순차적으로 처리하는지 여부
    +  모든 입력을 동시에 처리하기 때문에 순서 정보를 위치 인코딩으로 파악
    + 절대적 위치 인코딩: 입력 데이터의 각 토큰에 고유한 위치 정보를 부여해 모델이 순서를 인식할 수 있도록 하는 방법
    + 상대적 위치 인코딩: 입력 토큰 간의 상대적인 거리 정보로 더 유연하게 학습하는 방법
- 사진(그림(2.7))
## 어텐션 이해하기
- 딥러닝 모델이 작동하려면 단어 사이의 관계를 계산해 관련이 있는지 찾고, 관련이 있는 단어의 맥락을 포함시켜 단어를 재해석해야 함
- 쿼리: 우리가 입력하는 검색어
-  키: 문서가 가진 특징
-  값: 쿼리와 키를 기반으로 실제로 반환되는 정보
- 임베딩을 활용해 관련도를 직접 계산 시
    + 같은 단어끼리 임베딩이 동일하므로 관련도가 크게 계산되면서 주변 맥락 충분히 반영 못하는 경우 발생
    + 문법에 의해 토큰이 이어지는 경우처럼 간접적인 관련성 반영 X
- 트랜스포머에서는 토큰 임베딩을 변환하는 가중치를 도입해 문제 해결
    + Wq (쿼리 가중치): 입력 임베딩을 쿼리 벡터로 변환하는 행렬
    + Wk (키 가중치): 입력 임베딩을 키 벡터로 변환하는 행렬
    + Wv (값 가중치): 입력 임베딩을 값 벡터로 변환하는 행렬
    + 트랜스포머는 입력된 임베딩을 쿼리, 키, 값 벡터로 변환한 후, 쿼리와 키 벡터의 내적을 통해 각 단어들 간의 연관성을 계산
      * 쿼리와 키의 내적이 클수록 두 단어가 서로 강하게 관련된 것으로 해석
    + 어텐션 값이 구해지면, 각 단어의 값(Value) 벡터에 어텐션 값(가중치)을 곱해서 가중합을 구하고, 가중합을 통해 쿼리에 해당하는 단어가 다른 모든 단어와 얼마나 관련이 있는지를 파악
    + 
- 사진(그림 2.13)
- 멀티 헤드 어텐션
    + 하나의 어텐션만 수행하는 것이 아닌, 여러 어텐션 연산을 동시에 적용해 성능을 높이는 방식
    + 토큰 사이의 관계를 한 가지 측면에서 이해하는 것이 아닌, 여러 측면에서 동시에 고려하는 방식
- 사진(그림 2.17)
## 정규화와 피드 포워드 층
## 인코더
## 디코더
## BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
## 주요 사전 학습 메커니즘
# 3. 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리
## 허깅페이스 트랜스포머란
## 허깅페이스 허브 탐색하기
## 허깅페이스 라이브러리 사용법 익히기
## 모델 학습시키기
## 모델 추론하기
# 4. 말 잘 듣는 모델 만들기
## 코딩 테스트 통과하기: 사전 학습과 지도 미세 조정
## 채점 모델로 코드 가독성 높이기
## 강화 학습이 꼭 필요할까?
# 5. GPU 효율적인 학습
## GPU에 올라가는 데이터 살펴보기
## 단일 GPU 효율적으로 활용하기
## 분산 학습과 ZeRO
## 효율적인 학습 방법(PEFT): LoRA
## 효율적인 학습 방법(PEFT): QLoRA
# 6. sLLM 학습하기
## Text2SQL 데이터셋
## 성능 평가 파이프라인 준비하기
## 실습: 미세 조정 수행하기
