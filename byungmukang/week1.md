# LLM을 활용한 실전 AI 애플리케이션 개발
# 7. 모델 가볍게 만들기
## 언어 모델 추론 이해하기
- 언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하고 그 중 확률이 가장 높은 토큰을 입력 테스트에 추가하면서 토큰 생성
- ![](images/그림7_1.jpg)
- 밑의 그림과 같이 동일한 연산을 반복적으로 수행하기 때문에 비효율적
- ![](images/그림7_3.jpg)
- 이를 해결하기 위해 계산했던 결과를 메모리에 저장하는 방법인 KV 캐시 사용
- ![](images/그림7_4.jpg)
- 효율적인 서빙을 위해서는 높은 처리량과 낮은 지연 시간
- 메모리 바운드: 최적의 배치 크기보다 배치 크기가 작을 시 고대역폭 메모리에 있는 모델 파라미터를  SRAM으로 이동시키느라 연산이 멈추면서 생기는 비효율
- 연산 바운드: 배치 크기가 최적의 배치 크기보다 커지면 연산이 오래 걸리는 현상
- ![](images/그림7_9.jpg)
- 최대 배치 크기를 최적의 배치 크기와 가까워지게 만들어야 하기 때문에 배치 크기를 키워야 함
- 배치 크기를 키우는 방안
    + 모델의 용량 줄이기
    + KV 캐시 줄이기
- KV 캐시 메모리를 줄이기 위해서는 멀티 헤드 어텐션이 아닌 다른 연산 수행
    + 멀티 쿼리 어텐션: 여러 헤드의 쿼리 벡터가 하나의 키와 값 벡터를 사용하는 방법
    + 그룹 쿼리 어텐션: 멀티 헤드 어텐션보다는 키와 값의 수를 줄이지만 멀티 쿼리 어텐션보다는 많은 키와 값을 사용하는 방식으로 멀티 헤드 어텐션과 멀티 쿼리 어텐션을 절충한 방법
- ![](images/그림7_11.jpg)                                
## 양자화로 모델 용량 줄이기
  - 비츠앤바이츠: 양자화 방식을 쉽게 사용할 수 있는 라이브러리
    + 영점 양자화: 데이터의 최댓값과 최솟값을 변환하려는 데이터 형식의 범위로 변환하는 방식
    + 절대 최대값 양자화: 절대 최댓값을 기준으로 대칭적으로 새로운 데이터 타입으로 변환하는 방식
    + 입력 중 이상치가 포함된 열은 별도로 분리해 16비트 그대로 계산
  - GPTQ: 양자화를 위한 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하면서 양자화 이전과 유사한 결과가 나오도록 모델을 업데이트하는 방식 이용
  - AWQ: 중요한 파라미터의 값은 유지하면서 덜 중요한 파라미터들에만 양자화를 적용해 모델의 성능을 유지하면서 효율을 높이는 방식 이용
## 지식 증류 활용하기
  - 지식 증류: 더 크고 성능이 좋은 선생 모델의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델을 만드는 방법
    +단순히 선생 모델이 생성한 결과를 학생 모델이 모방하는 것이 아닌 학생 모델의 학습에 더 도움이 되는 데이터셋을 구축하거나 사람의 리소스가 필요한 작업에 선생 모델을 활용 가능
# 8.  sLLM 서빙하기
## 효율벅인 배치 전략
  - 일반 배치: 정적 배치라고도 부르며, 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식
  - 동적 배치: 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략
  - 연속 배치: 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가하는 방식
## 효율적인 트랜스포머 연산
  - 플래시어텐션: 메모리와 계산 효율성을 개선한 최적화된 어텐션 메커니즘
    + 트랜스포머 아키텍처는 학습 과정에서는 연산량이 시퀀스 길이의 제곱에 비례하고 추론 과정에서는 시퀀스 길이에 비례하게 증가
    + 연산이 오래 걸리는 이유는 GPU에서 메모리를 읽고 쓰는데 오랜 시간이 걸리기 때문
    + 데이터 이동 속도가 느린 고대역폭 메모리에 큰 어텐션 행렬을 쓰고 읽으면서 걸리는 시간을 줄이기 위해 블록 단위로 어텐션 연산을 수행하고 전체 어텐션 행렬을 쓰거나 읽지 않는 방식으로 어텐션 연산의 속도 높임
    + 작은 블록 단위로 연산 수행해서 SRAM에 데이터를 읽고 쓰면서 연산 빠르게 수행
    + 역전파 계산 과정에서 필요한 N * N 행렬 값은 역전파 과정에서 다시 순전파를 계산하는 방식으로 해결
    + 계산량은 증가하지만,  실행 시간은 짧아짐
  - 플래시어텐션2: 행렬 곱셉이 아닌 연산을 줄이고, 시퀀스 길이 방향의 병렬화를 추가해 플래시어텐션과 비교해 2배 정도 속도 개선
  - 사인파 위치 인코딩: 사인과 코사인을 이용해 위치 인코딩을 더하는 방식
  - 절대적 위치 인코딩은 학습 데이터보다 긴 입력이 들어오면 품질이 떨어진다는 단점
  - 상대적 위치 인코딩: 토큰의 절대적인 위치에 따라 임베딩을 더하는 것이 아닌 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 방식
  - ROPE(Rotary Positional Encoding): 각 토큰의 상대적 위치를 각도 기반의 주기 함수로 인코딩하여 순서 정보를 효율적으로 반영하는 방식
  - AliBI(Attention with Linear Biases): 토큰 간의 거리 정보를 선형 편향을 통해 반영하여, 더 먼 토큰에 가중치를 덜 주는 방식
## 효율적인 추론 전략
  - 커널 퓨전: 여러 개의 연산을 하나로 묶어서 오버헤드를 줄이는 기법
  - 페이지어텐션: 가상 메모리 개념을 이용해 메모리를 페이지 단위로 나누고, 필요한 데이터만 불러와 어텐션 연산을 수행함으로써 메모리 사용을 효율적으로 관리하는 방식
    + 블록 테이블: 논리적 메모리와 물리적 메모리를 연결하는 테이블
    + 페이지어텐션 기법을 이용하면 낭비되는 메모리양 감소
    + 병렬 샘플링에서 입력 프롬프트에 대한 메모리를 공유하면서 메모리 절약
    + 참조 카운트: 물리적 블록을 공유하고 있는 논리적 블록 수
- ![](images/그림8_26.jpg)  
- 추측 디코딩: 쉬운 단어는 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식
    + 드래프트 모델: 타깃 모델에 비해 빠르지만 생성 정확도는 떨어지는 모델
    + 타깃 모델: 드래프트 모델에 비해 느리지만 생성 정확도는 높은 모델
    + 2개의 모델을 이용해서 시스템 복잡도가 올라가지만 토큰 생성 시간은 줄어듬
## 실습: LLM 서빙 프레임워크
  - 오프라인 서빙: 정해진 입력 데이터에 대해 배치 추론 수행
  - 온라인 서빙: 사용자의 요청이 올 때 모델 추론을 수행
  - vLLM은 연속 배치와 페이지어텐션 기술을 사용해 허깅페이스의 파이프라인 추론 대비 높은 속도로 추론이 가능
# 9. LLM 어플리케이션 개발하기
## 검색 증강 생성(RAG)
  - RAG(검색 증강 생성): LLM에게 단순히 질문이나 요청만 전달하고 생성하는 것이 아닌 답변에 필요한 정보와 맥락을 제공하고 답변하도록 하는 방법
    + 검색할 데이터를 벡터 데이터베이스에 저장
    + 사용자 인터페이스를 통해 들어온사용자의 요청에 관련된 정보를 벡터 데이터베이스에서 검색한 후 사용자의 요청과 결합해 프롬프트 완성
  - 라마인덱스(LlamaIndex): 대형 언어 모델(LLM)을 활용해 문서나 데이터베이스에서 정보를 효율적으로 검색하고 조직하는 인덱싱 라이브러리
## LLM 캐시
  - LLM 캐시: 이전에 생성 결과를  기록하여 동일한 요청이 들어오면 이전의 생성 결과를 이용하는 방법
    + 일치 캐시: 요청이 완전히 일치하는 경우
    + 유사 검색 캐시: 임베딩 벡터를 통해 유사한 요청이 있었는지 확인하고, 유사한 벡터가 있더만 저장된 텍스트를 반환 
## 데이터 검증 
 - 데이터 검증: 벡터 검색 결과나 LLM 생성 겨로가에 포함되지 않아야 하는 데이터를 필터링하는 과정
    1. 규칙 기반: 문자열 매칭이나 정규 표현식을 활룡해 데이터를 확인하는 방식
    2. 분류 또는 회귀 모델: 입력 데이터가 적절한지 여부를 예측하기 위해 기계 학습 모델을 사용하는 방식
    3. 임베딩 유사도 기반: 데이터의 의미적 유사성을 비교하여 데이터가 올바른지 검증하는 방식
    4. LLM 활용: LLM을 활용해 텍스트에 부적절한 내용이 섞여 있는지 확인하는 방식
## 데이터 로깅
  - 데이터 로깅: 사용자의 입력과 LLM이 생성한 출력을 기록
    + 입력이 동일해도 출력이 달라질 수 있기 때문에 어떤 입력에서 어떤 출력을 반환했는지 기록 필요
    + 서비스 운영, 어플리케이션 개선, 고도화

# 10. 임베딩 모델로 데이터 의미 압축하기
## 텍스트 임베딩 이해하기
## 문장 임베딩 방식
## 실습: 의미 검색 구현하기
## 검색 방식을 조합해 성능 높이기
## 실습: 하이브리드 검색 구현하기
# 11. 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기
## 검색 성능을 높이기 위한 두 가지 방버
## 언어 모델을 임베딩 모델로 만들기
## 임베딩 모델 미세 조정하기
## 검색 품질을 높이는 순위 재정렬
## 바이 인코더와 교차 인코더로 개선된 RAG 구현하기
