# LLM을 활용한 실전 AI 애플리케이션 개발

## 1.1 딥러닝과 언어 모델링

**LLM (Large Language Models)**은 딥러닝 기반의 언어 모델로, 자연어 처리 분야(NLP)에 속한다.
* 딥러닝 - 인간의 두뇌에 영감을 받아 만들어진 신경망. 정형데이터, 비정형데이터 인식이 높다.
* 자연어 처리 분야 - 사람의 언어를 컴퓨터가 이해하고 생성 할 수 있는 연구 분야.

주요 특징:
- **정형/비정형 데이터**에서 패턴 인식 성능이 높음
- **딥러닝 모델**을 통해 자율적인 학습이 가능함
- **임베딩** 기술을 사용하여 데이터를 숫자형 벡터로 변환함

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝
**딥러닝 문제 해결 방법**
1) 문제 유형에 맞는 일반적으로 사용되는 모델 준비  
2) 풀고자 하는 학습 데이터를 준비  
3) 학습 데이터를 반복적으로 모델에 입력하여 학습 

딥러닝은 머신러닝과 반대로 상당한 양의 **학습데이터**를 제공하기만 하면 시스템이 **자율적으로 학습** 할 수 있다. 머신런닝은 개발자가 데이터의 ‘특징’을 직접 찾고 입력해야 한다.

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

**임베딩(embedding)**은 데이터의 의미와 특직을 포착하여 숫자의 집합으로 변환시킨다.
데이터를 임베딩을 하여 숫자로 표현 한 후, 데이터의 유사함을 거리로 측정이 가능하다. (ex:MBTI)

임베딩의 활용:
- **검색 및 추천**: 검색어와 관련있는 상품 추천
- **클러스터링 및 분류**: 유사 데이터를 묶음
- **이상치 탐지**: 나머지 데이터와 먼 비정상적 데이터를 탐지

단어 임베딩(word embedding) - 단어를 임베딩으로 변환하는 것.
일반적으로 수십-수만개의 숫자로 표현한다. 딥러닝 모델이 데이터 특징을 알아서 추출하기 때문에 각 숫자의 의미를 알기 어렵다. 숫자 집합 을 안다면 단어의 의미를 알 수 있다.


### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

**언어 모델링** - 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식

**전이 학습** - 하나의 문제를 해결하는 과정에서얻은 지식을 다른 문제에 활용하는 방식 (딥러닝 분야에서 애용)
+ 사전 학습 - 대량의 데이터로 모델을 학습
+ 미세 조정 - 특정 데이터를 해결하기 위한 추가 학습 (이 학습 모델을 사용하는 과제를 downtream 과제라도 부른다)

![전이 학습 방식](./picture/photo_2024-09-11_23-02-14.jpg)
본체 - 이미지넷으로 학습한 모델 (점이나 선같은 기본적인 이미지 특징을 이미 파악해 놓은 데이터셋)
___ 분류헤드 - 해결하려는 이미지의 데이터셋을 추가 학습

---

## 1.2 언어 모델이 CHAT GPT가 되기 까지

### 1.2.1 RNN에서 트랜스포머 아키텍처(구조)로

**시퀀스 데이터** - 순서대로 정열된 데이터의 연속. 텍스트, 오디오, 시계열 데이터 등이 이에 해당하며, 이를 처리하기 위해 **RNN** 또는 **트랜스포머**를 사용한다.

**RNN(Recurrent Neural Network)** - 트랜스포머 아키텍처를 사용하기 전 텍스트를 생성하기위해 사용되었던 모델. 정보가 쌓이면서 텍스트 맥락이 압축된다. 단점: 앞에 있는 단어들의 의미가 희석된다
![입력을 순차적으로 처리하는 RNN 모델의 방식](./picture/photo_2024-09-11_23-02-22.jpg)

**트랜스포머 아키텍쳐** - 문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 신경망. 맥락 데이터를 모두 활용한다. 무겁고 비효율적인 연산을 한다. 그러나 RNN의 단점(느린 학습 속도, 긴 시퀀스 처리 성능 저하)을 극복하고 병렬 연산을 가능하다.  
![트랜스포머 아키텍쳐의 어텐션 연산](./picture/photo_2024-09-11_23-02-21.jpg)

* 결론: RRN은 효율적이지만 성능이 낮다. 트랜스포머 아키텍쳐는 성능이 높지만 효율이 낮다. 하지만 병렬처리를 통해 학습속도를 높일 수 있어 대부분 LLM이 트랜스포머 아키텍처를 사용 한다.

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계 

모델 데이터가 많아질 수록 최종 모델의 성능이 높아진다. 학습하는 과정 중 
학습 데이터의 공통되고 중유한 패턴을 남겨 손실 압축한다. 그래서 학습데이터 모델 크기가 최대 모델 크기보다 많을 수 없다.

### 1.2.3 Chat GPT의 등장

**지도미세조정** - 언어 모델링으로 학습한 언어모델을 *지시 데이터 셋*으로
추가 학습한다. ( 데이터셋을 위해 OpenAi에서는 따로 작업자에게 LLM이 받을법한 질문과 답을 작성해야 했다. 그 결과 사용자의 요청에 맞춰 응답하는 모델을 만들 수 있었다)
지시 데이터 셋 - 사용자가 요청한 사항과 그이 대한 적절한 응답을 정리한 데이터 셋

**RLHF(reinforcement Learning from Human Feedback)** - 사람의 피드백을 활용한 강화 학습. 선호 데이터셋으로( 두가지 선택 중 사용자가 더 선호하는 답변을 정리한 데이터셋) 답변을 평가하는 리워드 모델을 만들어 LLM이 높은 점수
를 받게 추가 학습한다.

**지도 미세 조정** 과 **RLHF** 라는 기술 덕분에 사용자의 말을 이어서 작성하는 능력 뿐만이 아니라 사용자의 요청을 해결하는 능력이 생겼다.

##1.3 LLM 애플리케이션의 시대가 열린다

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM 
LLM은 다른 자연어 처리 모델과 다르게 언어를 *이해*하는 것 뿐만이 아니라 결과를 언어로 *생성*해야 하는 능력이 뛰어나다.

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기 

기업들이 LLM을 크게 두가지로 활용한다:
**상업용 API**: 장점: 모델이 크다, 텍스트 생성 능력이 뛰어나다. 단점: 추가 학습이 안된다. 여러 모델이 뒤섞여 있어 특정분야를 잘 모를 수 있다
**오픈소스 LLM**: 장점: 특정 도메인 데이터나 작업에 높은 성능을 보여준다. 단점: 모델 크기가 작다. 이를 **sLLM**이라고 한다

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증각 생성(RAG) 기술
LLM은 정보가 사실인지, 거짓인지 판단을 못한다. 그래서 잘못된 정보가 실제로 존재하지 않는 정보를 만드는 **‘환각 현상’**이 생긴다.
이를 해결하기 위해 **RAG**이라는(응답을 생성하기 전에 학습데이터에 신뢰 할 수 있는 지식 베이스를 참조하게 하는 기술) 기술을 사용한다.

(예: 위키페디아에서 참조)

## LLM의 미래: 인식과 행동의 확장
AI분야는 다양한 방향으로 연구가 진행되고 발전해 나가고 있다. 

- 멀티모달 LLM: 다양한 형식의 데이터(ex:오디노,비디오)를 입력으로 받고 여러 형태로 출력 할 수 있는 LLM 모델
- 에이전트: 텍스트 생성 능력을 사용해 계획을 세우거나 늬사결정을 내리고 필요한행동을 수행하는 LLM 모델
- 트랜스포머 아키텍쳐보다 성능이 높고 효율적인 아키텍쳐 연구도 진행하고 있다.  

---

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기

## 2.1 트랜스포머 아키텍처란

**RRN**의 모델 구조는 모델의 한번에 한 토큰씩 입력을 받아 추출을 한다. 이 출력을 다시 다음 입력 토큰과 함께 RRN에 입력해야 하기 때문에 
**비효율적**이다. (사람이 글을 읽는 것과 비슷) (학습속도가 느려지고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어진다.)

![RRN 모델 구조](../static/img_2.2_텍스트_임베딩.png)
> RRN 모델 구조

이를 해결하기 위해 트랜스포머는 **셀프 어텐션(self attention)**을 사용한다. 이는 인코더와 디코더에 가장 중요한 부분이다. 
셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관계인지 계산해서 각 단어의 표현을 조정한다.

트랜스포머 장점:
- 확장성: 더 깊은 모델을 만들어도 학습이 잘된다. 
- 효율성: 학습할때 병렬연산이 가능하기 때문에 학습 시간이 단축된다. 
- 더 긴 입력처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.

트랜스포머 아키텍쳐는 크게 **인코더**(언어를 이해하는 역할)와 **디코더**(언어를 생성하는 역할)로 나뉜다. 
- 인코더와 디코더 모두 처음에 모델을 토큰화 후 임베딩 층을 통해 임베딩(숫자 집합) 으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다
- 인코더에서 층 정규화(layer normalization), 멀티 헤드 어텐션(multi-head attention), 피드 포워드(feed forward)층을 거쳐 디코더로 전달한다.
- 디코더에서 층 정규화(layer normalization), 멀티 헤드 어텐션(multi-head attention)을 수행하면서 크로스 어텐션 연산을 한다
- 디코더와 인코더의 출력을 종합해서 피드 포워드 층을 거쳐 결과를 생산한다.

![트랜스포머 아키텍쳐를 구성하는 인코더와 디코더](./picture/photo_2024-09-11_23-02-19.jpg)
> 트랜스포머 아키텍쳐를 구성하는 인코더와 디코더

## 2.2 텍스트를 임베딩으로 변환하기
컴퓨터가 텍스트를 이해하고 계산 하기 위해 텍스트를 숫자 형식으로 바꿔야 한다.
1) **토큰화(tokenization)** 수행: 텍스트를 적절한 단위로 잘라 숫자형 ID를 부여한다
2) 토큰 ID를 **토큰 임베딩 층**을 통해 여러 숫자의 집합인 **토큰 임베딩**으로 변환한다.
3) **위치 인코딩**을 통해 토큰의 위치 정보를 담고 있는 임베딩을 추가 해 최종적으로 모델에 입력 할 임베딩을 만든다. 

![텍스트에서 임베딩으로 변환하는 과정](./picture/photo_2024-09-11_23-02-18.jpg)
> 텍스트에서 임베딩으로 변환하는 과정

### 2.2.1 토큰화 

**토큰화** - 텍스트를 적절한 단위로 나누고 ID를 부여하는 것
**서브워드 토큰화** - 자주 나오는 단어일수록 그대로 유지하여 토큰화

**토큰화 방법:**
1) 문장을 단위로 자른다.(한국어는 자모, 음절, 단어단위로 자를 수 있다.)
2) 토큰 딕셔너리를 만든다.(각각 단어에게 순서대로 토큰을 부여한다) (str2idx)
3) ID 딕셔너리를 만든다. (ID를 순서대로 각각 단어에게 부여된다) (idx2str)
4) ID 딕셔너리에서 토큰을 토큰 딕셔너리에서 가져온 토큰 ID로 변환한다.

![토큰화 방식](./picture/photo_2024-09-11_23-02-17.jpg)
> 등장빈도와 단위에 따른 서브워드 토큰화 방식


### 2.2.2 토큰 임베딩으로 변환하기
딥러닝 모델이 토큰 사이에 관계를 숫자로 계산해야 한다. 토큰ID는 숫자 하나일뿐이라서 의미를 못 담는다. 그래서 1.1.2에서 MBTI예시처럼 임베딩을 만들어야 한다. 
**임베딩** - 데0이터의 의미를 담아 숫자 집합으로 변환하는 것

이를 위해 PyTorch에 제공하는 no.Embedding 클래스를 사용해야 한다.
1) 임베딩 벡터의 차원을 정한다. (embedding_dim)(ex: 3차원: [0.0, 0.0, 0.0],[0.2,0.4,0,1]…)
2) 차원에 임베딩을 생성하는 임베딩 레이어(embed_layer)를 정한다. 이는 토큰 아이디 집합의 크기에 따른다.
3) 입력 토큰을 임베딩 층을 통해 임베딩으로 변환한다.

위 과정을 거친다고 해서 토큰의 의미가 담겨 벡터로 변환되는 것은 아니다. 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련되어야 한다.
딥러닝은 머신러닝과 달리 직접 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 점점 토큰의 의미를 잘 담는 임베딩을 같이 학습하게 된다.

### 2.2.3 위치 인코딩

위치 인코딩이 필요한 이유: 트랜스포머 아키텍쳐에서 RRN과 달리 모든 토큰 입력을 동시에 처리하는데, 그 과정에서 **순서 정보를 따로 추가**하기 위해 존재한다.

**절대적 위치 인코딩(absolute position encoding)**- 위치 인덱스 자체를 위치 임베딩으로 사용한다.
장점:간단하게 구현이 가능하다. 단점: 토큰과 토큰 사이에 상대적 위치 정보를 활용 못한다. 긴 텍스트를 추론하는 경우 성능이 떨어진다. 
상대적 위치 인코딩 (relative position encoding) - 절대적 위치 인코딩 단점을 보완한다. 

트랜스포머가 있는 지금은 모든 입력 토큰을 동일하게 처리하시 떄문에 위치 정보를 더해주는 역할만 한다.

*(인코딩, 임베딩 차이가 뭐지???)*

## 2.3 어텐션 이해하기

### 2.3.1 사람이 글을 읽는 방법과 어텐션

단어의 뜻을 찾으려면 단어와 단어 사이에 관계를 계산 해 그 값에 따라 관련이 깊은 단어와 깊지 않는 단어를 필터링 한다. **어텐션**이 이 기능을 수행한다.

어텐션은 문장에서 각 단어 간의 관련성을 학습하는 메커니즘입니다. **쿼리(Query)**, **키(Key)**, **값(Value)** 세 요소를 통해 작동한다.

### 2.3.2 쿼리, 키, 값 이해하기
검색 분야에서의 비유: 
- **쿼리**- 검색어 (단어)
- **키**- 문서 제목,본문,저자 (문장 속 각 단어)
- **값**- 키의 문서 (단어가 전달하려는 정보) (이것을 검색하면서 원하는 것이다)

쿼리와 키 관계 계산 방법: 쿼리와 키를 **토큰 임베딩**으로 변환하여 관계를 계산해서 관련도를 계산한다. 
그러나 이러면 같은 단어끼리(ex:파리(Paris,fly)) 임베딩이 똑같아서 관련도가 크게 계산된다. 또한 간접적인 관련성이 반영되기 어렵다.(ex: ‘나는‘, 
‘최근’은 ‘다녀왔다’토큰에 누가,언제를 나타내는 문법관계이지만 토큰 자체로 봤을때는 관련성을 찾기 어렵다.)
이를 해결하기 위해 **가중치**(Wq,Wk)를 도입했다. (딥러닝에서는 어떤 기능을 잘하고 싶을 때 가중치를 도입하고 학습단계에서 업데이트 되게 한다.)
가중치를 통해 토큰과 토큰 사이에 관계를 계산하는 능력을 학습시킨다.

스케일 점곱 방식의 어텐션 연산 코드:
1) 가중치 계산
1.1) PyTorch에서 제공하는 nn.Linear 층을 사용해 쿼리,키,값 각각의 가중치를 생성한다. 
1.2) 입력 임베딩(input_embedding)을 선형 층에 통과시켜 쿼리, 키, 값을 생성한다.
2) 쿼리, 키, 값 관계 를 계산한 새로운 단어 벡터 생성 
2.1) 쿼리와 키를 곱해 두 단어의 관련성을 계산한다. (분산이 커지는 것을 방지하기 위해 임베딩 차원 수의(dim_k)의 제곱근으로 나눈다.)
2.2) 그 값들을 소프트맥스(softmax)를 취해 0-1사이에 값으로 바꿔서 가중치로 바꾼다.
2.3) 가중치와 값을 곱해 입력은 동일하면서 주변 토큰과의 관련도에 따라 새로운 토큰 임베딩을 반환한다
이로서 트랜스포머에서는 쿼리,키,값을 토큰 임베딩 하여 가중치를 통해 변환한다. 이 세가지 가중치를 통해 토큰과 토큰 사이에 관계를 계산해 적절한 주변 맥락을 반영하는 학습을 시킨다.

![가중치 계산](./picture/photo_2024-09-11_23-02-30.jpg)
> 가중치 계산

![값 벡터를 가중합해서 새로운 단어 벡터 생성](./picture/photo_2024-09-11_23-02-29.jpg)
> 값 벡터를 가중합해서 새로운 단어 벡터 생성

### 2.3.4 멀티 헤드 어텐션 

**멀티 헤드 어텐션** - 어텐션 연산을 동시에 수행하는 어텐션 방식. 이를 통해 토큰 사이에 다양한 관점에서 단어 간에 관계를 학습 할 수 있다.
여러개에 어텐션을 동시에 사용하여 문맥 이해에 도움이 된다.

멀티 헤드 어텐션 코드:
1) 쿼리,키,값의 각각 선형층을 n_head(헤드의 수)로 나눠서 각각 변환 후 입력과 같은 형태로 변환한다.
2) 쿼리, 키, 값의 각각 어텐션 계산한다
3) 어텐션 결과를 다시 원래 현태로 변환한다
4) 마지막으로 선형 층을 통과시키고 최종 어텐션 결과를 반환한다.

## 2.4 정규화와 피드 포워드 층 

**정규화** - 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법이다. 

과거에는 배치 정규화를 사용하였지만, 지금은 트랜스포머 아키텍처에서는 특정 차원에서 정규화를 수행하는 층 정규화를 사용한다. 

어텐션 연산 - 단어와 단어 사이 의미를 계산하여 토큰 임베딩을 조정하는 연산
피드 포워드 층 - 전체 입력 문장을 계산하여 연산

### 2.4.1 층 정규화 이해하기 

딥러닝 모델에서 **정규화가 필요한 이유**: 입력 데이터가 딥러닝 모델의 각 층을 거치면서 어떤 특성은 좁은 분포를 갖고 어떤 특성은 넓은 분포를 갖게 된다. 특히 층이 깊은 모델에서는 분포 의 차이가 발생할 가능성이 높아지고 그렇게 되면 학습이 잘되지 않는다. 그래서 입력 변수가 데이터 분포를 비슷한 범위 내에 조정하여 <u>정확한 예측</u>을 하기 위해서다. 

다음과 같은 식으로 계산한다. 벡터 x를 정규화한 norm_x는 벡터 x에서 x의 평균을 빼고 x의 표준편차로 나 눠 평균이 0이고 표준편차가 1인 분포를 갖게 된다.
- `norm x = (x-평균)/표준편차` 

일반적으로 이미지 처리에서는 배치 정규화를 사용하고, 자 연어 처리에서는 층 정규화를 사용한다.

**배치 정규화** -모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행.

**배치 정규화의 한계**: 자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양해서 정규화에 포함되는 데이터의 수가 제각각이라 정 규화 효과를 보장하기 어렵다.

예시:
![그림 2.18 자연어 처리에서 배치 정규화의 한계](./picture/photo_2024-09-11_23-02-28.jpg)

<u>층 정규화</u>는 이런 단점을 보완한다.

**층 정규화** - 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다.

![그림 2.19 자연어 처리에서 층 정규화의 장점](./picture/photo_2024-09-11_23-02-27.jpg)
 
층 정규화를 작용하는 순서로 옛날에는 사후 정규화를 사용하였지만 요즘에는 사전 정규화를 사용한다.
**사전 정규화** - 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통과하는 방법

![그림 2.20 (a) 사후 정규화와 (b) 사전 정규화](./picture/photo_2024-09-11_23-02-26.jpg)


### 2.4.2 피드 포워드 층

**피드 포워드 층feed forward layer **- 데이터의 특징을 학습하는 완전 **연결 충fully connected layer** 말한다.

코드와 같이 <u>선형 중, 드림아웃 중, 중 정규화, 활성 함수</u>로 구성된다. 
쉽게 층을 쌓아 확장 하기 위해 입력과 출력의 형태가 동일하도 록 맞춘다.

```

class PreLayerNormFeedForward(nn.Module):
  def __init__(self, d_model, dim_feedforward, dropout):
    super().__init__()
    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1
    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2
    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1 (모든 건형 층이 성능이 햘상되게 일부 선형층을 아웃)
    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2
    self.activation = nn.GELU() # 활성 함수
    self.norm = nn.LayerNorm(d_model) # 층 정규화

  def forward(self, src):
    x = self.norm(src)
    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))
    x = self.dropout2(x)
    return x
```

## 2.5 인코더 


인코더는 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태다.   
잔차 연결 - 입력을 다시 더해주는 형태. 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층를 연결해서 안정적인 학습 가능하다

### 2.6 디코더 

**디코더,인코더 첫 번째 차이:** 인코더: 기본적인 멀티 헤드 어텐션을 사용. 디코더: **마스크 멀티 헤드** 어텐션을 사용. 

디코더는 **인과적causal **또는 **자기 회귀적auto-regressive** 특징을 갖는다.(  앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다.)

미래 시점에 작성해야 하는 텍스트를 미리 학습하지 않기 위해 그 이 전에 생성된 토큰까지만 확인 할 수 있도록 마스크를 추가한다.

1) 대각선 아래 부분만 1을 추가, 나머지는 음의 무한대로 마스크를 만든다.
2) 가중치를 만들기 위해 소프트맥스를 취할떄 마스크가 된 곳은 음의 무한대로 가중치가 0이 된다. 

![그림 2.23 마스크 연산](./picture/photo_2024-09-11_23-02-24.jpg)

**디코더,인코더 두 번째 차이:** 인코더의 결과를 디코더가 활용하는** 크로스 어텐션** (다른 두 데이터 집합 간의 상관관계를 학습하는 방식)연산이 있다.

## 2.7 BERT, GPT, T5등 트랜스포머를 활용한 아키텍처

트랜스포머 아키텍처를 활용한 모델. 
1. 인코더만 활용해 자연어 이해 작업에 집중한 그룹 
2. 디코더만 활용해 자연어 생성 작업에 집중한 그룹
3. 인코더와 디코더를 모두 활용해 더 넓은 범위의 작업을 수행할 수 있도록 한 그룹 

![표 2.1 각 모델 그룹의 장단점](./picture/photo_2024-09-11_23-02-23.jpg)

## 2.8 주요 사전 학습 메커니즘 

### 2.8.1 인과적 언어 모델링 
**인과적 언어 모델링** - 문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식. 이전 단어를 바탕으로 다음 단어를 예측

'A라는 단어 다음에는 B라는 단어가 자주 온다'는 직관을 배운다. 언어 모델도 인과적 언어 모델링 방식으로 많은 데이터에 대해 다음 단어를 예측하는 방법을 학습함으로써 <u>더 가능성이 높은 단어를 생성</u> 하는 능력을 갖추게 된다.

GPT 같은 생성 트랜스포머 모델에서는 인과적 언어 모델링을 핵심적인 학습 방법으로 사용한다.

### 2.8.3 마스크 언어 모델링

**마스크 언어 모델링** - 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습 시킨다

![그림 2.28 MLM 학습 방식](./picture/photo_2024-09-11_23-02-34.jpg)

# 03 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리

## 3.1 허깅페이스 트랜스포머란
**허깅페이스 트랜스포머** - 공통된 인터페이스로 다양한 트랜스포머 모델을 활용할 수 있도록 지원하는 오픈소스 라이브러리.

허깅페이스 제공하는 라이브러리:
-  트랜스포머 모델과 토크나이저를 활용할 때 사용하는 **transformers 라이브러리**
- 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원하는 **datasets 라이브 러리** 

> 예제 3.1. BERT와 GPT-2 모델을 활용할 때 허깅페이스 트랜스포머 코드 비교


```
from transformers import AutoTokenizer, AutoModel

text = "What is Huggingface Transformers?"
# BERT 모델 활용
bert_model = AutoModel.from_pretrained("bert-base-uncased")
bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
encoded_input = bert_tokenizer(text, return_tensors='pt')
bert_output = bert_model(**encoded_input)
# GPT-2 모델 활용
gpt_model = AutoModel.from_pretrained('gpt2')
gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')
encoded_input = gpt_tokenizer(text, return_tensors='pt')
gpt_output = gpt_model(**encoded_input)
```

## 3.2 허깅페이스 허브 탐색하기

**허깅페이스의 허브** -  다양한 사전 학습 모델과 데이터셋을 탐색하고 쉽게 불러와 사용 할 수 있도록 제공하는 기능.

**Spaces** - 자신의 모델 데모를 제공하고 다른 사람의 모델을 사용해 볼 수 있는 기능

### 3.2.1 모델 허브
모델 허브에는 작업, 언어 등다양한 기준으로 모델이 분류되어 있는 모델들을 제공한다.

### 3.2.2 데이터셋 허브
데이터셋 허브는 모델 허브와 거의 비슷한 형태로 데이터셋 크기, 유형 등 분류 기준이 추가 되어 있다. 또한 선택한 기준에 맞는 데이터셋을 보여준다. 

### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스 
**스페이스** - 사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능이다.

또한 허깅페이스는 <u>다양한 오픈소스 LLM</u>과 그 <u>성능 정보</u>를 게시하는 **리더보드 Leaderboardg** 운영하고 있다.

많은 오픈소스 모델이 새롭게 공개 되고 있기 때문에 어떤 모델을 사용하는 것이 좋을지 판단하기 쉽지 않다. 이럴 때 리더보드를 살펴보면 각 모델의 크기와 성능을 한눈에 비교할 수 있기 때문에 탐색에 큰 도움이 된다.

## 3.3 허깅페이스 라이브러리 사용법 익히기
모델을 학습시키거나 추론하기 위해서는 모델, 토크나이저, 데이터셋이 필요한다. 이번 절에서  코드를 통해 사용하는 법을 익힌다.
### 3.3.1 모델 활용하기

허깅페이스에서는 모델을** 바디body**와 **헤드head**로 구분한다.
바디 - 모델의 기본 구조
헤드 - 특정 작업을 처리하도록 설계된 부분

![그림 3.9 바디와 헤드의 구분이 필요한 이유](./picture/photo_2024-09-11_23-02-43.jpg)

### 3.3.2 토크나이저 활용하기 
**토크나이처** - 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환한다. 필요한 경우 특수 토큰을 추가하는 역할도 한다.

토크나이저도 허깅페이스 모델 저장소 아이디를 통해 불러올 수 있다. 허깅페 이스 허브에서 **모델**(config.json)과 **토크나이저**((tokenizer_config.json - 토크나이저의 종류나 설정) (tokenizer.json - 실제 어휘 사전)를 불러오는 경우 동일한 모델 아이디로 맞춰야 한다.

> 예제 3.8. 토크나이저 불러오기
```
from transformers import AutoTokenizer
model_id = 'klue/roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_id)
```
> 예제 3.9. 토크나이저 사용하기

```
tokenized = tokenizer("토크나이저는 텍스트를 토큰 단위로 나눈다")
print(tokenized)
# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2],
#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))
# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']

print(tokenizer.decode(tokenized['input_ids']))
# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]

print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))
# 토크나이저는 텍스트를 토큰 단위로 나눈다
```

특수 토큰:
[CLS]: 주로 문장을 시작하는 토큰 , 주로 분류 작업에 사용된다.
[SEP]: 주로 문장을 끝내는 토큰, 두 문장을 토큰하면 구분하는 역할도 한다

### 3.3.3 데이터 셋 활용하기 

datasets 라이브러리를 사용하면 앞서 허깅페이스 허브에서 살펴봤던 데이터셋을 코드로 불러올 수 있다.

> 예제 3.15. KLUE MRC 데이터셋 다운로드

```
from datasets import load_dataset
klue_mrc_dataset = load_dataset('klue', 'mrc')
# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')
```

로컬 데이터 셋을 다운로드 받고 싶다면 예제에 있는 코드로 파일이나 파이썬 객체를 받아 사용 할 수 있다.
아래 코드를 실행하기 위해서는 구글 코랩에 csv 파일이 업로드 되어야 한다.
>예제 3.16. 로컬의 데이터 활용하기

```
from datasets import load_dataset
# 로컬의 데이터 파일을 활용
dataset = load_dataset("csv", data_files="my_file.csv")

# 파이썬 딕셔너리 활용
from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)

# 판다스 데이터프레임 활용
from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)
```

## 3.4 모델 학습시키기
한국어 기사 제목을 바탕으로 기사의 카테고리를 분류하는 텍스트 분류 모델을 학습하는 실습 진행

## 3.5 모델 추론하기 
### 3.5.1 파이프라인을 활용한 추론

허깅페이스는 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 pipeline을 제공한다.

>학습한 모델을 불러와 pipeline을 활용해 추론하기

```
from transformers import pipeline

model_id = "본인의 아이디 입력/roberta-base-klue-ynat-classification"

model_pipeline = pipeline("text-classification", model=model_id)

model_pipeline(dataset["title"][:5])
```

### 3.5.2 직접 추론하기 

>예제 3.31. 커스텀 파이프라인 구현


```
import torch
from torch.nn.functional import softmax
from transformers import AutoModelForSequenceClassification, AutoTokenizer

#CustomPipeline의 인스턴스를 호출할 때 내부적으로 _call_ 메서드를 사용하는데, tokenizer를 통해 토큰화를 수행
class CustomPipeline:
    def __init__(self, model_id):
        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model.eval()

    def __call__(self, texts):
        tokenized = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
#모델 추론을 수행
        with torch.no_grad():
            outputs = self.model(**tokenized)
            logits = outputs.logits
#가장 큰 예측 확률을 갖는 클래스를 추출해 결과로 반환
        probabilities = softmax(logits, dim=-1)
        scores, labels = torch.max(probabilities, dim=-1)
        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]

        return [{"label": label, "score": score.item()} for label, score in zip(labels_str, scores)]

custom_pipeline = CustomPipeline(model_id)
custom_pipeline(dataset['title'][:5])
```
# 04 말 잘 듣는 모델 만들기 
## 4.1 코딩 테스트 통과하기: 사전 학습과 미세 조정
### 4.1.1 코딩 개념 익히기: LLM의 사전 학습 

라마 모델 LLM의 사전 학습 방법: 코드,블로그, 기사, 광고 등 약 10TB 분량의 여러가지 글을 학습하게 한다. 

사전 학습 동안은 ILM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어 를 점점 더 잘 예측하게 된다.

다음 단어를 예측하는 언어 모델을 학습시킬 때는 학습 데이터의 일부를 입력으로 넣고 바로 다음에 나오는 정답 토큰을 맞추도록 학습한다. 그림에 보이듯이 수많은 학습 데이터에 대해 수행하면서 어떤 단어가 다음에 올지 학습하게 된다.

>언어 모델은 정답 토큰이 올 확률을 높이는 방식으로 학습한다

![그림 4.3 언어 모델은 정답 토큰이 올 확률을 높이는 방식으로 학습한다](./picture/photo_2024-09-11_23-02-42.jpg)   

### 4.1.2 연습문제 풀어보기: 지도 미세 조정
**지도 미세 조정** - 사전 학습한 모델링 토대로 요청의 형식을 적절히 해석하고, 응당의 형태를 적절히 작성하며, 요청과 응답이 잘 연결되도록 추가 학습하는 것 

- 지도 미세 조정의 **지도**는 학습 데이터에 정답이 포함되어 있다는 뜻이다.
- 정렬: 사용자의 요청과 LLM의 응답이 정렬되도록 하는 것

**지시 데이터 셋** - 사용자가 요청한 사항에 대한 적절한 응답을 정리한 데이터 셋

문제점: 딥러닝 모델은 학습 데이터에 의존해 행동을 학습하므로, 요청에 응답하는 데이터가 적으면 그 행동을 잘 학습하지 못한다.

이런 문제를 보완하기 위해 사용자의 요구사항과 그에 대한 응답(정답)을 구조화한 데 이터를 구축하고 언어 모델의 학습에 활용한다.

지도 미세 조정도 사전 학습처럼 다음 단어를 예측하는 인과적 언어 모델링 causal language modeling을 사용해 학습한다.

### 4.1.3 좋은 지시 데이터셋이 갖춰야 할 조건 
- 지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다.
- 가설: 적지만 가치가 높은 데이터셋이 많지만 품질이 떨어지는 데이터셋보다 높은 성능을 달성 할 수 있다.

피상적 정렬 가설: 모델의 지식이나 능력은 사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델이 능력과 지식을 어떻게  나열하는지 정도만 추가로 배움

 **좋은 지시 데이터셋이 갖춰야 할 조건 **
- 지시 데이터셋을 작은 규모로 구축하더라도 모델이 지시사항의 형식을 인식하고 답변하도록 만들 수 있다.
-지시사항이 다양한 형태이고 답변의 품질이 높을수록 모델의 답변 품질도 높아진다.
- 학습 데이터의 품질을 높이기 위해 모델의 목적에 맞춰 학습 데이터의 교육적 가 치를 판단하고 교육적 가치가 낮은 데이터를 필터링하는 방법을 사용할 수 있다.
- 교재의 예제 데이터와 같은 고품질의 데이터를 학습 데이터에 추가하면 성능을 크 게 높일 수 있다.

## 4.2 채점 모델로 코드 가독성 높이기
### 4.2.1 선호 데이터셋을 사용한 채점 모델 만들기
선호 데이터셋: 두 데이터 중 사람이 더 선호하는 데이터를 모은 데이터셋

선호 점수를 매기지 않는 이유: 코등서 직접 매긴 데이터셋은 구축하기 어려움.

![그림 4.10 챗GPT를 학습하는 과정에서의 선호 데이터셋 구축과 리워드 모델 학습](./picture/photo_2024-09-11_23-02-40.jpg)   

### 4.2.2 강화 학습: 높은 코드 가독성 점수를 향해 
**RLHF(Reinforcement Learning from Human Feedback)** - 사람의 피드백을 이용한 강화 학습

![그림 4.11 강화 학습에서 에이전트가 학습하는 방식](./picture/photo_2024-09-11_23-02-39.jpg)   
**에이전트**는 보상을 많이 받으려고 행동을 수정하고 학습한다. 이 **보상**을 받기 위해서는 **행동**을 하여 **환경**의 **상태**를 바꿔야 한다. 이 모든 행동을 연속적으로  수행하는 행동의 모음을 **에피소드**라고 한다. 

![그림 4.12 강화 학습의 관점에서 나타낸 코딩 테스트 문제 풀이와 채점 과정](./picture/photo_2024-09-11_23-02-38.jpg)   
![그림 4.13 강화 학습의 관점에서 언어 모델의 텍스트 평가 과정](./picture/photo_2024-09-11_23-02-37.jpg) 

### 4.2.3 PPO: 보상 해킹 피하기 
문제점: 강화 학습 중 그런데 이때 보상을 높게 받는 데에만 집중하는 보상 **해킹reward hacking** 이 발생할 수 있다. 
예를 들어, 코드 가독성 점수를 높게 받는 방법으로 깔끔한 코드를 작성하는 게 아니라 아예 코드를 작성하지 않거나 print("hello world") 같은 간단한 코드만 작성해서 코드 가득성 점수를 높게 받으려고 할 수 있다.

해결법: **PPO 강화 학습 ** - 지도 미세 조정 모델을 기준으로 학습하는 모델이 너 무 멀지 않게 가까운 범위에서 리워드 모델의 높은 점수를 찾도록 한다는 의미

 ### 4.2.4 RLHF: 멋지지만 피할 수 있다면...

LHF의 어려움
- 성능이 높고 일관성 있는 리워드 모델이 필요
- 참고 모델, 학습 모델, 리워드 모델 총 3개의 모델이 필요하기 때문에 GPU와 같은 리소스가 더 많이 필요하다.
- 하리퍼파라미터에 민감하다

## 4.3 강화 학습이 꼭 필요할까?
### 4.3.1 기각 샘플링: 단순히 가장 점수가 높은 데이터를 사용한다면?

기각 샘플링 - 지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아 다시 지도 미세 조정을 수행한다. 

장점: 강화 학습을 사용하지 않기 때문에 학습 이 비교적 안정적이고 간단하고 직관적인 방법임에도 효과가 좋아 많이 활용된다

> 그림 4.16 라마-2의 학습 과정에 사용된 기각 샘플링
![그림 4.16 라마-2의 학습 과정에 사용된 기각 샘플링](./picture/photo_2024-09-11_23-02-35.jpg)   

### 4.3.2 DPO: 선호 데이터셋을 직접 학습하기

DPO 방식 : 리워드 모델이나 강화 학습을 사용하지 않고 선호 데이터셋을 직접 학습한다
DPO 방 법은 RLHF에 비해 훨씬 단순하면서 효과적이어서 많은 연구자와 개발자들이 환호했다.

![그림 4.17 DPO와 RLHF의 차이](./picture/photo_2024-09-11_23-02-50.jpg)   

### 4.3.3 DPO를 사용해 학습한 모델들

# 2부 LLM 길들이기

# 05 GPU 효율적인 학습 



## 5.1 GPU에 올라가는 데이터 살펴보기 

### 5.1.1. 딥러닝 모델의 데이터 타입

**딥러닝 모델**이 입력 데이터를 처리해 결과를 내놓을 때까지 많은 **행렬 곱셈 연산**을 처리한다.

**GPU**는 이렇게 단순한 곱셈을 동시에 여러 개 처리하는 데 특화된 처 리 장치다. 따라서 딥러닝 모델의 연산을 빠르게 처리하기 위해 GPU를 많이 활용한다.

딥러닝 모델은 과거에 32비트 부동소수점 형식을 사용했지만, 모델의 파라미터가 늘어나면서 메모리 용량과 계산 시간 문제가 발생했다. 이를 해결하기 위해 성능을 유지하면서 더 적은 비트의 데이터 타입을 사용하는 방향으로 발전했다.
 
S - 부호 sign 
E -  지수 exponent - 수를 표현할 수 있는 범위의 크기를 결정한다
M - 가수 mantissa -표현할 수 있는 수의 촘촘함을 결정한다

![그림 5.1 데이터 형식에 따른 표현 범위](./picture/photo_2024-09-11_23-02-49.jpg)   

### 5.1.2. 양자화로 모델 용량 줄이기
**양자화** - 더 적은 비트로 모델을 표현하는 기술 (예: 32bit 를 16bit으로 줄임)

모델 파라미터의 데이터 타입이 더 많은 비트를 사용할수록 모델의 용량이 커지기 때문에 필요한 기술이다.

그러나 변환하면 정보가 정보가 소실되어,  <u>양자화를 수행하면 딥 러닝 모델의 성능이 저하된다</u>. 따라서 양자화 기술에서는 더 적은 비트를 사용하면서도 원본 데이터의 정보를 최대한 소실 없이 유지하는 것이 핵심 과제라고 할 수 있다.

**퀀타일 방식** - 입력 데이터를 크기 순으로 등수를 매겨 각 데이터에 양자화 하는 방식. 

### 5.1.3 GPU 메모리 분해하기

GPU 메모리에는 다음과 같은 데이터가 저장된다.   
- 모델 파라미터
- 그레이디언트
- 옵티마이저 상태
- 순전파 상태 

딥러닝 학습 과정:
1) 순전파 수행
2) 그때 계산한 손실로부터 역전파 수행
3) 옵티마이저를 통해 모델 업데이트
이때 역전파를 수행하기 위해 저장하고 있는 값들이 순전파 상태 값이다. 그레이디언트는 역전과 결과 생성된다.

배치 크기가 증가해도 모델, 그레이디언트, 옵티마이저 상태를 저장하는 데 필요한 GPU 메모리는 동일한다. 총 메모리가 증가하는 것을 통해 순전파 상태의 계산에 필요한 메모리가 증가한다. 

## 5.2 단일 GPU 효율적으로 활용하기
### 5.2.1 그레이디언트 누적 
**그레이디언트 누적** - 메모리 제약을 해결하기 위해 제한된 메모리 안에서 배치 크기를 키우는 것과 동일한 효과를 얻는 방법 (각 모델을 풀지 않고 모델들의 틀린점을 공톰점을 찾아 그레이디언트를 하는 것)

### 5.2.2 그레이디언트 체크포인팅 
**그레이디언트 체크포인팅** - 순전파의 계산 결과를 전 저장하지 않고 중간중간에 값들을 저장해 메모리 사용량을 줄이고 필요한 경우 체크포인트부터 다시 계산해 순전파 계산량을 줄이는 방식

![그림 5.9 그레이디언트 체크포인팅 방식을 사용했을 때의 순전파와 역전파 결과 저장 ](./picture/photo_2024-09-11_23-02-48.jpg)   

## 5.3 분산 학습과 ZeRo
### 5.3.1 분산 학습 

분산 학습 - GPU를 여러 개 활용해 딥러닝 모델을 학습

**데이터 병렬화** - 여러 GPU에 각각 모델을 올리고 학습 데이터를 병렬로 처리해 학습 속도를 높이는 방식

모델 병렬화 : 모델을 여러 개의 GPU에 나눠서 올리는 방식
1)파이프라인 병렬화 - 모델의 층별로 나눈다
2) 텐서 병렬화 -  한 층의 모델도 나눈다

그림 5.11에서 모델을 그림 의 상하로 나누면(머신 1, 2와 머신 3, 4로 구분) 파이프라인 병렬화이고 좌우로 나누면(머 신 1.3과 머신 2, 4로 구분) 텐서 병렬화에 해당한다.

![그림 5.11 하나의 모델을 나눠 여러 GPU에 올리는 모델 병렬화](./picture/photo_2024-09-11_23-02-47.jpg)   

데이터 병렬화 장점: 모델을 각 GPU에 올리고 학습 데이터를 나눠 동시에 학습 해서 학습 속도를 높인다
단점: 동일한 모델을 여러 GPU에 올려 중복으로 메모리를 차지하기 때문에 메모리 관 점에서는 비효율적이다

### 5.3.2 데이터 병렬화에서 중복저장 줄이기(ZeRO)

데이터 병렬화의 단점을 극복하기 위해  ZeRO 사용. 각 GPU가 모델을 부분적으로 가지고, 필요한 순간에만 모델 파라미터를 복사해 연산을 수행하는 방식으로 메모리를 효율적으로 사용한다.

## 효율적인 학습 방법(PEFT) LoRA 
### 5.4.1 모델 파라미터의 일부만 재구성해 학습하는 LoRA 

PEFT(Parameter Efficient Fine-Tuning) : 일부 파라미터만 학습 
**LoRA** - PEFT의 일종으로 모델 파라미터를 재구성해 더 적은 파라미터를 학습함으로써 GPU 메모리 사용량을 줄인다.

LoRA를 통해 GPU 메모리 사용량이 줄어드는 부분은 바로 그레이디언트와 올티마 이저 상태를 저장하는 데 필요한 메모리가 줄어들기 때문이다.

![그릴 5.16 파라미터 재구성을 통해 더 적은 파라미터 학습](./picture/photo_2024-09-11_23-02-46.jpg)  

![ 그림 5.17 전체 미세 조정과 LORA에서 저장하는 데이터 비교](./picture/photo_2024-09-11_23-02-45.jpg)  

## 5.5 효율적인 학습 방법(PEFT): QLoRA
### 5.5.1 4비트 양자화와 2차 양자화 

QLoRA - LoRA에 양자화를 추가해 메모리 효율성을 한 번 더 높인 학습 방법

QLORA는 대형 언어 모델을 최대한 효율적으로 다룰 수 있도록 여러 기법을 적용했기 때문에 메모리 효율화를 이해하기 위한 좋은 참고 자료이기도 하다. 

기존 데이터의 분포를 알고 있다면 많은 연산이나 메모리 사용 없이도 빠르게 데이터의 순위를 정할 수 있다. 

### 5.5.2 페이지 옵티마이저
그레이디언트 체크포인팅 과정에서 발생할 수 있는 OOM에러를 방지하기 위해 페이지 옵티마이저를 활용

**페이지 옵티마이저** - 엔비디아의 통합 메모리를 통해 GPU가 CPU 메모리(RAM)를 공유하는 것

# 06 sLLM 학습하기 
## 6.1 Text2SQL 데이터셋 
### 6.1.1 대표적인 Text2SQL 데이터셋

대표적인 Text2SQL 데이터셋 : WikiSQL, Spider 

SQL을 생성하기 위해서는 크게 두 가지 데이터가 필요:
1) 어떤 데이터가 있는지 알 수 있는 데이터베이스 정보(테이블과 컬럼)
2) 어떤 데이터를 추출하고 싶은지 나타낸 요청사항(request, question)

### 6.1.3 합성 데이터 활용

생성한 데이터셋은 db_ld, context, question, answer 4개의 컬럼으로 구성돼 있다.
- db_id : 테이블이 포함된 데이터베이스 아이디
- context : SQL 생성에 사용할 테이블정보
- question : 데이터 요청사항
- answer : 요청에 대한 SQL 정답 

## 6.2 성능 평가 파이프라인 준비하기 
### 6.2.1 Text2SQL 평가 방식 


일방적으로 사용하는 성능평가 방법:
- EM(Exact Match) 방식 : 생성한 SQL이 문자열 그대로 동일한지 확인하는 방식
- EX(Execution Accuracy) 방식 : 쿼리를 수행할 수 있는 데이터베이스를 만들고 프로그래밍 방식으로 SQL 쿼리를 수행해 정답과 일치하는지 확인하는 방식

최근에는 LLM을 활요해 LLM의 생성 결과를 평가하는 방식이 활발히 연구되고 있다

![그림 6.5 LLM을 활용한 Text2SQL 평가 방식](./picture/photo_2024-09-11_23-02-44.jpg)

### SQL 생성 프롬프트

>SQL 프롬프트 데이터 예시

```
#학습 데이터 예시

***당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question SQL 허리를 생성하세요.

DOL:

CREATE TABLE messages (

);

"message id" SERIAL PRIMARY KEY,

*conversation_id" INT NOT NULL,

"sender_ld" INT NOT NULL,

"content" TEXT,

"timestamp" TIMESTAMP WITH TIME ZONE DEFAULT CURRENT TIMESTAMP, "read" BOOLEAN DEFAULT FALSE,

FOREIGN KEY ("conversation_id") REFERENCES conversations("conversation_id"), FOREIGN KEY ("sender_id") REFERENCES users("user_id")

### Question:

messages 테이블에서 모든 데이터를 조회해 줘

###SQL:

SELECT FROM messages;"*"

#생성 프롬프트 예시

**"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.
### DDL:

CREATE TABLE messages (

"message id" SERIAL PRIMARY KEY, "conversation td" INT NOT NULL,

"sender (d" INT NOT NULL,

"content" TEXT,

"timestamp" TIMESTAMP WITH TIME ZONE DEFAULT CURRENT TIMESTAMP,

"read" BOOLEAN DEFAULT FALSE, FOREIGN KEY ("conversation td") REFERENCES conversations("conversation_td"), FOREIGN KEY ("sender (d") REFERENCES users("user_(d")

###Question:

messages 테이블에서 모든 데이터를 조회해 줘

###SQL:
"""
```
### GPT-4 평가 프롬프트와 코드 준비
GPT-4 API를 통해 평가 데이터셋으로를 수행하면 시간이 오래 걸려 OpenAI의 비동기 요청 코드(api_request_parallel_processor.py)를 활용해 요청 제한을 관리하며 처리 속도를 높일 수 있다. 이 코드는 jsonl 파일을 읽어 순차적으로 요청을 보내고, 에러나 제한에 걸리면 다시 요청을 보내 결과 누락을 방지한다.

> 예제 6.4. 평가를 위한 요청 jsonl 작성 함수


```
import json
import pandas as pd
from pathlib import Path

def make_requests_for_gpt_evaluation(df, filename, dir='requests'):
  if not Path(dir).exists():
      Path(dir).mkdir(parents=True)
  prompts = []
  for idx, row in df.iterrows():
      prompts.append("""Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return "yes" else return "no". Output JSON Format: {"resolve_yn": ""}""" + f"""

DDL: {row['context']}
Question: {row['question']}
gt_sql: {row['answer']}
gen_sql: {row['gen_sql']}"""
)

  jobs = [{"model": "gpt-4-turbo-preview", "response_format" : { "type": "json_object" }, "messages": [{"role": "system", "content": prompt}]} for prompt in prompts]
  with open(Path(dir, filename), "w") as f:
      for job in jobs:
          json_string = json.dumps(job)
          f.write(json_string + "\n")
```


## 6.3 실습: 미세조정 수행하기 
### 6.3.1 기초 모델 평가하기

> 예제 6.7. 기초 모델로 생성하기


```
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

def make_inference_pipeline(model_id):
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
  pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
  return pipe

model_id = 'beomi/Yi-Ko-6B'
hf_pipe = make_inference_pipeline(model_id)

example = """당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.

### DDL:
CREATE TABLE players (
  player_id INT PRIMARY KEY AUTO_INCREMENT,
  username VARCHAR(255) UNIQUE NOT NULL,
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  date_joined DATETIME NOT NULL,
  last_login DATETIME
);

### Question:
사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요.

### SQL:
"""

hf_pipe(example, do_sample=False,
    return_full_text=False, max_length=512, truncation=True)
#  SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';

# ### SQL 봇:
# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';

# ### SQL 봇의 결과:
# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%'; (생략)
```

### 6.3.2 미세 조정 수행
> 예제 6.10. 미세 조정 명령어

```
base_model = 'beomi/Yi-Ko-6B'
finetuned_model = 'yi-ko-6b-text2sql'

!autotrain llm \
--train \
--model {base_model} \
--project-name {finetuned_model} \
--data-path data/ \
--text-column text \
--lr 2e-4 \
--batch-size 8 \
--epochs 1 \
--block-size 1024 \
--warmup-ratio 0.1 \
--lora-r 16 \
--lora-alpha 32 \
--lora-dropout 0.05 \
--weight-decay 0.01 \
--gradient-accumulation 8 \
--mixed-precision fp16 \
--use-peft \
--quantization int4 \
--trainer sft
```
### 6.3.3 학습 데이터 정제와 미세정제

학습을 통해 사용하는 데이터셋이 커지면서 성능이 함께 향상되는 것을 확인할 수 있다. 이 결과를 통해 학습 데이터 정제 과정에서 데이터셋이 줄었음에도 성능이 유지 되는 것이 정제의 긍정적인 효과임을 짐작해 볼 수 있다.

# 모델 가볍게 만들기

## 7.1 언어 모델 추론 이해하기

### 7.1.1 언어 모델이 언어를 생성하는 방법

언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하고 그중에서 가장 확률이 높은 토큰을 입력 텍스트에 추가하면서 한 토큰씩 생성한다. 이 방식을 반복한다.

언어 모델이 텍스트 생성을 마치는 이유로 첫 번쨰, 생성 종료를 의미하는 특수 토큰(예: EOSEnd Of Sentence 토큰)을 생성하는 경우 생성을 종료한다. 두 번쨰로 사용자가 최대 길이로 설정 한 길이에 도달하면 더 이상 생성하지 않고 종료한다. 

토큰은 바로 다음의 토큰만 예측할 수 있다. 이를 자기희기적 특성이라고 부른다. 

추론 과정:
- 사전 계산 단계(prefill phase): 프롬프트를 처리하는 단계
- 디코딩 단계(decoding phase): 이후 한 토큰씩 생성하는 단계

셀프 어텐션처럼 똑같은 입력토큰을 반복해서 수행하지않기 위해 KV캐시에 대해 알아야 한다.

### 7.1.2 중복 연산을 줄이는 KV 캐시

**KV 캐시**는 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법. 키와 값 계산 결과를 저장하기 때문에 'KVKey-Value' 캐시라고 한다.

KV 캐시 없이는 다음 토큰을 예측하는 것 마다 이전에 수행했던 토큰을 키와 값 벡터로 변환하는 동일한 연산을 반복해야 한다.

![그림 7.4 KV 캐시를 사용할 때와 사용하지 않을 때의 차이](photo_2024-09-18_22-21-36.jpg)

KV 캐시 메모리 사용량 계산식

``KV 캐시 메모리 = 2바이트×2(키와 값) × (레이어 수) × (토큰 임베딩 차원)×(최 대 시퀀스 길이)×(배치 크기)

### 7.1.3 GPU 구조와 최적의 배치 크기

서빙이 효율적인지 판단하는 큰 기준:
1) 비용
2) 처리량 (throughput) - 시간당 처리한 요청 수(query/s)
3) 지연 시 간(latency) 하나의 토큰을 생성하는 데 걸리는 시간(token/s)

**적은 비용**으로 **더 많은 요청** 을 처리하면서 생성한 다음 **토큰을 빠르게 전달**할 수 있다면 효율적인 서빙이라고 할 수 있다.

- 스트리밍 멀티프로세서 Streaming Multiprocessors(이하 SM)
- SRAM (Static Random Access Memory) - 각각의 SM에는 연산을 수행하는 부분과 계산할 값을 저장
- 큰 고대역폭 메모리 High Bandwidth Memory - 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어렵기 때문에 여기에 큰 데이터를 저장한다. 

또한 메모리를 이동시키는 것도 시간을 잡아먹는다. 모델 이동 과정과 연산수행 과정은 함께 진행되기 때문에 두 가지 시간이 같을 때가 **최적의 배치 크기**가 된다. 

**메모리 바운드(memory bound)** - 최적의 배치 크기보다 배치 크기가 작으면 모델 파라미터를 이동시키느라 연산이 멈추는 비효율

**연산 바운드(compute bound)** - 크기가 최적 크기보다 더 커지면 연산에 오랜 시간이 걸리기 때문에 지연 시간이 길어 지는 비효율

최적의 배치 크기 계산식:

`` 2×P×배치 크기/하드웨어 연산 속도 = P / 메모리 대역폭``

`` 배치 크기 = 하드웨어 연산 속도/(2×메모리 대역폭) = (312×10¹²) / (2x 1555×10°) = 102.73``

### 7.1.4 KV 캐시 메모리 줄이기

멀티 헤드 어텐션을 할 시 많은 수의 키와 값 벡터를 저장하기 때문에 KV 캐시에 더 많 은 메모리를 사용하고 KV 캐시에서 더 많은 데이터를 불러와 계산하기 때문에 그만큼 속도가 느려진다.

이를 해결하기 위해 멀티 **쿼리 어텐션**(모든 쿼리 벡터가 하나의 키와 값 벡터를 공유)를 사용한다

하지만 이도 멀티 쿼리 어텐션은 키와 값 벡터를 공유하므로 메모리 사용량은 줄어들지만, 멀티 헤드 어텐션에 비해 성능이 떨어질 수 있습니다.

그래서 **그룹 쿼리 어텐션**(grouped-query attention, GQA)을 개발했다.

**그룹 쿼리 어텐션**은 2개의 쿼리 벡터당 1개의 키와 값 벡터를 사용해 결과적으로 4개의 키와 값을 사용한다. 그룹 쿼리 어텐션은 사용하는 키와 값 벡터의 수를 줄임으로써 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 메모리 사용량을 줄일 수 있었다.

![그림 7.11 멀티 헤드 어텐션, 그룹 쿼리 어텐션, 멀티 쿼리 어텐션 비교](photo_2024-09-18_22-21-39.jpg)

## 12 양자화로 모델 용량 줄이기

**양자화**란 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU 를 효율적으로 사용하는 방법

양자화:
- **학습 후 양자화** (Post-Training Quantization, PTQ) 
- **양자화 학습** (Quantization-Aware Training, QAT) 

주로  학습 후 양자화를 활용한다.

학습 후 양자화 방식:

- 비츠앤바이츠 bits-and-bytes

- GPTQ GPT Quantization

- AWQ Activation-aware Weight Quantization

현재 활발히 연구되고 있는 분야이므로 언제든지 새로운 방식이 추가될 수 있다

### 7.2.1 비츠앤바이츠

 **비츠앤바이츠** - 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리다.

1) 8비트 행렬 연산 - 8비트로 연산을 수행하면서도 성능 저하가 거의 없다
2) 4비트 정규 분포 양자화 - 4비트로  정규 분포를 따르는 값들을 효율적으로 압축

![그림 7.15 비츠앤바이츠의 8비트 양자화 방식](photo_2024-09-18_22-21-40.jpg)   

16비트 이상인 큰 값들은 그대로 계산한다. (중요한 정보를 갖고 있다고 예상하기 떄문)그 외에 정상 범위 의 있는 열은 양자화 할 때 벡터 단위로 절대 최대 값을 찾고 그 값을 기준으로 양자화 한다.  

### 7.2.2 GPTQ

**GPTQ**는 양자화 이전의 모델에 입력 X를 넣었을 때와 양자화 이후의 모델에 입력 X를 넣었을 때 오차가 가장 작아지도록 모델의 양자화를 수행한다. 직관적으로 봤을 때 양자화 전과 후의 결과 차이가 작다면 훌륭한 양자화라고 볼 수 있다.

![그림 7.16 GPTQ 양자화 진행 과정](photo_2024-09-18_22-21-41.jpg)

GPTQ의 양자화 과정은 전체 가중치 행렬에서 흰색으로 표시된 열을 양자화하고, 그 결과가 이전 상태와 최대한 가까워지도록 오른쪽 열의 파라미터를 업데이트하는 방식으로 진행된다. 블록은 오른쪽으로 이동하면서 점차 전체 모델의 파라미터를 업데이트한다.

### 7.2.3 AWQ

AWQ - 중요한 파라미터의 정보를 유지하며 수행하는 양자화 방식

중요한 파라미터란
- 모델 파라미터 값이 크다
- 데이터 활성화 값이 큰 채널의 파라미터

상위 1% 파라미터만 FP16으로 유지하고 나머지를 양자화 하였을 떄 성능이 거의 떨어지지 않았다. 그러나 모델 파라미터에 서로 다른 데이터 타입이 섞여 있는 경우 한 번에 일괄적으로 연산하기 어렵기 때문에 연산이 느려지고 하드웨어 효율성이 떨어지는 문제가 발생한다. 

![그림 7.19 양자화 과정에서 중요한 정보의 손실](photo_2024-09-18_22-21-43.jpg)

위 그림처럼 파라미터를 양자화 할 시 이전에는 서로 다른 값이었는데 양자화를 수행하면서 값이 같아지고 중요한 정보가 소실될 수 있다.

그래서 이를 막기 위해 붕요 파라미터만 1보다 큰 값을 곱하는 방식으로 이 문제를 해결했다. 이때 곱해 주는 값을 스케일러scaler라고 부른다.

![그림 7.20 스케일러를 곱했을 때 중요한 정보의 소실을 막음](photo_2024-09-18_22-21-44.jpg)

## 7.3 지식 증류 활용하기

**지식 증류** (knowledge distillation)란 더 크고 성능이 높은 선생 모델(teacher model)의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델(student model)을 만드는 방법

지식 증류는 학생 모델이 선생 모델을 모방하는 것을 넘어서, 새로운 데이터셋 구축이나 개발 속도 향상 등 다양한 방식으로 활용을 한다.

# sLLM 서빙하기

## 8.1 효율적 배치 전략

### 8.1.1 일반 배치(정적 배치)

**일반 배치** - 입력 데이터를 배치 처리할 때, 가장 기본적인 방식은 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식

일반 배치의 문제점:
- 입력 중 먼저 생성이 종류 된 입력은 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 대기하게 된다. GPU를 효율적으로 사용 못한다.
- 처음에는 모든 문장에 대해 수행하게 되지만 문장의 길이 차이로 인해 추론을 끝낸 문장이 생기고 추론하는 문장이 적어져 배치 크기가 작아지고 GPU를 비효율적으로 사용하게 된다.   

### 8.1.2 동적 배치

**동적 배치**dynamic batching는 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략이다.

![그림 8.2 일정한 시간 동안의 요청을 묶어서 처리하는 동적 배치 전략](photo_2024-09-18_22-21-45.jpg)

### 8.1.3 연속 배치

**연속 배치**continuous batching는 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가한다. 

![그림 8.3 생성이 끝나면 새로운 문장을 추가하는 연속 배치 전략](photo_2024-09-18_22-21-46.jpg)

생성이 끝나고도 다른 문장의 생성이 끝나길 기다리면서 대기 시간이 길어지는 문제를 줄이고 배치 크기가 줄면서 GPU를 비효율적으로 사용하는 문제도 해결할 수 있다.

## 8.2 효율적인 트랜스포머 연산

### 8.2.1 플래시어텐션

**플래시어텐션** FlashAttention - 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발 됐다. 플래시어텐션은 어텐션 연산 과정을 변경해 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하도록 개선했다. 

> 셀프 어텐션 연산에 드는 시간

![그림 8.5 셀프 어텐션의 각 연산에 걸리는 시간](photo_2024-09-18_22-21-48.jpg)

마스크, 소프트맥스, 드롭아웃 처리에 드는 시간이 행렬 곱셈에 드는 시간보다 더 길다. 하지만 연산량 자체는 행렬 곱셈이 다른 연산에 비해 훨씬 크다.

플래시어텐션에서는 데이터 이동 속도가 느린 고대역포 메모리에 큰 어텐션 행렬을 쓰고 읽으면서 걸리는 시간을 줄이기 위해 블록 단위로 어텐션 연산을 수행하고 전체 어텐션 행렬을 쓰거나 읽지 않는 방식으로 속도를 높였다. 또 작은 블록 단위로 연산을 수행하기 때문에 SRAM에 데이터를 읽고 쓰면서 더 빠르게 연산을 수행한다.

![그림 8.7 플래시어텐션에서 어텐션 연산 과정을 개선한 방식](photo_2024-09-18_22-33-14.jpg)

### 8.2.2 플래시어텐션 2

**플래시어텐션 2**는 플래시어텐션을 개선해 2배 정도 속도를 향상했다.

개선한 부분:
-행렬 곱셈이 아닌 연산 줄이기
- 시퀀스 길이 방향의 병렬화 추가

### 8.2.3 상대적 위치 인코딩

사인파 위치 인코딩 - 최초의 트랜스포머 아키텍처에 쓰인 위치 인코딩. 수식에 따라 정해지는 값을 위치 임베딩 값으로 더해준다.

절대적 위치 인코딩의 한계점인 학습 데이터보다 더 긴 입력이 들어오면 언어 모델의 생성 품질이 빠르게 떨어지는 것을 상대적 위치 인코딩이 대신해준다.

상대적 위치 인코딩 - 절대적인 위치에 따라 임베딩을 더하는 것이 아니라 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 인코딩.

대표적인 상대적 위치 인코딩 방식:
- RoPE: 각각의 토큰 임베딩을 토큰 위치에 회전시킨다. 토큰 사이의 위치 정보가 두 임베딩 사이의 각도를 통해 모델에 반영된다.

![그림 8.18 RoPE의 상대적 위치 인코딩 방식](photo_2024-09-18_22-33-15.jpg)

- ALiBI - 쿼리, 키, 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식

![그림 8.19 ALiBi의 상대적 위치 인코딩 방식](photo_2024-09-18_22-33-17.jpg)

>오른쪽의 행렬은 현재 쿼리의 위치 를 의미하는 0을 기준으로 앞에 있을수록 더 작은 값을 더해 상대적인 위치를 나타낸다고 볼 수 있다. 

## 8.3 효율적인 추론 전략

### 8.3.1 커널 퓨전

GPU 연산은 커널 단위로 수행되며, 연산을 수행하기 위해서 전후에 추가적인 작업을 위한 오버헤드가(ex: 고대역폭 메모리에서 데이터를 읽어오고 연산 결과를 쓰는 작업) 발생한다. 여러 커널을 사용하는 경우 오버헤드가 증가하여 연산 시간이 길어질 수 있으므로, **커널 퓨전**(kernel fusion)을 통해 연산을 하나로 묶어 오버헤드를 줄이고 효율성을 높일 수 있다.

### 8.3.2 페이지어텐션

KV의 단점인 GPU의 메모리를 많이 차지하는 현상을 **페이지 어텐션**과 함께 사용하면 보완 할 수 있다.
페이지어텐션 - 운영체제의 가상 메모리 개념을 빌려와 중간에서 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 관리해서 실제로는 물리적으로 연속된 메모리를 사용하지 않으면서도 논리적 메모리에서는 서로 연속적이도록 만들었다.

또한 다양한 디코딩 방식에서 메모리를 절약할 수 있는데 동일한 입력 프롬프트에서 여러 개의 출력을 생성하는 병렬 샘플링에서 입력 프롬프트에 대한 메모리를 공유함으로써 메모리를 절약한다.

참조 카운트 - 물리적 블록을 공유하고 있는 논리적 블록 수를 의미

### 8.3.3 추측 디코딩

**추측 디코딩** speculative decoding - 쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식


디코딩은 작은 **드래프트 모델**draft model과 큰 **타깃 모델**target model 이라는 2개의 모델을 사용해 추론을 수행한다. 


![그림 8.28 추측 디코딩 방식의 추론 과정](photo_2024-09-18_22-33-19.jpg)

먼저 드래프트 모델이 토큰을 생산 한다. 그 후 타깃 모델과 비교하여 동일하다면 승인하고 동일하지 않다면 거절한다.

## 8.4 실습: LLM 서빙 프레임워크

### 8.4.1 오프라인 서빙

**오프라인 서빙**이란 정해진 입력 데이터에 대해 배치 추론을 수행한다.

### 8.4.2 온라인 서빙

**온라인 추론**이란 사용자의 요청에 따라 모델 추론을 수행한다.

대표적인 LLM 추론 프레임워크인 vLLM은 허깅페이스 라이브러리를 활용한 추론 대비 더 빠른 오프라인 추론이 가능하고 온라인 서빙에 사용할 수 있는 편리한 API 서버를 제공한다.

# LLM 애플리케이션 개발하기

## 9.1 검색 증강 생성(RAG)

LLM의 답변의 근거나 출처가 불명확하고 부정확한 정보를 지어내는 환각 현상을 해결 하기 위해 **RAG**라는 기법이 활용된다. **검색 증강 생성**이란, LLM에게 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법을 말한다.

이때 답변에 필요한 정보를 '검색retrieval 을 통해 선택하기 때문에 '검색을 통해 보충한 생성'이라는 의미로 붙은 이름이다.

### 9.1.1 데이터 저장

![그림 9.3 데이터 저장 작업 과정](photo_2024-09-18_22-33-20.jpg)

> 데이터 소스의 텍스트를 임베딩 모델을 사용해 임베딩 벡터로 변환한다. 변환한 임베딩 벡터는 벡터 사이의 거리를 기준으로 검색하는 특수한 데이터베이스인 벡터 데이터베이스에 저장한다.

벡터 데이터베이스는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공한다.

특정 문장(검색 쿼리)으로 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리도 벡터로 변환해 벡터 데이터베이스에서 위치를 찾고 쿼리 임베 딩과 가장 가까운 벡터를 찾는다. 

### 9.1.2 프롬프트에 검색 결과 통합
LLM은 결과를 생성할 때 프롬프트만 입력으로 받기 때문에 사용자가 앞서 저장한 켁스트를 LLM에게 전달하기 위해서 요청과 관련이 큰 문서를 벡터 데이터베이스에서 찾고 검색 결과를 프롬프트에 통합해야 한다.

![그림 9.6 검색 결과를 프롬프트에 통합](photo_2024-09-18_22-33-21.jpg)

## 9.2 LLM 캐시
### 9.2.1 LLM 캐시 작동 원리

**LLM 캐시**는 추론을 수행할 때 사용자의 요청과 생성 결과를 기록하고 이후에 동일하거나 비슷한 요청이 들어오면 새롭게 텍스트를 생성하지 않고 이전의 생성 결과를 가져와 바로 응답한다

LLM 캐시를 사용하지 않는다면 통합한 프롬프트를 바로 LLM에 전달하고 결과를 생성한다. 하지만 LLM 캐시를 사용한다면 캐시 요청을 통해 이전에 동일하거나 유사한 요청이 있었는지 확인하고 만약 있었다면 LLM 캐시에 저장된 답변을 전달하고 없었다면 LLM에 프롬프트를 전달해 새롭게 텍스트를 생성해서 LLM 오케스트레이션 도구로 전달한다.

![그림 9.12 프롬프트 통합과 LLM 사이에 LLM 캐시 확인 과정 추가](photo_2024-09-18_22-33-22.jpg)

LLM 캐시:
_ 일치 캐시: 요청이 완전히 일치 할 경우 저장된 응답을 발휘한다.
- 유사 검색 캐시: 유사한 요청이 있는지 확인하고 만약 있다면 저장된 텍스츠를 반환한다. 만약 없다면 LLM으로 새롭게 텍스트를 생성해 응답하면서 벡터 데이터베이스에 요청의 임베딩 벡터와 생성 결과를 저장한다.

### 9.2.2 실습: OpenAI API 캐시 구현

## 9.3 데이터 검증

### 9.3.1 데이터 검증 방식

**데이터 검증**이란, 벡터 검색 결과나 LLM 생성 결과에 포함되지 않아야 하는 데이터를 필터링하고 답변을 피해야 하는 요청을 선별함으로써 LLM 애플리케이션이 생성한 텍스트로 인해 생길 수 있는 문제를 줄이는 방법

- 규칙 기반: 문자열 매칭이나 정규 표현식을 활용해 데이터를 확인
- 분류 또는 회귀 모델: 명확한 문자열 패턴이 없는 경우 별도의 분류 또는 희귀 모델을 만들어 활용
- 임베딩 유사도 기반: 민담한 의견을 물었을 떄 임베딩과 유사할 떄 답변을 피할 수 있다.
- LLM 활용: LLM을 활용해 텍스트 내의 부적절한 내용이 섞여 있어 적절하지 않은 경우 이를 다시 생성하거나 삭제

### 9.3.2 데이터 검증 실습

## 9.4 데이터 로깅
**데이터 로깅** - 사용자의 입력과 LLM이 생성한 출력을 기록한다
LLM의 경우 입력이 동일해도 출력이 달라질 수 있기 때문에 어떤 입력에서 어떤 출력을 반환했는지 반드시 기록해야 하기 때문에 데이터 로깅이 필요하다.

대표적인 로깅 도구 중 하나인 W&B(Weight adn Bias)에서 제공하는 Trace 기능을 활용하면 요청과 응답을 기록하고 확인 할 수 있다.

### 9.4.1 OpenAl API 로깅

대표적인 상업용 LLM API인 OpenAI의 API를 로깅하기 위해서는 예제 9.16과 같은 코 드를 사용하면 된다.

> 예제 9.16 OpenAI API 로깅하기
```
import datetime
from openai import OpenAI
from wandb.sdk.data_types.trace_tree import Trace

client = OpenAI()
system_message = "You are a helpful assistant."
query = "대한민국의 수도는 어디야?"
temperature = 0.2
model_name = "gpt-3.5-turbo"

response = client.chat.completions.create(model=model_name,
                                        messages=[{"role": "system", "content": system_message},{"role": "user", "content": query}],
                                        temperature=temperature
                                        )

root_span = Trace(
      name="root_span",
      kind="llm",
      status_code="success",
      status_message=None,
      metadata={"temperature": temperature,
                "token_usage": dict(response.usage),
                "model_name": model_name},
      inputs={"system_prompt": system_message, "query": query},
      outputs={"response": response.choices[0].message.content},
      )

root_span.log(name="openai_trace")
```

### 9.4.2 라마인덱스 로깅


>예제 9.17 라마인덱스 W&B 로깅


```
from datasets import load_dataset
import llama_index
from llama_index.core import Document, VectorStoreIndex, ServiceContext
from llama_index.llms.openai import OpenAI
from llama_index.core import set_global_handler
# 로깅을 위한 설정 추가
llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
set_global_handler("wandb", run_args={"project": "llamaindex"})
wandb_callback = llama_index.core.global_handler
service_context = ServiceContext.from_defaults(llm=llm)

dataset = load_dataset('klue', 'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

print(dataset[0]['question']) # 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?

query_engine = index.as_query_engine(similarity_top_k=1, verbose=True)
response = query_engine.query(
    dataset[0]['question']
)
```

# 10 임베딩 모델로 데이터 의미 함축하기

데이터의 의미를 함축 해 숫자로 표현하는 것은 오랜 기간 데이터 분야에 과제였다. 이를 해결하기 위해 어떤 다양한 시도를 했었는지 알아본다.

## 10.1 텍스트 임베딩 이해하기

**임베딩** (embedding)이란 '데이터의 의미를 압축한 숫자 배열(벡터)'을 말한다.


**텍스트 임베딩** 또는 **문장 임베딩** - 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식. 

### 10.1.1 문장 임베딩 방식의 장점

문장 임베딩 방식을 사용하면 단어나 문장 사이에 관계를 계산 할 수 있다.

데이터를 숫자로 변환하기 위해 원핫 인코딩을 활발히 사용한다.

### 10.1.2 원핫 인코딩

원핫 인코딩은 다음 예시와 같이 표현한다.
>학교 - 1, 공부 - 2, 운동 - 3

↓

> 학교 - [1,0,0], 공부 - [0,1,0], 운동 - [0,0,1]

원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만, 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 단점이 있다. 

### 10.1.3 백오브워즈
**백오브워즈** 는 '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환한다. 백오브워즈는 단어의 순서에 관계없이 해당 문서에 등장한 단어와 그 등장 횟수를 집계한다.

백오브워즈의 단점: 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는 데 크게 도움이 되지 않는 경우가 있다는 점이다. (ex: 한국어 은/는, 이가; AI)

### 10.1.4 TF-IDF
앞서 단점을 보완 하기 위해 TF-IDF는 다음 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만든다.

> TF-IDF(w) = TF(w) x log(N/DF(w))

- TF(w): 특정 문서에서 특정 단어 w가 등장한 횟수
- DF(w): 특정 단어 w가 등장한 문서의 수

그러나 만약 100000 차원에 벡터를 사용하게 되면 이러한 경우 대부분이 0인 벡터가 되어 이를 '희소(sparse)'하다고 한다. **희소한 벡터**는 의미를 '압축'해서 담고 있지 못하기 때문에 벡터 사이의 관계를 활용하기 어렵다. 이를 대비하기 위해 **밀집 임베딩**(dense embedding)을 사용한다.

### 10.1.5 워드투벡

**워드투벡**word2vec은 단어가 '함께 등장하는 빈도' 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법. 특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델이 생성할 수 있다

![그림 10.1 워드투벡의 두 가지 학습 방식](photo_2024-09-18_22-33-23.jpg)

- CBOW: 주변 단어로 가운데 단어를 예측하는 방식
- 스킵그램: 중간 단어로 주변 단어를 예측하는 방식

## 10.2 문장 임베딩 방식

### 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법

**문장 임베딩**: 텍스트를 밀집 벡터로 표현하여 문장 간 유사도나 관련성을 쉽게 계산할 수 있게 해준다. BERT 모델은 이러한 문장 임베딩 변환에서 뛰어난 성능을 보인다.

BERT 모델을 사용해 문장 관계 계산 방법:
- **바이 인코더** (bi-encoder): 두 문장을 독립적으로 BERT 모델에 입력하고, 출력된 문장 임베딩 벡터 간의 유사도를 별도로 계산한다.

- **교차 인코더** (cross-encoder): 두 문장을(쿼리 문장, 검색 대상 문장) 함께 BERT 모델에 입력하여 모델이 직접 문장 간의 유사도를 계산한다. 0-1사이 값으로 출력한다. 이 방법은 계산량이 많지만 더 정확한 관계 예측이 가능하다. 하지만 입력으로 넣은 두 문장의 유사도만 계산하기 때문에 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 연산을 반복해야 한다는 단점이 있다.

![그림 10.3 문장 사이 관계를 계산하는 두 가지 방법](photo_2024-09-18_22-33-24.jpg)

바이 인코더가 교차 인코더를 해결하는 방법:
1) 검색 쿼리 문장과 검색 대상 문장을 독립적으로 모델에 입력하여 각각의 문장 임베딩을 생성한다.
2) 생성된 문장 임베딩을 사용해 유사도를 계산하며, 이렇게 하면 교차 인코더와 달리 바이 인코더는 각 문장의 독립적인 임베딩을 결과로 반환 하기 때문에 유사도를 계산하고 싶은 문장이 바뀌더라도 추가적인 BERT 연산 없이 문장 유사도 계산이 가능하다.

### 10.2.2 바이 인코더 모델 구조

바이 인코더에서는 BERT 모델은 입력 문장의 각 토큰에 대해 출력 임베딩을 생성한다. 문장 길이에 따라 임베딩 수가 달라질 수 있다. 문장의 길이가 다를 때 서로 다른 개수의 임베딩이 반환된다면, 문장과 문장 사이의 유사도를 쉽게 계산하기 어렵다.

풀링 층의 역할: 문장 길이가 달라도 동일한 크기의 고정된 임베딩으로 통합해준다. 이는 문장을 대표하는 1개의 임베딩을 통합되며, 문장 간 유사도를 쉽게 계산할 수 있게 한다.

> 예제 10.3 Sentence-Transformers 라이브러리로 바이 인코더 생성하기


```
from sentence_transformers import SentenceTransformer, models
# 사용할 BERT 모델
word_embedding_model = models.Transformer('klue/roberta-base')
# 풀링 층 차원 입력하기
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
# 두 모듈 결합하기
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
```

**폴링 모드**란, 언어 모델이 출력한 결과 임베딩을 고정된 크기의 문장 임베딩으로 통합 할 때 통합하는 방식

세가지 방식이있다:
- **클래스 모드**(pooling_mode_cls_tokens): BERT 모델의 첫 번째 토큰인 [CLS] 토큰의 출력 임베딩을 문장 임베딩으로 사용한다.
- **평균 모드**(pooling_mode_mean_tokens): BERT 모델에서 모든 입력 토큰의 출력 임베딩을 평균한 값을 문장 임베딩으로 사용한다
- **최대 모드**(pooling_mode_max_tokens): BERT 모델의 모든 입력 토큰의 출력 임베딩에서 문장 길이sequence 방향에서 최댓값을 찾아 문장 임베딩으로 사용한다.

세 가지 풀링 모드 중에서는 평균 모드를 일반적으로 많이 활용한다.

### 10.2.3 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해 보기

**Sentence-Transformers 라이브러리**: 허깅페이스 모델을 불러와 텍스트와 이미지 모델을 쉽게 사용할 수 있다.

문장 임베딩 예시:
1) snunlp/KR-SBERT-V40K-klueNLI-augSTS 모델을 사용하여 한국어 문장을 임베딩으로 변환한다.
2) 예시에서는 "잠이 안 옵니다", "졸음이 옵니다", "기차가 옵니다"라는 세 문장을 입력한다.
3) util.cos_sim 함수를 통해 문장 임베딩 사이의 코사인 유사도를 계산한다.
4) 유사도 결과: "잠"과 "졸음" 사이의 유사도는 0.641로 높고, "잠"과 "기차"는 0.1887로 낮으며, "졸음"과 "기차"는 0.273으로 상대적으로 낮다.

> 예제 10.6 한국어 문장 임베딩 모델로 입력 문장 사이의 유사도 계산


```
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

embs = model.encode(['잠이 안 옵니다',
                     '졸음이 옵니다',
                     '기차가 옵니다'])

cos_scores = util.cos_sim(embs, embs)
print(cos_scores)
# tensor([[1.0000, 0.6410, 0.1887],
#         [0.6410, 1.0000, 0.2730],
#         [0.1887, 0.2730, 1.0000]])
```

이 방법을 이미지에서도 이미지 임베딩으로 쉡게 변환 가능하다. 

> 예제 10.7 CLIP 모델을 활용한 이미지와 텍스트 임베딩 유사도 계산


```
from PIL import Image
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('clip-ViT-B-32')

img_embs = model.encode([Image.open('dog.jpg'), Image.open('cat.jpg')])
text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])

cos_scores = util.cos_sim(img_embs, text_embs)
print(cos_scores)
# tensor([[0.2771, 0.1509],
#         [0.2071, 0.3180]])
```

### 10.2.4 오픈소스와 상업용 임베딩 모델 비교하기

임베딩 모델을 활용하는 방법:
- **상업용 모델**: 대량의 데이터로 학습된 만큼 성능이 뛰어나고 LLM 텍스트 생성에 비해 훨씬 낮은 비용으로 사용이 가능하다. 그러나 일부 생성 모델에는 임베딩 모델에 대한 미세 조정 기능이 없어 사용자가 자신의 데이터에 특화된 임베딩 모델을 만들 수 없다.
- **오픈소스 모델**: 자신의 데이터에 맞춰 미세 조정을 수행할 수 있다

## 10.3 실습: 의미 검색 구현하기

**의미 검색**이란, 단순히 키워드 매칭을 통한 검색이 아니라 밀집 임베딩을 이용해 문장이나 문서의 의미를 고려한 검색을 수행하는 것을 말한다. 

실습에 사용할 faiss 라이브러리는 메타(구 페이스북)에서 개발한 벡터 연산 라이브러리다.

### 10.3.1 의미 검색 구현하기
### 10.3.2 라마인덱스에서 Sentence-Transformers 모델 사용하기

## 10.4 검색 방식을 조합해 성능 높이기

키워드 검색 - 동일한 키워드가 많이 포함 될수록 유사도를 높게 평가하는 검색 방식

장점: 키워드 검색은 동일한 키워드가 등장한 문서를 상위 검색 결과로 반환하기 때문에 관련성이 떨어지는 검색 결과가 나타날 가능성이 낮다

단점: 동일한 키워드를 사용하지 않으면 의미가 유사하더라도 검색하지 못한다


### 10.4.1 키워드 검색 방식: BM25

**BM25**는 TF-IDF와 유사한 통계 기반 스코어링 방법으로, TF-IDF에 문서의 길이에 대한 가중치를 추가한 알고리즘이다.

![그림 10.13 BM25 수식](photo_2024-09-18_22-33-26.jpg)

IDF(qi) - 문서 빈도 부분, 특정 단어가 전제 문서에 얼마나 등장 했는 지 알 수 있다.
TF - 단어 빈도,특정 문서(D) 내의서 단어가 얼마나 등장했는지 나타낸다.

![그림 10.14 BM25의 IDF 계산식](photo_2024-09-18_22-33-27.jpg)

n(qi) -  쿼리 단어의 토큰이 등장하는 문서의 수

N - 전체 문서의 수

만약 모든 문서에 등장하는 토큰이라면 IDF 부분은 최솟값이 되고 적게 등장해 n(qi) 가 작아지면 IDF 부분의 값은 커진다. 직관적으로 설명하면, 여러 문서에 등장하는 단어라면 덜 중요한 단어이기 때문에 값이 작아지도록 한 것이다.

### 10.4.2 상호 순위 조합 이해하기

**하이브리드 검색**을 위해서는 통계 기반 점수와 임베딩 유사도 점수를 결합할 때, 각 점수의 분포 차이로 인해 직접 합치면 한 점수의 영향이 더 클 수 있다.

문제 해결 방법: **상호 순위 조합 (Reciprocal Rank Fusion, RRF)** 을 사용하여 문제를 해결한다.

각 점수의 순위를 기반으로 점수를 산출한다. 예를 들어, BM25와 임베딩 점수를 순위로 변환한다.
- 순위에 따라 점수를 계산하고, 만약 한쪽 점수에서 순위에 들지 않았으면 0점으로 처리한다.
- 두 점수를 합쳐 최종 순위를 결정하며, 높은 순위를 받은 문서가 최종적으로 더 높은 순위를 차지한다.

## 10.5 실습: 하이브리드 검색 구현하기

이번 실습에서는 앞서 구현한 밀집 임베딩 기반의 의미 검색에 BM25 검색을 더해 하이 브리드 검색을 구현한다

# 11 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기

## 11.1 검색 성능을 높이기 위한 두 가지 방법

바이 인코더와 교차 인코더를 결합하여 사용할 수 있다.

1) 바이 인코더를 사용해 대규모의 문서에서 검색 쿼리와 유사한 소수의 문서(예: 상위 100개)를 선별한다. 
2) 의미 검색을 통해 선별한 소수의 문서는 유사도를 더 정확히 계산할 수 있는 교차 인코더를 사용해 유사한 순서대로 재정렬한다. 

![그림 11.1 바이 인코더와 교차 인코더를 결합해 검색 성능 높이기](photo_2024-09-18_22-33-29.jpg)

검색 성능을 높이기 위한 방법:
- 바이 인코더를 추가 학습해 검색 성능을 높인다. (문장 임베딩 모델을 사용하려는 데이터셋으로 추가 학습한다.)
- 교차 인코더를 추가해 검색 성능을 높인다. (검색 쿼리와 관련이 높은 문서가 상위 검색 결과에 포함되어야 검색 증강 생성이 효과적)

## 11.2 언어 모델을 임베딩 모델로 만들기

![그림 11.2 문장 임베딩 모델의 구조](photo_2024-09-18_22-33-30.jpg)

사전 학습된 언어 모델을 불러 오고 그 위에 풀링 층을 추가하고 문장의 의미를 잘 담을 수 있도록 학습해야 한다.


### 11.2.1 대조 학습

문장 임베딩 모델을 학습시킬 때는 대조 학습contrastive learning을 사용한다. 

**대조 학습**이란, 관련이 있거나 유사한 데이터는 더 가까워지도록 만들고 관련이 없거나 유사하지 않은 데이터는 더 멀어지도록 하는 학습 방식

일반적으로는 서로 유사한 데이터를 수집한 데이터셋이나 서로 이어지는 문장을 수집 해 임베딩 모델의 학습에 사용한다.

### 11.2.2 실습: 학습 준비하기

이번 실습에서는 다음 세 가지 데이터 전처리를 수행한다.
- 학습 데이터의 일부를 검증을 위한 데이터셋으로 분리한다.
- 유사도 점수를 0~1 사이로 정규화한다
- torch.utils.data. DataLoader를 사용해 배치 데이터로 만든다.

> 예제 11.3 학습 데이터에서 검증 데이터셋 분리하기
```
# 학습 데이터셋의 10%를 검증 데이터셋으로 구성한다.
klue_sts_train = klue_sts_train.train_test_split(test_size=0.1, seed=42)
klue_sts_train, klue_sts_eval = klue_sts_train['train'], klue_sts_train['test']
```

> 예제 11.4 label 정규화하기

```
from sentence_transformers import InputExample
# 유사도 점수를 0~1 사이로 정규화 하고 InputExample 객체에 담는다.
def prepare_sts_examples(dataset):
    examples = []
    for data in dataset:
        examples.append(
            InputExample(
                texts=[data['sentence1'], data['sentence2']],
                #원본 데이터셋에는 0~5 점 척도로 되어 있는 label 점수를 5로 나눠 0~1 범위로 정규화 
                label=data['labels']['label'] / 5.0)
            )
    return examples
#앞서 준비한 3개 의 데이터셋(학습, 검증, 평가)을 prepare_sts_examples 함수를 사용해 전처리한다
train_examples = prepare_sts_examples(klue_sts_train)
eval_examples = prepare_sts_examples(klue_sts_eval)
test_examples = prepare_sts_examples(klue_sts_test)
```

> 예제 11.5 학습에 사용할 배치 데이터셋 만들기

```
from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
```

### 11.2.3 실습: 유사한 문장 데이터로 임베딩 모델 학습하기

> 예제 11.8 임베딩 모델 학습

```
from sentence_transformers import losses

num_epochs = 4
model_name = 'klue/roberta-base'
model_save_path = 'output/training_sts_' + model_name.replace("/", "-")
train_loss = losses.CosineSimilarityLoss(model=embedding_model)

# 임베딩 모델 학습
embedding_model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    evaluator=eval_evaluator,
    epochs=num_epochs,
    evaluation_steps=1000,
    warmup_steps=100,
    output_path=model_save_path
)
```

## 11.3 임베딩 모델 미세 조정하기

임베딩 모델 미세 조정: 검색 쿼리와 관련된 문서를 찾기 위해 임베딩 모델을 활용한다.

좋은 임베딩 모델은 관련 문서와의 유사도를 높게 (1에 가까운 값으로) 계산하고, 관련이 없는 문서와의 유사도는 낮게 (0에 가까운 값으로) 계산해야 한다.

### 11.3.1 실습: 학습 준비

KLUE의 MRC 데이터셋 - 기사 본문 및 해당 기사와 관 련된 질문을 수집한 데이터셋

KLUE의 MRC 데이터셋을 사용하여 사용자의 질문에 적절히 응답하는 챗봇을 만들어보자. 검색 증강 생성을 통해 관련 기사 본문을 찾고 이를 LUM의 프롬프트에 추가한다.
검색 증강 생성이 잘 작동하려면 관련 있는 질문과 기사 쌍에 높은 유사도 점수를, 관련 없는 쌍에는 낮은 유사도 점수를 부여해야 한다.

문장 임베딩 모델은 학습 데이터와 유사한 데이터에서 가장 잘 작동한다. 따라서, 사전 학습된 임베딩 모델이 MRC 데이터셋에 맞지 않을 경우 성능이 낮아질 수 있다. 이 문제를 해결하기 위해 MRC 데이터셋에 임베딩 모델을 활용하려는 경우 그 목적 에 맞게 MRC 데이터셋으로 미세 조정해야 한다.

> 예제 11.13 데이터 전처리

```
from datasets import load_dataset
klue_mrc_train = load_dataset('klue', 'mrc', split='train')
klue_mrc_test = load_dataset('klue', 'mrc', split='validation')

df_train = klue_mrc_train.to_pandas()
df_test = klue_mrc_test.to_pandas()

df_train = df_train[['title', 'question', 'context']]
df_test = df_test[['title', 'question', 'context']]
```

> 예제 11.14 질문과 관련이 없는 기사를 irrelevant_context 컬럼에 추가


```
def add_ir_context(df):
  irrelevant_contexts = []
  for idx, row in df.iterrows():
    title = row['title']
    irrelevant_contexts.append(df.query(f"title != '{title}'").sample(n=1)['context'].values[0])
  df['irrelevant_context'] = irrelevant_contexts
  return df

df_train_ir = add_ir_context(df_train)
df_test_ir = add_ir_context(df_test)
```

irrelevant_context 컬럼에 질문과 관련 없는 기사 본문을 추가한 데이터셋을 활용해 임베딩 모델의 성능 평가에 사용할 데이터를 만든다.

> 예제 11.15 성능 평가에 사용할 데이터 생성


```
from sentence_transformers import InputExample

examples = []
for idx, row in df_test_ir[:100].iterrows():
  examples.append(
      InputExample(texts=[row['question'], row['context']], label=1)
  )
  examples.append(
      InputExample(texts=[row['question'], row['irrelevant_context']], label=0)
  )
```

### 11.3.2 MNR 손실을 활용해 미세 조정하기

MNR 학습 - MRC 데이터셋과 같이 데이터셋에 서로 관련이 있는 문장만 있는 경우 사용하기 좋은 손실 함수

이전 기사 예제와 달리 MNR 손실을 사용하면 하나의 배치 데이터 안에서 다른 데이터의 기사 본문을 관련이 없는negative 데이터로 사용해 모델을 학습시킬 수 있다.

> 예제 11.19 MNR 손실 함수 불러오기

```
from sentence_transformers import losses

loss = losses.MultipleNegativesRankingLoss(sentence_model)
```

> 예제 11.20 MRC 데이터셋으로 미세 조정


```
epochs = 1
save_path = './klue_mrc_mnr'

sentence_model.fit(
    train_objectives=[(loader, loss)],
    epochs=epochs,
    warmup_steps=100,
    output_path=save_path,
    show_progress_bar=True
)
```


## 11.4 검색 품질을 높이는 순위 재정렬

효율적으로 바이 인코더로 후보군을 압축하고 후보군 내에서 교차 인코더로 순위를 재정렬하는 방법을 사용한다. 실습을 위해 sentence- transformers에서 제공하는 CrossEncoder와 미세 조정 메서드를 사용한다.

교차 인코더가 문장의 관련성을 자 계산 하기 위해 미세 조정이 필요하다. 수행 하기 전에 교차 인코더 학습 데이터셋을 생성해야 한다.

> 예제 11.25 교차 인코더 학습 데이터셋 준비


```
train_samples = []
for idx, row in df_train_ir.iterrows():
    train_samples.append(InputExample(
        texts=[row['question'], row['context']], label=1
    ))
    train_samples.append(InputExample(
        texts=[row['question'], row['irrelevant_context']], label=0
    ))
```

> 예제 11.26 교차 인코더 학습 수행


```
train_batch_size = 16
num_epochs = 1
model_save_path = 'output/training_mrc'

train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)

cross_model.fit(
    train_dataloader=train_dataloader,
    epochs=num_epochs,
    warmup_steps=100,
    output_path=model_save_path
)
```

## 11.5 바이 인코더와 교차 인코더로 개선된 RAG 구현하기

이번 장에는 검색 증강 생성을 다음과 같은 세 가지 케이스로 나눠 비교한다.

• 기본 임베딩 모델로 검색하기

• 미세 조정한 임베딩 모델로 검색하기

• 미세 조정한 모델과 교차 인코더를 결합해 검색하기

>예제 11.33 기본 임베딩 모델 평가

```
from sentence_transformers import SentenceTransformer
base_embedding_model = SentenceTransformer('shangrilar/klue-roberta-base-klue-sts')
base_index = make_embedding_index(base_embedding_model, klue_mrc_test['context'])
evaluate_hit_rate(klue_mrc_test, base_embedding_model, base_index, 10)
# (0.88, 13.216430425643921)
```

>예제 11.34 미세 조정한 임베딩 모델 평가


```
finetuned_embedding_model = SentenceTransformer('shangrilar/klue-roberta-base-klue-sts-mrc')
finetuned_index = make_embedding_index(finetuned_embedding_model, klue_mrc_test['context'])
evaluate_hit_rate(klue_mrc_test, finetuned_embedding_model, finetuned_index, 10)
# (0.946, 14.309881687164307)
```

> 예제 11.36 임베딩 모델과 교차 인코드를 조합해 성능 평가


```
hit_rate, cosumed_time, predictions = evaluate_hit_rate_with_rerank(klue_mrc_test, finetuned_embedding_model, cross_model, finetuned_index, bi_k=30, cross_k=10)
hit_rate, cosumed_time
# (0.973, 1103.055629491806)
```

결과:
![표 11.1 세 가지 검색 방식의 성능 비교](photo_2024-09-18_22-33-32.jpg)

성능 향상 분석:
- 기본 임베딩 모델에 비해 미세 조정된 임베딩 모델을 사용했을 때, 거의 동일한 시간에 6.6% 성능 향상이 있었다.
- 임베딩 모델과 교차 인코더를 모두 사용해 순위 재정렬을 했을 때, 추가로 **2.7%**의 성능 향상이 있었다.

미세 조정된 임베딩 모델이 실패한 5.4%의 데이터 중 절반인 2.7%에서 추가로 정답을 포함시켰다.
즉, 틀린 문제를 기준으로 보면, **오답률이 절반으로 줄어들었다.**

# 12 벡터 데이터베이스로 확장하기: RAG 구현하기

## 12.1 벡터 데이터베이스란

**벡터 데이터베이스**란, 벡터 임베딩을 키로 사용하는 데이터베이스를 말한다.

데이터 -> <임베딩 모델>로 데이터를 벡터 표현 ->임베딩 벡터 -> <벡터 데이터베이스>를 통해 벡터 사이 거리 계산

### 12.1.1 딥러닝과 벡터 데이터베이스

딥러닝이 머신러닝보다 뛰어난 가장 큰 이유는 직접 데이터의 특징을 정의 할 필요 없이 대량의 학습 데이터를 준비해 직접 데이터의 특징을 정의 하고 학습을 할 수 있기 떄문이다.

**표현 학습** - 테이터의 특징을 뽑는 과정을 학습

딥러닝 모델은 데이터를 학습되는 과정에서 데이터의 특징을 추출하는 방법을 **함께** 학습한다.

임베딩 벡터의 특징을 이용해 거리를 계산하면 서로 비슷한 데이터를 찾을 수 있다.

벡터 데이터베이스를 활용하기 위한 단계:
1. 저장: 저장할 데이터(예: 문서)를 임베딩 모델을 거쳐 벡터로 변환하고 벡터 데이터베이스에 저장한다.
2. 검색: 검색할 데이터(예: 검색 쿼리)를 임베딩 모델을 거쳐 벡터로 변환하고 벡터 데이 터베이스에서 검색한다.
3. 결과 반환: 벡터 데이터베이스에서는 검색 쿼리의 임베딩과 거리가 가까운 벡터를 찾 아 반환한다.

![그림 12.5 벡터 데이터베이스 작동 과정]

### 12.1.2 벡터 데이터베이스 지형 파악하기

벡터 임베딩을 저장하고 검색하는 기능을 구현하기 위해 3가지 소프트웨어가 필요하다.
1) 벡터 라이브러리 - 벡터를 저장하고 검색하는 핵심 기능
2) 벡터 전용 데이터베이스 
3) 벡터 기능 추가 데이터베이스 - 기존 데이터베이스에 벡터 저장과 검색기능을 추가

벡터 전용 데이터베이스 벡터 라이브러와 달리 추가적으로:
- 메타 데이터의 저장 및 필터링 기능
- 데이터의 백업 및 관리 라이
- 모니터링, 관련 AI 도구 등 에코시스템과의 통합
- 데이터 보안과 액세스 관리 

를 한다

>벡터 데이터베이스 구분
![그림 12.7 벡터 데이터베이스 구분]

## 12.2 벡터 데이터 베이스 작동원리

### 12.2.1 KNN 검색과 그 한계

**KNN 검색**은 검색하려는 벡터와 가장 가까운 K개의 이웃 벡터를 찾는 검색 방식을 말 한다.

징점: 직관적이고 모든 데이터를 조사하기 때문에 정확하다

단점: 모든 벡터를 조사하기 때문에 연산량이 데이터 수에 비례하게 늘어나 속도가 느려지기 때문에 확장성이 떨어진다

벡터 검색을 위해 **인덱스를 생성**해야 한다. 관계형 데이터베이스의 테이블과 유사.

 벡터를 저장하는 과정을 **색인**이라고 한다. 성능 지표로는 색인 시<u> 인덱스의 메모리 사용량과 시간</u>, 검색 시<u> 검색 시간과 재현율</u>이 중요하다. 

**재현율**은 실제 정답 데이터 중 검색 결과로 반환된 비율을 나타낸다. KNN 검색은 재현율이 100%이다.

 연산량이 데이터 수에 비례하게 늘어나 속도가 느려지는데 맞는지 확인하기 위해 100만개의 128차원 임베딩 데이터인 SIFTIM 데이터셋을 활용해 데이터 수가 증가할 때 검 색 시간이 어떻게 달라지는지 확인해 보았다. 결과는 밑에 표와 같이 데이터가 늘어나면서 색인 시간이 증가하고 메모리 사용량도 약 97MB 정도 지속적으로 증가하는 것을 확인할 수 있다.

![표 12.1 데이터양에 따른 검색 지표 확인]

정확성을 떨어트리더라고 속도를 높이기 위해 ANN이라는 것을 개발했다.

ANN(Approximate Nearest Neighbor) - 용량 데이터셋에서 주어 진 쿼리 항목과 가장 유사한 항목을 효율적으로 찾는 데 사용되는 검색 기술

KNN 검색의 한계를 극복하기 위해 ANN은 임베딩 벡터를 빠르게 탐색할 수 있는 구조로 저장해서 검색 시 탐색할 범위를 좁히는 데 집중한다.

대표적 ANN 알고리즘: 
- **IVF**: 검색 공간을 제한하기 위해 데이터셋 벡터들을 **클러스터로 그룹화**(중심점(쿼리와 유사한 근처 벡터들의 클레스터)를 먼저 찾아 해당 중심점 내에서 가장 가까운 데이터 포인트를 찾음으로써 가장 가 까운 데이터 포인트를 찾는다 )하는 근사 최근접 이웃 검색 알고리즘이다
- **HNSW**: 그래프 기반 인덱싱 구조다.  상위 계층은 연결이 적고 하위 계층은 연결이 밀집된 다층 그래프를 구축한다. 상위 계층에서는 먼 거리를 빠르게 이동할 수 있고, 하위 계층에서는 더 꼼꼼한 탐색이 가능하다. 검색은 상위 계층에서 시작해 하위 계층으로 진행된다.

### 12.2.3 탐색 가능한 작은 세계(NSW)

![그림 12.9 그래프 용어 설명]

그래프는 노드(벡터 임베딩)와 간선(노드끼리 연결하는 선)으로 이루어져 있다. 노드 사이에 간선이 얼마나 많이 연결했느냐에 따라 검색성능과 검색 속도에 영향을 미친다. 
 
**탐색 가능한 작은 세계**란, 완전히 랜덤한 그래프와 완전히 규칙적인 그래프 사이에 '적당히 랜덤하게' 연결된 그래프 상태다. 규칙적인 연결을 통해 정확한 탐색이 가능 하면서도 랜덤한 성질을 통해 빠른 탐색이 가능해진다는 특징이 있다.

![그림 12.10 랜덤한 그래프(a)와 규칙적인 그래프(c) 사이의 작은 세계(b)]

완전히 규칙적으로 연결된 그래프는 정확한 탐색이 가능하지만, 많은 단계를 거쳐야 하므로 시간이 오래 걸린다. **랜덤 간선**을 추가한 그래프는 더 적은 탐색 단계로 빠르게 가까운 노드를 찾을 수 있다. 그러나 **지역 최솟값 문제를** (진입점에서 출발했을 때 찾으려는 검색 벡터와 가장 가까운 점이 아닌 다른 점에서 탐색을 멈춤) 피하기 위해 계층 구조가 추가된다.

### 12.2.4 계층 구조

연결 리스트 - 많이 활용되는 자료구조

연결 리스트는 데이터 추가/삭제가 자유롭지만, 탐색 속도가 느리다. 

하지만 데이터를 **크기순으로 정렬**하고 상위 **레벨**을 만들어 하위 레벨보다 듬성듬성 데이터를 배치하면, 탐색 속도를 높일 수 있다. 예를 들어, 스킵 리스트 구조를 사용하면 단계적으로 탐색 범위를 좁혀 빠르게 원하는 데이터를 찾을 수 있다.

![그림 12.15 스킵 리스트 작동 원리]

HNSW에는 이런 계층 구조가 접목되어 있다.

![그림 12.16 HNSW 계층 그림]

벡터를 어떤 층에 넣는지 고르는 방법:
 - 최대가 6인 주사위를 굴려서 6이 나오면 0, 1, 2층 모두 배치한다.
- 주사위를 굴려서 4~5가 나오면 0, 1층에 배치한다.
- 주사위를 굴려서 1, 2, 3이 나오면 0층에만 배치한다.

이렇게 확률을 이용해 높은 층으로 갈수록 데이터가 적어지는 구조를 만든다.




## 12.3 실습: HNSW 인덱스의 핵심 파라미터 이해하기

### 12.3.1 파라미터 m 이해하기 

파라미터 m은 추가하는 임베딩 벡터에 연결하는 간선의 수다.

예제 12.4는 파라미터 이 색인과 검색 성능에 어떤 영향을 미치는지 확인하는 코드다.

> 예제 12.4 파라미터 m의 변경에 따른 성능 확인


```
import numpy as np

k=1
d = xq.shape[1]
nq = 1000
xq = xq[:nq]

for m in [8, 16, 32, 64]:
    index = faiss.IndexHNSWFlat(d, m)
    time.sleep(3)
    start_memory = get_memory_usage_mb()
    start_index = time.time()
    index.add(xb)
    end_memory = get_memory_usage_mb()
    end_index = time.time()
    print(f"M: {m} - 색인 시간: {end_index - start_index} s, 메모리 사용량: {end_memory - start_memory} MB")

    t0 = time.time()
    D, I = index.search(xq, k)
    t1 = time.time()

    recall_at_1 = np.equal(I, gt[:nq, :1]).sum() / float(nq)
    print(f"{(t1 - t0) * 1000.0 / nq:.3f} ms per query, R@1 {recall_at_1:.3f}")
```

결과를 확인해 보면 **m이 커지면서 재현율이 크게 증가한다**. 또한 메모리 사용량이 증가하고 색인과 검색 시간이 길어 졌다. 주목할 만한 부분은 m울 64로 설정했을 때 검색 시간이 0.237ms밖에 걸리지 않는 데 KNN과 비교했을 때 93.2% 정도는 정확하게 가장 가까운 벡터를 찾았다.

### 12.3.2 파라미터 ef_construction 이해하기

인덱스에 새로운 벡터를 추가하는 경우 검색할 때와 유사하게 추가한 벡터와 가장 가까 운 벡터를 탐색한다. 이때 **ef_construction**은 M개의 가장 가까운 벡터를 선택할 후보군 의 크기를 결정한다. 크기가 클수록 추가한 벡터와 가장 유사한 벡터를 선택 할 가능성이 높아진다. 커질수롯 재현율과 생인시간이 올라간다. 메모리 사용량과 검색시간은 영향을 받지 않는다. 

### 12.3.3 파라미터 ef_search 이해하기

ef_search는 검색 단계에서 후보군의 크기를 결정한다 (ef_construction과 비슷)

재현율가 검색시간이 늘어난다.

## 12.4 실습: 파인콘으로 벡터 검색 구현하기

데이터베이스의 기본 동작 CRUD:
- Create 생성
- Read조회
- Update 수정
- Delete 삭제

### 12.4.1 파인콘 클라이언트 사용법

파이콘을 사용하기 위해 API KEY가 필요하다. 또한 데이터를 저장 할 인덱스를 생성 해야 한다.

> 예제 12.7 파인콘 계정 연결 및 인덱스 생성

```
from pinecone import Pinecone, ServerlessSpec

pinecone_api_key = "자신의 API 키를 입력"
pc = Pinecone(api_key=pinecone_api_key)

pc.create_index("llm-book", spec=ServerlessSpec("aws", "us-east-1"), dimension=768)
index = pc.Index('llm-book')
```

그 다음 임베딩 생성한다. 임베딩을 파인콘 인덱스에 저장 할 수 있도록 형태를 변환한다

> 예제 12.8 임베딩 생성

```
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
# 임베딩 모델 불러오기
sentence_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
# 데이터셋 불러오기
klue_dp_train = load_dataset('klue', 'dp', split='train[:100]')

embeddings = sentence_model.encode(klue_dp_train['sentence'])
```
> 예제 12.9 파인콘 입력을 위한 데이터 형태 변경


```
# 파이썬 기본 데이터 타입으로 변경
embeddings = embeddings.tolist()
# {"id": 문서 ID(str), "values": 벡터 임베딩(List[float]), "metadata": 메타 데이터(dict) ) 형태로 데이터 준비
insert_data = []
for idx, (embedding, text) in enumerate(zip(embeddings, klue_dp_train['sentence'])):
  insert_data.append({"id": str(idx), "values": embedding, "metadata": {'text': text}})

# 임베딩 데이터를 인덱스에 저장
upsert_response = index.upsert(vectors = insert_data, namespace='llm-book-sub')
```

저장한 데이터를 검색하기 위해 쿼리 메서드를 사용한다

> 예제 12.11 인덱스 검색하기


```
query_response = index.query(
    namespace='llm-book-sub', # 검색할 네임스페이스
    top_k=10, # 몇 개의 결과를 반환할지
    include_values=True, # 벡터 임베딩 반환 여부
    include_metadata=True, # 메타 데이터 반환 여부
    vector=embeddings[0] # 검색할 벡터 임베딩
)
query_response
```



> 예제 12.12 파인콘에서 문서 수정 및 삭제


```
new_text = '변경할 새로운 텍스트'
new_embedding = sentence_model.encode(new_text).tolist()
# 업데이트
update_response = index.update(
    id= '기존_문서_id',
    values=new_embedding,
    set_metadata={'text': new_text},
    namespace='llm-book-sub'
)

# 삭제
delete_response = index.delete(ids=['기존_문서_id'], namespace='llm-book-sub')
```


## 12.5 실습: 파인콘을 활용해 멀티 모달 검색 구현하기

CLIP - 이미지와 텍스트를 동일한 벡터 공간에 임베디으로 변환

### 12.5.1 데이터셋

실습에 사용 할 데이터셋 다운로드

### 12.5.2 실습 흐름

원본 이 미지와 세 가지 프롬프트로 생성한 3개의 합성 이미지를 비교해 본다.

![그림 12.19 실습 흐름]

### 12.5.3 GPT-40로 이미지 설명 생성하기

GPT-40에 전달하기 위해 이미지 를 알맞은 형식으로 변환하고 GPT-40에 요청을 보내는 예제 12.15의 함수를 사용한다. 그 다음 정의한 함수를 화룡해 이미지 설명을 생성한다.

> 예제 12.15 GPT-4o 요청에 사용할 함수 + 예제 12.16 이미지 설명 생성


```
import requests
import base64
from io import BytesIO

def make_base64(image):
  buffered = BytesIO()
  image.save(buffered, format="JPEG")
  img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
  return img_str

def generate_description_from_image_gpt4(prompt, image64):
  headers = {
      "Content-Type": "application/json",
      "Authorization": f"Bearer {client.api_key}"
  }
  payload = {
      "model": "gpt-4o",
      "messages": [
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": prompt
            },
            {
              "type": "image_url",
              "image_url": {
                "url": f"data:image/jpeg;base64,{image64}"
              }
            }
          ]
        }
      ],
      "max_tokens": 300
  }
  response_oai = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
  result = response_oai.json()['choices'][0]['message']['content']
  return result

#이미지 설명 생성
image_base64 = make_base64(original_image)
described_result = generate_description_from_image_gpt4("Describe provided image", image_base64)
described_result
```

 
### 12.5.4 프롬프트 저장

12.5.4 프롬프트 저장

예제 12.17의 코드로 실습에서 사용할 파인콘 클라이언트와 OpenAI 클라이언트를 초기화하고 API 키를 입력한다. 

> 예제 12.17 클라이언트 준비

```
image_base64 = make_base64(original_image)
described_result = generate_description_from_image_gpt4("Describe provided image", image_base64)
described_result
```

다음으로 예제 12.18을 통해 프롬프트 임베딩을 저장하고 이미지 임베딩으로 검색할 인덱스를 생성한다

> 예제 12.18 인덱스 생성

```
print(pc.list_indexes())

index_name = "llm-multimodal"
try:
  pc.create_index(
    name=index_name,
    dimension=512,
    metric="cosine",
    spec=ServerlessSpec(
      "aws", "us-east-1"
    )
  )
  print(pc.list_indexes())
except:
  print("Index already exists")
index = pc.Index(index_name)
```

이제 프롬프트 데이터를 텍스트 임베딩으로 변환할 모델을 불러오고 변환을 수행한다. 

>예제 12.19 프롬프트 텍스트를 텍스트 임베딩 모델을 활용해 임베딩 벡터로 변환

```
import torch
from tqdm.auto import trange
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, CLIPTextModelWithProjection

device = "cuda" if torch.cuda.is_available() else "cpu"

text_model = CLIPTextModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

tokens = tokenizer(dataset['prompt'], padding=True, return_tensors="pt", truncation=True)
batch_size = 16
text_embs = []
for start_idx in trange(0, len(dataset), batch_size):
    with torch.no_grad():
        outputs = text_model(input_ids = tokens['input_ids'][start_idx:start_idx+batch_size],
                        attention_mask = tokens['attention_mask'][start_idx:start_idx+batch_size])
        text_emb_tmp = outputs.text_embeds
    text_embs.append(text_emb_tmp)
text_embs = torch.cat(text_embs, dim=0)
text_embs.shape # (1000, 512)
```

다음으로 예제 12.20을 사용해 생성한 임베딩 벡터를 벡터 데이터베이스에 저장한다.

>예제 12.20 텍스트 임베딩 벡터를 파인콘 인덱스에 저장

```
input_data = []
for id_int, emb, prompt in zip(range(0, len(dataset)), text_embs.tolist(), dataset['prompt']):
  input_data.append(
      {
          "id": str(id_int),
          "values": emb,
          "metadata": {
              "prompt": prompt
          }
      }
  )

index.upsert(
  vectors=input_data
)
```

### 12.5.5 이미지 임베딩 검색

이제 원본 이미지를 이미지 임베딩으로 변환하고 해당 임베딩 벡터로 파인콘에서 유사 한 프롬프트를 검색해 본다.

>예제 12.21 이미지 임베딩을 사용한 유사 프롬프트 검색

```
from transformers import AutoProcessor, CLIPVisionModelWithProjection

vision_model = CLIPVisionModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

inputs = processor(images=original_image, return_tensors="pt")

outputs = vision_model(**inputs)
image_embeds = outputs.image_embeds

search_results = index.query(
  vector=image_embeds[0].tolist(),
  top_k=3,
  include_values=False,
  include_metadata=True
)
searched_idx = int(search_results['matches'][0]['id'])
```

### 12.5.6 DALL-E 3로 이미지 생성

이제 준비한 3개의 프롬프트로 이미지를 생성해 보자.

>예제 12.23 프롬프트로 이미지를 생성하고 저장하는 함수 정의

```
from PIL import Image

def generate_image_dalle3(prompt):
  response_oai = client.images.generate(
    model="dall-e-3",
    prompt=str(prompt),
    size="1024x1024",
    quality="standard",
    n=1,
  )
  result = response_oai.data[0].url
  return result

def get_generated_image(image_url):
  generated_image = requests.get(image_url).content
  image_filename = 'gen_img.png'
  with open(image_filename, "wb") as image_file:
      image_file.write(generated_image)
  return Image.open(image_filename)
```

> 예제 12.24 준비한 3개의 프롬프트로 이미지 생성

```
# GPT-4o가 만든 프롬프트로 이미지 생성
gpt_described_image_url = generate_image_dalle3(described_result)
gpt4o_prompt_image = get_generated_image(gpt_described_image_url)
gpt4o_prompt_image
# 원본 프롬프트로 이미지 생성
original_prompt_image_url = generate_image_dalle3(original_prompt)
original_prompt_image = get_generated_image(original_prompt_image_url)
original_prompt_image
# 이미지 임베딩으로 검색한 유사 프롬프트로 이미지 생성
searched_prompt_image_url = generate_image_dalle3(dataset[searched_idx]['prompt'])
searched_prompt_image = get_generated_image(searched_prompt_image_url)
searched_prompt_image
```

마지막으로 이미지를 출력해본다

>예제 12.25 이미지 출력


```
import matplotlib.pyplot as plt

images = [original_image, gpt4o_prompt_image, original_prompt_image, searched_prompt_image]
titles = ['(a)', '(b)', '(c)', '(d)']

fig, axes = plt.subplots(1, len(images), figsize=(15, 5))

for ax, img, title in zip(axes, images, titles):
    ax.imshow(img)
    ax.axis('off')
    ax.set_title(title)

plt.tight_layout()
plt.show()
```

# 13 LLM 운영하기

## 13.1 MLOps 

**MLOps** (Machine Learning Operations)는 데브옵스Development and Operations, **DevOps**의 개념을 머신 러닝과 데이터 과학 분야로 확장한 방법론이다. 

기존의 데브옵스가 소프트웨어 개발과 운영의 협업과 자동화에 초점을 맞췄다면, MLOps는 여기에 데이터와 머신러닝 모델이 라는 두 가지 핵심 요소를 더해 확장한 것이다. 

MLOps의 목표는 **데이터 수집, 전처리, 모델 학습, 평가, 배포**, 모니터링 등 머신러닝 프로젝트의 전 과정을 자동화하고 효율화 하는 것이다. 이를 통해 데이터 과학자와 엔지니어 간의 협업을 강화하고, 모델의 품질 과 안정성을 향상하며, 비즈니스 가치 실현까지의 시간을 단축할 수 있다.

MLOps는 **데이터 수집, 전처리, 모델 학습, 평가, 배포 wkehdghk**, 다시 말하자면 머신러닝 파이프라인을 자동화하고 관리하며, 특히 모델의 **재현성**을 보장하는 것이 중요하다. 이를 위해 데이터, 모델, 코드의 버전 관리와 각 단계의 입력값과 파라미터를 추적하고 기록해랴 한다. 또한, MLOps는 새로운 데이터에 따라 자동으로 모델을 재학습 및 배포하여 성능을 유지하는 CI/CD 개념을 적용한다.

그림을 기준으로 했을 때 MLOps에서는 다음과 같은 주제를 다룬다.
• 데이터 관리
• 실험 관리
• 모델 저장소
• 모델 모니터링

![그림 13.2 머신러닝 파이프라인]

### 13.1.1 데이터 관리

학습 데이터 범위 선택, 전처리 방식, 숫자 변환 방식, 값의 크기 조정, 특성 공학 등으로 데이터의 크기와 의미가 달라지고 모델의 성능이나 일반화 능력에 영향을 미칠 수 있다.

머신러닝 모델의 학습 과정에서 모델의 성능과 일반화 능력을 높이기 위해 다양한 시도 를 하게 되고 다양한 데이터셋이 만들어진다. 모델 학습 결과를 재현하기 위해서는 데 이터셋의 버전을 관리하고 어떤 학습 데이터셋으로 모델을 학습시켰는지 기록해야 한 다. 이를 위해 DVC(Data Version Control)와 같은 도구를 사용할 수 있다.

### 13.1.2 실험 관리

머신러닝 모델 학습 시, 다양한 모델과 하이퍼파라미터를 선택해야 하며, 최적의 하이퍼파라미터를 탐색하는 과정도 필요하다. 여러 사람이 협업하는 경우 실험을 관리하고 추적하기 어려워질 수 있어, 이를 해결하기 위해 MLflow나 W&B 같은 실험 관리 도구를 사용한다. 정확한 실험 추적을 통해 재현성을 확보하여 불필요한 추가 실험으로 인한 낭비를 막을 수 있다.

### 13.1.3 모델 저장소

MLOps에서 모델 저장소model registry는 머신러닝 모델을 체계적으로 관리하고 버전 제어하는 데 필수적인 요소다. 
모델 저장소를 사용하면 모델에 문제가 발생하는 경우 빠르게 이전 버전으로 되돌릴 수 있게 해준다. 또한 모델의 메타 데이터(모델 생성일, 성능 지표, 하이퍼파라미터 등)를 저장하여 모델에 대한 중요한 정보를 쉽게 확인할 수 있도록 한다. 

모델 저장소와 연동하여 모델을 서빙하고 배포하는 과정을 자동화할 수 있어 모델의 배포와 관리가 간소화된다. 그뿐 아니라, 모델 저장소는 데이터 과학자와 엔지니어 간의 협업을 촉진하여 팀 내 커뮤니케이션을 원활 하게 한다. 

### 13.1.4 모델 모니터링

머신러닝 모델은 학습 데이터를 통해 학습한 패턴을 바탕으로 예측을 수행하기 때문에 정상적으로 요청에 응답하고 있다고 하더라도 엉뚱한 값을 반환한 것은 아닌지 확인해야 한다. 특히 학습한 이후 오랜 시간이 흘렀다면 입력 데이터의 변동으로 인해 모델의 성능 저하가 발생할 수 있다. 따라서 이런 문제를 빠르게 발견하고 대응할 수 있도록 **모니터링**을 수행해야 한다.

## 13.2 LLMOps는 무엇이 다를까?

### 13.2.1 상업용 모델과 오픈소스 모델 선택하기

MLOps와 ILMOps가 다른 이유: MLOps에서 다루는 머신러닝 모델은 회귀나 분류 같은 하나의 문제를 해결하는 모델이었다. 하지만 LLMOps는 모델의 파라미터 수가 훨씬 많고 하나 이상의 문제를 해결하기 위해 만들어지는 경우가 많다. 

모델이 커지면 필요한 학습 데이터양과 컴퓨팅 자원이 크게 증가하지만 다양한 작업을 수행할 수 있어 성능도 높아진다. 일부 기업들은(OpenAI,Google)은 더 크게 만들어 성능이 뛰어난 모델을 만들기 위해 노력 하고 있다.

대규모 언어 모델은 API를 통해 사용할 수 있는 **상업용 모델**과 직접 모델을 학습 시키고 실행해 활용할 수 있는 **오픈소스 모델**로 양분됐다. 사용자는 대규모 언어 모델 을 사용하려는 목적과 문제의 난이도에 따라 상업용 모델과 오픈소스 모델 중에서 적절 한 모델을 선택해야 한다.

상업용 모델은 성능이 높고 사용이 편리하지만, 비용 관리가 어렵고 미세 조정이 제한적이다. 반면, 오픈소스 모델은 자유로운 미세 조정이 가능하고 비용 효율적이지만 기술적 난이도가 높다. 

모델의 크기는 추론할 때 가장 큰 문제가 된다. 모델이 크면 추론에 더 많은 GPU를 사 용해서 비용이 많이 발생한다. 따라서 모델의 용량을 줄이 기 위한 다양한 방법을 사용한다. 모델 경량화를 위해 양자화와 지식 증류가 사용되며, 효율적인 추론을 위해 연속 배치, 플래시 어텐션, 페이지 어텐션 같은 방법이 활용된다.

### 13.2.2 모델 최적화 방법의 변화

LLM을 최적화하는 방법은 사전 학습, 미세 조정, 프롬프트 엔지니어링, 검색 증강 생성(RAG)으로 나눌 수 있다. LLMOps에는 모델 크기가 크기 떄문에 사전 학습은 거의 사용되지 않으며, 오픈소스 모델은 자유롭게 미세 조정할 수 있지만 상업용 모델은 제한적이다. 또한, 프롬프트를 구조화해 원하는 결과를 얻는 프롬프트 엔지니어링이나 필요한 정보를 프롬프트에 추가하는 RAG를 사용할 수 있다.

![표 13.2 LLM을 최적화할 때 사용하는 방법]

MLOps에 비해 LLMOps는 다루는언어 모델이 매우 크기 떄문에 처음부터 학습시킬 때 들어가는 계산량 이 크기 때문에 일반적으로 사전 학습은 하지 않고 사전 학습된 모델을 가져와 미세 조정하는 전이 학습을 기본으로 사용한다.

미세 조정은 사전 학습된 모델에 대해 지도 미세 조정이나 DPO 같은 방법으로 추가 학습하는 것을 말하며, 적은 데이터로 학습하지만 여전히 많은 GPU가 필요하다. 이를 해결하기 위해 LORA나 QLORA 같은 효율적인 학습 방법이 개발되었다. 

LLM은 프롬프트에 따라 성능이 달라지므로, 프롬프트 엔지니어링을 통해 더 나은 결과를 얻는 것이 중요하다. 

MLOps에서는 모델 학습할 떄 설정한 하이퍼파라미터를 기록해 동일한 성능의 모델을 다시 만들 수 있도록 관리했다. LLMOps에서는 입력하는 프롬프트에 따라 모델 성능이 달라지기 때문에 프롬프트도 다른 학습 하이퍼파라미터와 마찬가지로 실험의 대상이 된다. 이를 추적하고 기록하는 도구로 W&B(Prompts Trace)와 MLflow(LLM Tracking) 등이 있다.

### 13.2.3 LLM 평가의 어려움

LLM은 일반적인 ML 모델과 달리 다양한 작업을 수행할 수 있으며, 특정 작업의 성능 평가 방식으로 모두 평가하기 어렵다. 프롬프트에 따라 성능이 달라지기 때문에 명확한 기준을 잡기 어려워 지금도 활발히 연구되고 있다. 이어지는 절에서는 현재 활용되는 LLM 평가 방식과 새로운 시도를 다룬다.

## 13.3 LLM 평가하기

### 13.3.1 정량적 지표


텍스트 생성 작업을 평가하는 대표적인 정량 지표:
- BLEU(Bilingual Evaluation Understudy Score): 기계 번역 결과와 사람이 번역한 결과의 유사도를 측정한다. 이 지표는 n-그램을 기반으로 모델이 생성한 문장과 참조 문장 간의 정밀도를 계산하며, 여러 개의 참조 번역문을 사용할수록 평가가 정확해진다. 하지만 BLEU는 문장의 유창성이나 문법적 오류를 반영하지 못하는 한계가 있다.
- ROUGE(Recall-Oriented Understudy for Gisting Evaluation): 요약이나 번역 등 자연어 생성 모델의 성능을 평가할 때 사용된다. 모델이 생성한 요약문과 사람이 작성한 참조 요약문 간의 n-그램 중복도를 재현율 관점에서 측정한다.그러나 ROUGE는 단어의 순서나 문장 구조를 고려하지 않는 한계가 있다.
- 펄플렉시티 (perplexity. PPL): 모델이 새로운 단어를 생성할 때의 불확실성을 수치화한 것으로, 값이 낮을수록 모델의 예측 성능이 우수하다는 의미다.

세 가지 정량 지표 모두 빠르게 언어 모델의 성능을 평가할 수 있지만 문장의 의미, 문법, 유창성 등 질적인 측면의 평가에는 한계가 있다. 따라서 이런 정량 지표들은 언어 모델 평가 시 참고는 할 수 있으나, 절대적인 잣대로 삼기에는 무리가 있다.

### 13.3.2 벤치마크 데이터셋을 활용한 평가

**벤치마크 데이터셋**: 다양한 모델의 성능을 비교하기 위해 공통으로 사용하는 데이터셋

대표적 벤치마크 데이터셋: 
- ARC 사지선다형 과학문제
- HellaSwag 다음으로 연결될 문장을 고르는 문제
- MMLU 초등학교 수학부터 인문학, 사회과학, 자연 과학 등 57개 분야에 대한 사지선다형 문제
- TruthfulQA 신뢰 할 수 있는 문제인지 확인 할 수 있는 린문으로 이루어져 있는 데이터셋

### 13.3.3 사람이 직접 평가하는 방식

정량적 지표는 모델의 성능을 빠르게 평가할 수 있지만, 사람의 평가와 일치하지 않는 경우가 많기 때문에 LLM 기반 애플리케이션 개발 시에는 사람이 직접 결과를 확인하여 정량적 지표를 보완해야 한다. 개발 단계에서는 구성원들이 입력과 출력을 확인하며 평가하고, 별도의 평가자가 있는 경우도 있다. 

그러나 사람이 직접 평가하는 방식은 언어의 유창성과 같은 정량적으로 평가하기 어려운 요소를 평가할 수 있지만, 시간이 오래 걸리고 비용이 많이 드는 단점이 있다. 이러한 문제를 해결하기 위해 LLM 자체를 평가자로 활용하는 연구가 활발히 진행되고 있다.

### 13.3.4 LLM을 통한 평가

벤치마크 데이터를 활용해 언어모델이 '얼마나 똑똑한지'를 알 수 있지만 '사람의 요청에 얼마나 잘 대응하는지'는 알지 못한다. 그래서 사람이 평가하면 너무 오래 걸리니 LLM이 대신 평가자로 활용하는 방법을 찾아냈다.

선별한 멀티 턴 질문 데이터인 MT-Bench와 여러 LLM의 답변에 대해 사람들이 선호를 평가한 챗봇 아레나 Chatbot Arena 데이터를 활용한다. MT-Bench 질문의 예시는 표 13.4와 같다. 첫 번째 턴에서 하나의 요청을 하고 응답 이후에 다시 두 번째 요청을 한다. 여러 턴에 걸쳐 LIM이 사용자의 요구사항에 맞춰 대응하는지 확인하기 위해서다.

![표 13.4 MT-Bench의 멀티 턴 질문 예시]

### 13.3.5 RAG 평가

RAG가 LLM 애플리케이션의 핵심이다. RAG의 성능 평가가 어렵다.

 RAG의 성능을 평가하기 위해서는 검색 단계와 생성 단계로 나눌 수 있으며, 각 단계의 성능을 평가해야 한다. 따라서 최초 요청과 최종 생성 결과를 바탕으로 평가하는 세 가지 측면에서 RAG의 성능을 분석할 필요가 있다.

![그림 13.9 RAG의 세 가지 데이터와 평가]

- 신뢰성faithfulness: 생성된 응답이 검색된 맥락 데이터에 얼마나 사실적으로 부합하는지 평가
- 답변 관련성answer relevancy: 생성된 답변이 요청과 얼마나 관련성이 있는지 평가
- 맥락 관련성context relevancy: 검색 결과인 맥락 데이터가 요청과 얼마나 관련 있는지 평가

# 14. 멀티 모달 LLM

## 14.1 멀티 모달 LLM이란

멀티 모달 LLM이란, 텍스트뿐만 아니라 이미지, 비디오, 오디오, 3D 등 다양한 형식의 데 이터를 이해하고 생성할 수 있는 LLM

### 14.1.1 멀티 모달 LLM의 구성요소

일반적으로 멀티 모달 LLM은 다섯 가지 구성 요소로 이루어져 있으며, LLM이 핵심 역할을 한다.

 LLM은 뛰어난 이해 및 추론 능력을 통해 이미지 형식의 데이터를 **모달리티 인코더**(modality encoder)와 **입력 프로젝터**(input projector)를 통해 텍스트로 변환하여 LLM에 입력한다. LIM의 출력은 기본적으로 텍스트이며,** 출력 프로젝터**(output projector)를 통해 이미지 형태의 데이터 출력을 판단하고, **모달리티 생성기**(modality generator)를 통해 특정 데이터 형식의 출력을 생성한다.

![그림 14.2 얼티 오달 LLM의 구성요소]

그림 14.2에서 입력 프로젝터와 출력 프로젝터는 불꽃 아이콘이 있고, 나머지 모달리티 인코더, LLM 백본, 모달리티 생성기에는 얼음 아이콘이 있다. 얼음 아이콘이 있는 세 구성 요소는 많은 데이터와 연산량이 필요해 학습 과정에서 파라미터를 업데이트하지 않고 사전 학습된 모델을 그대로 사용한다. 반면, 불꽃 아이콘이 있는 입력 프로젝터와 출력 프로젝터는 파라미터를 업데이트하여 멀티 모달 이해와 생성 성능을 높인다.


비전 트랜스포머 : 텍스트를 처리하기 위해 개발된 트랜스포머 아키텍처를 이미지에 적용한 모델, 이미지를 처리하는 모달리티 인코더로 비전 트랜스포머가 널리 사용된다. 이 모델은 이미지를 패치 단위로 나눈 후 텍스트의 단어처럼 일렬로 나열해 처리한다. 이미지 패치는 선형 변환을 통해 이미지 임베딩 벡터로 변환되어 토큰 임베딩과 유사한 형태로 입력된다.

### 14.1.2 멀티 모달 LLM 학습 과정

멀티 모달 LLIM을 만들기 위해서는 사전 학습된 텍스트 전용 LLM이 멀티 모달 입력과 출력을 지원하기 위한 추가 훈련 단계를 거친다. 멀티 모달 LLM의 학습 과정은 LLM과 마찬가지로 **사전 학습**과 지시 데이터셋을 활용한 **지시 학습**instruction tuning 으로 나뉜다. 

멀티 모달 사전 학습 단계에서 LLM은 이미지-텍스트 쌍과 같은 대규모 멀티 모달 데이 터 세트로 학습된다.

사전 학습이 끝난 후에는 멀티 모달 지시 튜닝 단계를 진행한다. 이때 멀티 모달 LIM을 소규모 멀티 모달 지시 데이터셋으로 미세 조정한다.


## 14.2 이미지와 텍스트를 연결하는 모델: CLIP

### 14.2.1 CLIP 모델이란

CLIP은 텍스트 데이터와 이미지 데이터의 관계를 계산할 수 있도록 텍스트 모델과 이미 지 모델을 함께 학습시킨 모델

### 14.2.2 CLIP 모델의 학습 방법

임베딩 모델을 학습시킬 때는 서로 유사 또는 비유사한 문장 쌍을 사용했다. 이와 마찬가지로 CLIP 모델을 학습시킬때는 서로 관련이 있는 이미지와 텍스트 쌍을 활용한다. 

이미지-텍스트 쌍의 데이터란 이미지와 이미지에 대한 설명이 대응된 데이터이다
퀄리티가 높은 데이터셋은 데이터의 수가 적고, 데이터 수 가 많으면 텍스트가 이미지의 의미를 충분히 담지 못한다는아쉬움이 있었다. 이런 이유로 CLIP 모델 연구팀은 직접 인터넷상에서 50만 개의 검색어로 4억 개의 쌍 데이터를 구축했다. 

그 다음 대조 학습을 통해 모델을 학습시킨다. 대조 학습은 문장 임베딩 모델의 학습에서도 사용 됐는데(11장 참고), 유사한 데이터 쌍은 더 가까워지도록 하고 유사하지 않은 데이터 쌍 은 더 멀어지도록 학습시키는 학습 방법을 말한다.

![그림 14.5 CUP 모델 학습 방법: 대조]

### 14.2.3 CLIP 모델의 활용과 뛰어난 성능

학습을 마친 CLIP 모델은 제로샷 추론zero-shot prediction을 수행할 수 있다. 

제로샷 추론이란, 사전 학습 데이터 이외에 특정 작업을 위한 데이터로 미세 조정 하지 않은 상태에서 추론을 수행하는 것을 말한다.

![그림 14,6 CLIP 모델을 활용한 제로샷 추론]

CLIP 모델은 이미지와 텍스트 데이터의 유사도 계산을 활용해 이미지 검색에도 활용 할 수 있다. 

이미지 검색이란 이미지와 텍스트의 유사도를 기반으로 텍 스트를 입력했을 때 유사한 이미지를 찾는 기능을 말한다.

## 14.3 텍스트로 이미지를 생성하는 모델: DALL-E

DALL-E: 멀티 모달 LLM에서 LLM 백본의 텍스트 출력을 기반으로 이미지를 생성하는 데 사용할 수 있는 대표적인 이미지 생성 모델.

### 14.3.1 디퓨전 모델 원리

디퓨전 모델은 확산 현상에서 영감을 받아 만들어진 생성 모델이다. 그림 14.10과 같이 잉크를 물에 떨어뜨리면, 처음에는 잉크 분자가 서로 모여 있지만 점점 퍼져나가면서 물 전체에 균일하게 섞인다.

![그림 14.10 물감이 물 전체로 퍼지는 확산 현상]

예시를 데이터 관점에서 보자면 이미지가 완전히 랜덤한 노이즈로 변하는 과정과 유사하다. 디퓨전 모델은 이미지에서 어떤 부분이 노이즈인지 예측하는 방식으로 학습 하는데, 그 능력을 사용해 완전한 노이즈 상태의 이미지에서 노이즈를 예측하고 예측된 노이즈를 제거하면서 점차 완전한 노이즈에서 의미가 있는 이미지를 생성한다.

![그림 14.11 디퓨전 모델의 원리]

일반적인 이미지에서 노이즈를 예측하는 디퓨전 모델:  U-Net이라는 인코더 디코 더 모델 

인코더 디코더 - 입력 데이터의 차원을 낮추는 인코딩 단계와 차원을 높이는 디코딩 단계를 통해 데이터의 의미를 압축하기 위해 사용되는 모델 구조 
U-Net - 인코더 디코더 구조를 변형해 인코딩 단계의 고차원 정보를 디코딩 단계에도 활용함으로써 이미지의 위치 정보가 손실되는 것을 막을 수 있다는 장점이 있다.

그런데 완전한 노이즈 상태라면 노이즈를 제거했을 때 어떤 이미지가 생성될지 어떻게 알 수 있을까?

그림 14.14와 같이 디퓨전 모델에 노이즈를 넣어주면서 원하는 결과물의 형태를 테스트 임베딩으로 변환해 디퓨전 모델에 함께 입력으로 넣어준다. 그러면 디퓨전 모델이 텍스트 임베딩을 참고해 그림과 같이 원하는 이미지를 생성하게 된다.

![그림 14.14 텍스트 임베딩을 추가해 원하는 이미지 생성]

### 14.3.2 DALL-E 모델

DALL-E 모델은 텍스트 임베딩을 만들고 두 단계를 거쳐 이미지를 생성한다.

1[그림 14.15 DALL-E 2 모델 구조]

먼저, CLIP의 텍스트 인코더를 사용해 입력한 텍스트를 텍스트 임베딩으로 만든다. 그리고 프라이어prior 모델을 통해 CLIP 이미지 임베딩을 만들고 디코더를 사용해 이미지를 생성한다

## 14.4 LLAVA

LLaVA 모델은 이미지를 인식하는 CLIP 모델과 LLM을 결합해 모델이 이미지를 인식하고 그 이미지에 대한 텍스트를 생성할 수 있다.

### 14.4.1 LLaVA의 학습 데이터
멀티 모달 대화 모델을 만들고자 할 때 가장 어려운 부분은 대부분 데이터셋이 부족하다는 점이다. 이 문제를 해결하기 위해 ChatGPT와 CPT-4를 활용해 데이터셋을 생성했다.
 이미지에 대한 설명과 위치 정보 Bounding Box를 통해 GPT-4가 이미지를 인식하도록 했다.

### 14.4.2 !LLaVA 모델 구조

![그림 14.21 LLAVA 모델 구조]

그림에서 입력 이미지(X)를 CLIP의 이미지 인코더vision encoder를 통해 이미지 임 베딩(Z)으로 만들고 간단한 선형 충(그림 14.21의 텍스트 변환 W)을 통과해 LLM에 입력 할 임베딩 토큰(H)으로 만든다. 텍스트 지시사항은 토큰 임베딩(FH)으로 변환해 함께 입력으로 넣고 결과 X₂를 생성한다.

### 14.4.3 LLAVA 1.5

LAVA 1.5 버전은  이미지 인코더를 CLIP의 VIT-L/336px로 바꾸고 한 층의 선형 층으로 이미지 임베딩을 토큰 임베딩으로 변환하던 구조를 2층의 MLPMulti- Layer Perceptron로 변경하는 간단한 수정만으로 성능을 대폭 끌어올렸다

![그림 14.22 LLaVA-1.5 모델 구조(]

LLaVA-1.5는 비교 대상이 된 다른 모델에 비해 훨씬 적은 데이터로 학습하지만 성능은 제일 뛰어나다. 

### 14.4.4 LLAVA NEXT

LLAVA-NeXT는 LLaVA-1.5를 더 발전시킨 모델로, 다음과 같 은 사항이 변경됐다.

- 기존 모델 대비 입력 이미지의 해상도가 4배 높아졌다
- 고품질의 지시 데이터셋을 구축해 시각적 추론 능력과 OCR 성능이 개선됐다
- 더 많은 시나리오에서 응답할 수 있어 다양한 애플리케이션에 활용할 수 있다
- SGLang 프레임워크를 사용해 추론 성능이 높아졌다

# 15. 에이전트

에이전트agent - 같이 알아서 생각하고 행동하는 시스템 (ex: LLM이 알아서 어떤 작업이 필요한지 계획하고 검색, 계산기, 코드 등 다양한 도구를 활 용해 목표를 달성)

## 15.1 에이전트란

### 15.1.1 에이전트의 구성요소

에이전트의 세 가지 구성요소 : 
- 감각: 외부환경과 사용자 요청 인식
- 두뇌: 지식이나 기억을 가지고 의사결정을 내린다
- 행동: 적절한 도구를 선택해 행동한다.

### 15.1.2 에이전트의 두뇌

에이전트의 두뇌는 감각을 통해 현재 상황과 사용자의 요청을 인식한 것을 바탕으로 사용자의 요청이 무엇인지 현재 어떤 상황인지 이해하고 목표 달성을 위해 어떤 행동을 취할지 결정한다.

그 과정에서 지금까지 수행했던 사용자와의 대화나 행동을 저장한 기억memory을 확인한다. 보강한 정보를 바탕으로 에이전트는 목표 달성을 위한 작업을 세분화하는 계획 세우기planning 단계를 거치고 바로 다음에 어떤 행동이 필요한지 결정해 행동 단계로 넘어간다

### 5.1.3 에이전트의 감각

LLM은 기본적으로 텍스트를 입력으로 받고 텍스트를 출력하지만, 멀티 모달 모델은 텍스트와 이미지를 모두 처리할 수 있다. 
이미지와 음성 데이터 이외에도 촉각 센서나 후각 센서를 사용해 촉감 정보나 냄새 정보를 LLM에 전달하거나 gps와 같은 위치 정보, 3D 공간 정보 등을 활용해 LLM에 풍부한 정보를 제공할 수 있다.

### 15.1.4 에이전트의 행동

LLM이 외부에 영향을 미치는 행동을 하기 위해서는 LLM이 사용할 수 있는 도구를 제공해야한다.

> 도구 종류

![그림 15.4 대표적인 도구 종류]

## 15.2 에이전트 시스템의 형태

### 15.2.1 단일 에이전트

AutoGPT는 입력된 프롬프트를 통해 목표, 제약사항, 도구 등을 설정하고, 이를 바탕으로 작업을 계획하고 실행한다. 에이전트의 이름과 역할을 정하고, 목표를 설정하며, 제약사항을 전달하는 방식으로 작동한다. 2023년 초에 공개된 AutoGPT는 긴 입력을 처리하기 어려워 데이터를 파일에 저장하거나 사용자에게 결정을 요청하지 않도록 제약을 두었다.

단일 에이전트는 모든 과정을 스스로 처리해 편리하지만, 작업 수행 중 길을 잃을 위험이 크다. 또한, 에이전트는 LLM의 성능에 크게 의존하므로 LLM 성능이 낮으면 빈번히 실패할 수 있다. 현재로서는 목표가 명확하고 작업이 작은 경우에만 사용하는 것이 좋으며, 이러한 단점을 보완하기 위해 사용자 피드백이나 여러 에이전트의 협력 방식이 필요하다.


### 15.2.2 사용자와 에이전트의 상호작용

특정 도메인에서 LLM의 전문성이 사람에 비해 떨어지는 경우 사용자가 문제 해결 과정에 직접 개입하는 것이 특히 중요하다. 사용자가 명확한 명령과 피드백으로 에이전트의 작업 방향을 설정하고 에이전트는 작은 작업을 자동으로 수행하서 사용자가 빠르고 편하게 작업을 진행하도록 지원할 수 있다.

### 15.2.3 멀티 에이전트

멀티 에이전트 방식의 경우 하나의 역할을 가진 에이전트가 모든 일을 수행하는 단일 에이전트와 달리 각 에이전트마다 서로 다른 프로필profile을 주고 작업을 수행한다. LILM 에 구체적인 프로필을 부여하는 것만으로도 관련된 작업의 전문성을 높여 결과 품질을 높일 수 있기 때문에 에이전트 간의 협력을 통해 작업의 품질이 향상될 수 있다.

> AutoGen이 지원하는 다양한 멀티 에이전트 형태
![그림 15.7 AutoGen이 지원하는 다양한 멀티 에이전트 형태]



## 15.3 에이전트 평가하기

에이전트 평가 방식:
1. 사람이 평가하는 주관적인 방식
1.1 사람이 직접 평가하는 방식
1.2 에이전트가 생성한 것과 사람이 생성한 것을 구분할 수 있는지 확인하는 방식
2. 테스트 데이터로 평가하 는 객관적인 방식 (유용성, 사회성, 가치관, 진화 능력)


튜링 테스트 - 에이전트와 사람의 결과물을 구 분할 수 있는지 확인하는 평가 방식

![그림 15.9 사람이 LLM 에이전트를 평가하는 두 가지 방식]

# 16 새로운 아키텍처

## 16.1 기존 아키텍처의 장단점

크게 자연어 처리를 사용하던 모델을 두 그룹으로 무끙ㄹ 수 있다.

![표 16.1 RNN과 트랜스포어 비교]

트랜스포머보다 연산이 가벼우면서 성능이 높은 모델을 개발하기 위해 RNN을 변형하 는 연구가 꾸준히 있었다. 대표적으로 SSM 계열의 모델은 RNN이 갖고 있는 추론의 효 윤성을 유지하면서 트랜스포머가 가진 학습 시 병렬연산을 가능하게 하겠다는 목표로 개발됐다.

## SSM

맘바는 SSM 계열의 모델이다.

SSM: 내부 상태를 가지고 시간에 따라 달라지는 시스템을 해석하기 위해 사용하는 모델링 방법

일반적으로 SSM은 다음과 같은 식으로 내부 상태와 출력이 시간에 따라 달라지는 과정을 수식화한다. h(t) = Ah(t - 1) + Bx(t) y(t) = Ch(t) + Dx(t)


## S4

S4 - 대표적인 SSM모델 중 하나, SSM모델의 계산 효율성을 높여 쉽게 모델을 학습시킬 수 있도록 한다.

## 16.3 선택 메커니즘

텍스트의 맥락을 저장하는 과정은 일종의 '압축'으로 이해할 수 있다.

그림 16.4에서 왼쪽의 RNN은 입력을 잠재 상태 h에 그대로 누적하는 방식을 사용했는데, RNN을 발전시킨 LSTM 모델은 맥락을 더 효율적으로 압축하기 위해 입력을 얼마나 반영할지, 기존 상태를 얼마나 망각할지 결정하는 게이트를 추가했다.

![그림 16.4 RNN과 LSTM 모델]

## 맘바

### 16.4.1 맘바의 성능

맘바 아키텍처는 높은 성능과 빠른 학습, 추론 속도를 갖춰 연산 부담이 큰 트랜스포머 아키텍처의 강력한 대안으로 주목받고 있다. 맘바가 트랜 스포머를 대체하지는 않더라도 트랜스포머 모델을 개선하고 효율화하는 데 맘바의 아 이디어를 활용하는 등 긍정적인 영향을 주게 될 것이다

### 16.4.2 기존 아키텍처와의 비교

![표 16.4 아키텍처 비교]

## 16.5 코드로 보는 맘바

![그림 16.9 맘바 블록]

코드에 중요한 인자:

- d_model: 토큰 임베딩 차원이다.
- d_inner: 모델 내부적으로 토큰 임베딩을 확장해 사용하는 차원이다. 기본적으로d_model을 2배 확장한다.
- d_state: 선택 메커니즘에서 입력을 확장할 때 사용하는 상태 차원이다


```
class MambaBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)
        self.conv1d = nn.Conv1d(
            in_channels=args.d_inner,
            out_channels=args.d_inner,
            bias=args.conv_bias,
            kernel_size=args.d_conv,
            groups=args.d_inner,
            padding=args.d_conv - 1,
        )
        # ssm 내부에서 사용
        # 입력 x를 확장해 Δ, B, C를 위한 벡터를 생성하는 층
        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)
        # dt_rank차원을 d_inner차원으로 확장해 Δ 생성하는 층
        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)
        A = repeat(torch.arange(1, args.d_state + 1), 'd_state -> d_model d_state',
        d=args.d_inner)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(args.d_inner))
        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)
    def forward(self, x):
        (b, l, d_model) = x.shape
        x_and_res = self.in_proj(x) # shape (b, l, 2 * d_inner)
        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner],
        dim=-1)
        x = rearrange(x, 'b l d_inner -> b d_inner l')
        x = self.conv1d(x)[:, :, :l]
        x = rearrange(x, 'b d_inner l -> b l d_inner')
        x = F.silu(x)
        y = self.ssm(x)
        y = y * F.silu(res)
        output = self.out_proj(y)
    return output
```
