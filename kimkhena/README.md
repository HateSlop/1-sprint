# LLM을 활용한 실전 AI 애플리케이션 개발

## 1.1 딥러닝과 언어 모델링

**LLM (Large Language Models)**은 딥러닝 기반의 언어 모델로, 자연어 처리 분야(NLP)에 속한다.
* 딥러닝 - 인간의 두뇌에 영감을 받아 만들어진 신경망. 정형데이터, 비정형데이터 인식이 높다.
* 자연어 처리 분야 - 사람의 언어를 컴퓨터가 이해하고 생성 할 수 있는 연구 분야.

주요 특징:
- **정형/비정형 데이터**에서 패턴 인식 성능이 높음
- **딥러닝 모델**을 통해 자율적인 학습이 가능함
- **임베딩** 기술을 사용하여 데이터를 숫자형 벡터로 변환함

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝
**딥러닝 문제 해결 방법**
1) 문제 유형에 맞는 일반적으로 사용되는 모델 준비  
2) 풀고자 하는 학습 데이터를 준비  
3) 학습 데이터를 반복적으로 모델에 입력하여 학습 

딥러닝은 머신러닝과 반대로 상당한 양의 **학습데이터**를 제공하기만 하면 시스템이 **자율적으로 학습** 할 수 있다. 머신런닝은 개발자가 데이터의 ‘특징’을 직접 찾고 입력해야 한다.

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

**임베딩(embedding)**은 데이터의 의미와 특직을 포착하여 숫자의 집합으로 변환시킨다.
데이터를 임베딩을 하여 숫자로 표현 한 후, 데이터의 유사함을 거리로 측정이 가능하다. (ex:MBTI)

임베딩의 활용:
- **검색 및 추천**: 검색어와 관련있는 상품 추천
- **클러스터링 및 분류**: 유사 데이터를 묶음
- **이상치 탐지**: 나머지 데이터와 먼 비정상적 데이터를 탐지

단어 임베딩(word embedding) - 단어를 임베딩으로 변환하는 것.
일반적으로 수십-수만개의 숫자로 표현한다. 딥러닝 모델이 데이터 특징을 알아서 추출하기 때문에 각 숫자의 의미를 알기 어렵다. 숫자 집합 을 안다면 단어의 의미를 알 수 있다.


### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

**언어 모델링** - 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식

**전이 학습** - 하나의 문제를 해결하는 과정에서얻은 지식을 다른 문제에 활용하는 방식 (딥러닝 분야에서 애용)
+ 사전 학습 - 대량의 데이터로 모델을 학습
+ 미세 조정 - 특정 데이터를 해결하기 위한 추가 학습 (이 학습 모델을 사용하는 과제를 downtream 과제라도 부른다)

![전이 학습 방식](./picture/photo_2024-09-11_23-02-14.jpg)
본체 - 이미지넷으로 학습한 모델 (점이나 선같은 기본적인 이미지 특징을 이미 파악해 놓은 데이터셋)
___ 분류헤드 - 해결하려는 이미지의 데이터셋을 추가 학습

---

## 1.2 언어 모델이 CHAT GPT가 되기 까지

### 1.2.1 RNN에서 트랜스포머 아키텍처(구조)로

**시퀀스 데이터** - 순서대로 정열된 데이터의 연속. 텍스트, 오디오, 시계열 데이터 등이 이에 해당하며, 이를 처리하기 위해 **RNN** 또는 **트랜스포머**를 사용한다.

**RNN(Recurrent Neural Network)** - 트랜스포머 아키텍처를 사용하기 전 텍스트를 생성하기위해 사용되었던 모델. 정보가 쌓이면서 텍스트 맥락이 압축된다. 단점: 앞에 있는 단어들의 의미가 희석된다
![입력을 순차적으로 처리하는 RNN 모델의 방식](./picture/photo_2024-09-11_23-02-22.jpg)

**트랜스포머 아키텍쳐** - 문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 신경망. 맥락 데이터를 모두 활용한다. 무겁고 비효율적인 연산을 한다. 그러나 RNN의 단점(느린 학습 속도, 긴 시퀀스 처리 성능 저하)을 극복하고 병렬 연산을 가능하다.  
![트랜스포머 아키텍쳐의 어텐션 연산](./picture/photo_2024-09-11_23-02-21.jpg)

* 결론: RRN은 효율적이지만 성능이 낮다. 트랜스포머 아키텍쳐는 성능이 높지만 효율이 낮다. 하지만 병렬처리를 통해 학습속도를 높일 수 있어 대부분 LLM이 트랜스포머 아키텍처를 사용 한다.

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계 

모델 데이터가 많아질 수록 최종 모델의 성능이 높아진다. 학습하는 과정 중 
학습 데이터의 공통되고 중유한 패턴을 남겨 손실 압축한다. 그래서 학습데이터 모델 크기가 최대 모델 크기보다 많을 수 없다.

### 1.2.3 Chat GPT의 등장

**지도미세조정** - 언어 모델링으로 학습한 언어모델을 *지시 데이터 셋*으로
추가 학습한다. ( 데이터셋을 위해 OpenAi에서는 따로 작업자에게 LLM이 받을법한 질문과 답을 작성해야 했다. 그 결과 사용자의 요청에 맞춰 응답하는 모델을 만들 수 있었다)
지시 데이터 셋 - 사용자가 요청한 사항과 그이 대한 적절한 응답을 정리한 데이터 셋

**RLHF(reinforcement Learning from Human Feedback)** - 사람의 피드백을 활용한 강화 학습. 선호 데이터셋으로( 두가지 선택 중 사용자가 더 선호하는 답변을 정리한 데이터셋) 답변을 평가하는 리워드 모델을 만들어 LLM이 높은 점수
를 받게 추가 학습한다.

**지도 미세 조정** 과 **RLHF** 라는 기술 덕분에 사용자의 말을 이어서 작성하는 능력 뿐만이 아니라 사용자의 요청을 해결하는 능력이 생겼다.

##1.3 LLM 애플리케이션의 시대가 열린다

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM 
LLM은 다른 자연어 처리 모델과 다르게 언어를 *이해*하는 것 뿐만이 아니라 결과를 언어로 *생성*해야 하는 능력이 뛰어나다.

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기 

기업들이 LLM을 크게 두가지로 활용한다:
**상업용 API**: 장점: 모델이 크다, 텍스트 생성 능력이 뛰어나다. 단점: 추가 학습이 안된다. 여러 모델이 뒤섞여 있어 특정분야를 잘 모를 수 있다
**오픈소스 LLM**: 장점: 특정 도메인 데이터나 작업에 높은 성능을 보여준다. 단점: 모델 크기가 작다. 이를 **sLLM**이라고 한다

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증각 생성(RAG) 기술
LLM은 정보가 사실인지, 거짓인지 판단을 못한다. 그래서 잘못된 정보가 실제로 존재하지 않는 정보를 만드는 **‘환각 현상’**이 생긴다.
이를 해결하기 위해 **RAG**이라는(응답을 생성하기 전에 학습데이터에 신뢰 할 수 있는 지식 베이스를 참조하게 하는 기술) 기술을 사용한다.

(예: 위키페디아에서 참조)

## LLM의 미래: 인식과 행동의 확장
AI분야는 다양한 방향으로 연구가 진행되고 발전해 나가고 있다. 

- 멀티모달 LLM: 다양한 형식의 데이터(ex:오디노,비디오)를 입력으로 받고 여러 형태로 출력 할 수 있는 LLM 모델
- 에이전트: 텍스트 생성 능력을 사용해 계획을 세우거나 늬사결정을 내리고 필요한행동을 수행하는 LLM 모델
- 트랜스포머 아키텍쳐보다 성능이 높고 효율적인 아키텍쳐 연구도 진행하고 있다.  

---

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기

## 2.1 트랜스포머 아키텍처란

**RRN**의 모델 구조는 모델의 한번에 한 토큰씩 입력을 받아 추출을 한다. 이 출력을 다시 다음 입력 토큰과 함께 RRN에 입력해야 하기 때문에 
**비효율적**이다. (사람이 글을 읽는 것과 비슷) (학습속도가 느려지고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어진다.)

![RRN 모델 구조](../static/img_2.2_텍스트_임베딩.png)
> RRN 모델 구조

이를 해결하기 위해 트랜스포머는 **셀프 어텐션(self attention)**을 사용한다. 이는 인코더와 디코더에 가장 중요한 부분이다. 
셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관계인지 계산해서 각 단어의 표현을 조정한다.

트랜스포머 장점:
- 확장성: 더 깊은 모델을 만들어도 학습이 잘된다. 
- 효율성: 학습할때 병렬연산이 가능하기 때문에 학습 시간이 단축된다. 
- 더 긴 입력처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.

트랜스포머 아키텍쳐는 크게 **인코더**(언어를 이해하는 역할)와 **디코더**(언어를 생성하는 역할)로 나뉜다. 
- 인코더와 디코더 모두 처음에 모델을 토큰화 후 임베딩 층을 통해 임베딩(숫자 집합) 으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다
- 인코더에서 층 정규화(layer normalization), 멀티 헤드 어텐션(multi-head attention), 피드 포워드(feed forward)층을 거쳐 디코더로 전달한다.
- 디코더에서 층 정규화(layer normalization), 멀티 헤드 어텐션(multi-head attention)을 수행하면서 크로스 어텐션 연산을 한다
- 디코더와 인코더의 출력을 종합해서 피드 포워드 층을 거쳐 결과를 생산한다.

![트랜스포머 아키텍쳐를 구성하는 인코더와 디코더](./picture/photo_2024-09-11_23-02-19.jpg)
> 트랜스포머 아키텍쳐를 구성하는 인코더와 디코더

## 2.2 텍스트를 임베딩으로 변환하기
컴퓨터가 텍스트를 이해하고 계산 하기 위해 텍스트를 숫자 형식으로 바꿔야 한다.
1) **토큰화(tokenization)** 수행: 텍스트를 적절한 단위로 잘라 숫자형 ID를 부여한다
2) 토큰 ID를 **토큰 임베딩 층**을 통해 여러 숫자의 집합인 **토큰 임베딩**으로 변환한다.
3) **위치 인코딩**을 통해 토큰의 위치 정보를 담고 있는 임베딩을 추가 해 최종적으로 모델에 입력 할 임베딩을 만든다. 

![텍스트에서 임베딩으로 변환하는 과정](./picture/photo_2024-09-11_23-02-18.jpg)
> 텍스트에서 임베딩으로 변환하는 과정

### 2.2.1 토큰화 

**토큰화** - 텍스트를 적절한 단위로 나누고 ID를 부여하는 것
**서브워드 토큰화** - 자주 나오는 단어일수록 그대로 유지하여 토큰화

**토큰화 방법:**
1) 문장을 단위로 자른다.(한국어는 자모, 음절, 단어단위로 자를 수 있다.)
2) 토큰 딕셔너리를 만든다.(각각 단어에게 순서대로 토큰을 부여한다) (str2idx)
3) ID 딕셔너리를 만든다. (ID를 순서대로 각각 단어에게 부여된다) (idx2str)
4) ID 딕셔너리에서 토큰을 토큰 딕셔너리에서 가져온 토큰 ID로 변환한다.

![토큰화 방식](./picture/photo_2024-09-11_23-02-17.jpg)
> 등장빈도와 단위에 따른 서브워드 토큰화 방식


### 2.2.2 토큰 임베딩으로 변환하기
딥러닝 모델이 토큰 사이에 관계를 숫자로 계산해야 한다. 토큰ID는 숫자 하나일뿐이라서 의미를 못 담는다. 그래서 1.1.2에서 MBTI예시처럼 임베딩을 만들어야 한다. 
**임베딩** - 데0이터의 의미를 담아 숫자 집합으로 변환하는 것

이를 위해 PyTorch에 제공하는 no.Embedding 클래스를 사용해야 한다.
1) 임베딩 벡터의 차원을 정한다. (embedding_dim)(ex: 3차원: [0.0, 0.0, 0.0],[0.2,0.4,0,1]…)
2) 차원에 임베딩을 생성하는 임베딩 레이어(embed_layer)를 정한다. 이는 토큰 아이디 집합의 크기에 따른다.
3) 입력 토큰을 임베딩 층을 통해 임베딩으로 변환한다.

위 과정을 거친다고 해서 토큰의 의미가 담겨 벡터로 변환되는 것은 아니다. 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련되어야 한다.
딥러닝은 머신러닝과 달리 직접 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 점점 토큰의 의미를 잘 담는 임베딩을 같이 학습하게 된다.

### 2.2.3 위치 인코딩

위치 인코딩이 필요한 이유: 트랜스포머 아키텍쳐에서 RRN과 달리 모든 토큰 입력을 동시에 처리하는데, 그 과정에서 **순서 정보를 따로 추가**하기 위해 존재한다.

**절대적 위치 인코딩(absolute position encoding)**- 위치 인덱스 자체를 위치 임베딩으로 사용한다.
장점:간단하게 구현이 가능하다. 단점: 토큰과 토큰 사이에 상대적 위치 정보를 활용 못한다. 긴 텍스트를 추론하는 경우 성능이 떨어진다. 
상대적 위치 인코딩 (relative position encoding) - 절대적 위치 인코딩 단점을 보완한다. 

트랜스포머가 있는 지금은 모든 입력 토큰을 동일하게 처리하시 떄문에 위치 정보를 더해주는 역할만 한다.

*(인코딩, 임베딩 차이가 뭐지???)*

## 2.3 어텐션 이해하기

### 2.3.1 사람이 글을 읽는 방법과 어텐션

단어의 뜻을 찾으려면 단어와 단어 사이에 관계를 계산 해 그 값에 따라 관련이 깊은 단어와 깊지 않는 단어를 필터링 한다. **어텐션**이 이 기능을 수행한다.

어텐션은 문장에서 각 단어 간의 관련성을 학습하는 메커니즘입니다. **쿼리(Query)**, **키(Key)**, **값(Value)** 세 요소를 통해 작동한다.

### 2.3.2 쿼리, 키, 값 이해하기
검색 분야에서의 비유: 
- **쿼리**- 검색어 (단어)
- **키**- 문서 제목,본문,저자 (문장 속 각 단어)
- **값**- 키의 문서 (단어가 전달하려는 정보) (이것을 검색하면서 원하는 것이다)

쿼리와 키 관계 계산 방법: 쿼리와 키를 **토큰 임베딩**으로 변환하여 관계를 계산해서 관련도를 계산한다. 
그러나 이러면 같은 단어끼리(ex:파리(Paris,fly)) 임베딩이 똑같아서 관련도가 크게 계산된다. 또한 간접적인 관련성이 반영되기 어렵다.(ex: ‘나는‘, 
‘최근’은 ‘다녀왔다’토큰에 누가,언제를 나타내는 문법관계이지만 토큰 자체로 봤을때는 관련성을 찾기 어렵다.)
이를 해결하기 위해 **가중치**(Wq,Wk)를 도입했다. (딥러닝에서는 어떤 기능을 잘하고 싶을 때 가중치를 도입하고 학습단계에서 업데이트 되게 한다.)
가중치를 통해 토큰과 토큰 사이에 관계를 계산하는 능력을 학습시킨다.

스케일 점곱 방식의 어텐션 연산 코드:
1) 가중치 계산
1.1) PyTorch에서 제공하는 nn.Linear 층을 사용해 쿼리,키,값 각각의 가중치를 생성한다. 
1.2) 입력 임베딩(input_embedding)을 선형 층에 통과시켜 쿼리, 키, 값을 생성한다.
2) 쿼리, 키, 값 관계 를 계산한 새로운 단어 벡터 생성 
2.1) 쿼리와 키를 곱해 두 단어의 관련성을 계산한다. (분산이 커지는 것을 방지하기 위해 임베딩 차원 수의(dim_k)의 제곱근으로 나눈다.)
2.2) 그 값들을 소프트맥스(softmax)를 취해 0-1사이에 값으로 바꿔서 가중치로 바꾼다.
2.3) 가중치와 값을 곱해 입력은 동일하면서 주변 토큰과의 관련도에 따라 새로운 토큰 임베딩을 반환한다
이로서 트랜스포머에서는 쿼리,키,값을 토큰 임베딩 하여 가중치를 통해 변환한다. 이 세가지 가중치를 통해 토큰과 토큰 사이에 관계를 계산해 적절한 주변 맥락을 반영하는 학습을 시킨다.

![가중치 계산](./picture/photo_2024-09-11_23-02-30.jpg)
> 가중치 계산

![값 벡터를 가중합해서 새로운 단어 벡터 생성](./picture/photo_2024-09-11_23-02-29.jpg)
> 값 벡터를 가중합해서 새로운 단어 벡터 생성

### 2.3.4 멀티 헤드 어텐션 

**멀티 헤드 어텐션** - 어텐션 연산을 동시에 수행하는 어텐션 방식. 이를 통해 토큰 사이에 다양한 관점에서 단어 간에 관계를 학습 할 수 있다.
여러개에 어텐션을 동시에 사용하여 문맥 이해에 도움이 된다.

멀티 헤드 어텐션 코드:
1) 쿼리,키,값의 각각 선형층을 n_head(헤드의 수)로 나눠서 각각 변환 후 입력과 같은 형태로 변환한다.
2) 쿼리, 키, 값의 각각 어텐션 계산한다
3) 어텐션 결과를 다시 원래 현태로 변환한다
4) 마지막으로 선형 층을 통과시키고 최종 어텐션 결과를 반환한다.

## 2.4 정규화와 피드 포워드 층 

**정규화** - 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법이다. 

과거에는 배치 정규화를 사용하였지만, 지금은 트랜스포머 아키텍처에서는 특정 차원에서 정규화를 수행하는 층 정규화를 사용한다. 

어텐션 연산 - 단어와 단어 사이 의미를 계산하여 토큰 임베딩을 조정하는 연산
피드 포워드 층 - 전체 입력 문장을 계산하여 연산

### 2.4.1 층 정규화 이해하기 

딥러닝 모델에서 **정규화가 필요한 이유**: 입력 데이터가 딥러닝 모델의 각 층을 거치면서 어떤 특성은 좁은 분포를 갖고 어떤 특성은 넓은 분포를 갖게 된다. 특히 층이 깊은 모델에서는 분포 의 차이가 발생할 가능성이 높아지고 그렇게 되면 학습이 잘되지 않는다. 그래서 입력 변수가 데이터 분포를 비슷한 범위 내에 조정하여 <u>정확한 예측</u>을 하기 위해서다. 

다음과 같은 식으로 계산한다. 벡터 x를 정규화한 norm_x는 벡터 x에서 x의 평균을 빼고 x의 표준편차로 나 눠 평균이 0이고 표준편차가 1인 분포를 갖게 된다.
- `norm x = (x-평균)/표준편차` 

일반적으로 이미지 처리에서는 배치 정규화를 사용하고, 자 연어 처리에서는 층 정규화를 사용한다.

**배치 정규화** -모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행.

**배치 정규화의 한계**: 자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양해서 정규화에 포함되는 데이터의 수가 제각각이라 정 규화 효과를 보장하기 어렵다.

예시:
![그림 2.18 자연어 처리에서 배치 정규화의 한계](./picture/photo_2024-09-11_23-02-28.jpg)

<u>층 정규화</u>는 이런 단점을 보완한다.

**층 정규화** - 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다.

![그림 2.19 자연어 처리에서 층 정규화의 장점](./picture/photo_2024-09-11_23-02-27.jpg)
 
층 정규화를 작용하는 순서로 옛날에는 사후 정규화를 사용하였지만 요즘에는 사전 정규화를 사용한다.
**사전 정규화** - 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통과하는 방법

![그림 2.20 (a) 사후 정규화와 (b) 사전 정규화](./picture/photo_2024-09-11_23-02-26.jpg)


### 2.4.2 피드 포워드 층

**피드 포워드 층feed forward layer **- 데이터의 특징을 학습하는 완전 **연결 충fully connected layer** 말한다.

코드와 같이 <u>선형 중, 드림아웃 중, 중 정규화, 활성 함수</u>로 구성된다. 
쉽게 층을 쌓아 확장 하기 위해 입력과 출력의 형태가 동일하도 록 맞춘다.

```

class PreLayerNormFeedForward(nn.Module):
  def __init__(self, d_model, dim_feedforward, dropout):
    super().__init__()
    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1
    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2
    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1 (모든 건형 층이 성능이 햘상되게 일부 선형층을 아웃)
    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2
    self.activation = nn.GELU() # 활성 함수
    self.norm = nn.LayerNorm(d_model) # 층 정규화

  def forward(self, src):
    x = self.norm(src)
    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))
    x = self.dropout2(x)
    return x
```

## 2.5 인코더 


인코더는 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태다.   
잔차 연결 - 입력을 다시 더해주는 형태. 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층를 연결해서 안정적인 학습 가능하다

### 2.6 디코더 

**디코더,인코더 첫 번째 차이:** 인코더: 기본적인 멀티 헤드 어텐션을 사용. 디코더: **마스크 멀티 헤드** 어텐션을 사용. 

디코더는 **인과적causal **또는 **자기 회귀적auto-regressive** 특징을 갖는다.(  앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다.)

미래 시점에 작성해야 하는 텍스트를 미리 학습하지 않기 위해 그 이 전에 생성된 토큰까지만 확인 할 수 있도록 마스크를 추가한다.

1) 대각선 아래 부분만 1을 추가, 나머지는 음의 무한대로 마스크를 만든다.
2) 가중치를 만들기 위해 소프트맥스를 취할떄 마스크가 된 곳은 음의 무한대로 가중치가 0이 된다. 

![그림 2.23 마스크 연산](./picture/photo_2024-09-11_23-02-24.jpg)

**디코더,인코더 두 번째 차이:** 인코더의 결과를 디코더가 활용하는** 크로스 어텐션** (다른 두 데이터 집합 간의 상관관계를 학습하는 방식)연산이 있다.

## 2.7 BERT, GPT, T5등 트랜스포머를 활용한 아키텍처

트랜스포머 아키텍처를 활용한 모델. 
1. 인코더만 활용해 자연어 이해 작업에 집중한 그룹 
2. 디코더만 활용해 자연어 생성 작업에 집중한 그룹
3. 인코더와 디코더를 모두 활용해 더 넓은 범위의 작업을 수행할 수 있도록 한 그룹 

![표 2.1 각 모델 그룹의 장단점](./picture/photo_2024-09-11_23-02-23.jpg)

## 2.8 주요 사전 학습 메커니즘 

### 2.8.1 인과적 언어 모델링 
**인과적 언어 모델링** - 문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식. 이전 단어를 바탕으로 다음 단어를 예측

'A라는 단어 다음에는 B라는 단어가 자주 온다'는 직관을 배운다. 언어 모델도 인과적 언어 모델링 방식으로 많은 데이터에 대해 다음 단어를 예측하는 방법을 학습함으로써 <u>더 가능성이 높은 단어를 생성</u> 하는 능력을 갖추게 된다.

GPT 같은 생성 트랜스포머 모델에서는 인과적 언어 모델링을 핵심적인 학습 방법으로 사용한다.

### 2.8.3 마스크 언어 모델링

**마스크 언어 모델링** - 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습 시킨다

![그림 2.28 MLM 학습 방식](./picture/photo_2024-09-11_23-02-34.jpg)

# 03 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리

## 3.1 허깅페이스 트랜스포머란
**허깅페이스 트랜스포머** - 공통된 인터페이스로 다양한 트랜스포머 모델을 활용할 수 있도록 지원하는 오픈소스 라이브러리.

허깅페이스 제공하는 라이브러리:
-  트랜스포머 모델과 토크나이저를 활용할 때 사용하는 **transformers 라이브러리**
- 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원하는 **datasets 라이브 러리** 

> 예제 3.1. BERT와 GPT-2 모델을 활용할 때 허깅페이스 트랜스포머 코드 비교


```
from transformers import AutoTokenizer, AutoModel

text = "What is Huggingface Transformers?"
# BERT 모델 활용
bert_model = AutoModel.from_pretrained("bert-base-uncased")
bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
encoded_input = bert_tokenizer(text, return_tensors='pt')
bert_output = bert_model(**encoded_input)
# GPT-2 모델 활용
gpt_model = AutoModel.from_pretrained('gpt2')
gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')
encoded_input = gpt_tokenizer(text, return_tensors='pt')
gpt_output = gpt_model(**encoded_input)
```

## 3.2 허깅페이스 허브 탐색하기

**허깅페이스의 허브** -  다양한 사전 학습 모델과 데이터셋을 탐색하고 쉽게 불러와 사용 할 수 있도록 제공하는 기능.

**Spaces** - 자신의 모델 데모를 제공하고 다른 사람의 모델을 사용해 볼 수 있는 기능

### 3.2.1 모델 허브
모델 허브에는 작업, 언어 등다양한 기준으로 모델이 분류되어 있는 모델들을 제공한다.

### 3.2.2 데이터셋 허브
데이터셋 허브는 모델 허브와 거의 비슷한 형태로 데이터셋 크기, 유형 등 분류 기준이 추가 되어 있다. 또한 선택한 기준에 맞는 데이터셋을 보여준다. 

### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스 
**스페이스** - 사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능이다.

또한 허깅페이스는 <u>다양한 오픈소스 LLM</u>과 그 <u>성능 정보</u>를 게시하는 **리더보드 Leaderboardg** 운영하고 있다.

많은 오픈소스 모델이 새롭게 공개 되고 있기 때문에 어떤 모델을 사용하는 것이 좋을지 판단하기 쉽지 않다. 이럴 때 리더보드를 살펴보면 각 모델의 크기와 성능을 한눈에 비교할 수 있기 때문에 탐색에 큰 도움이 된다.

## 3.3 허깅페이스 라이브러리 사용법 익히기
모델을 학습시키거나 추론하기 위해서는 모델, 토크나이저, 데이터셋이 필요한다. 이번 절에서  코드를 통해 사용하는 법을 익힌다.
### 3.3.1 모델 활용하기

허깅페이스에서는 모델을** 바디body**와 **헤드head**로 구분한다.
바디 - 모델의 기본 구조
헤드 - 특정 작업을 처리하도록 설계된 부분

![그림 3.9 바디와 헤드의 구분이 필요한 이유](./picture/photo_2024-09-11_23-02-43.jpg)

### 3.3.2 토크나이저 활용하기 
**토크나이처** - 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환한다. 필요한 경우 특수 토큰을 추가하는 역할도 한다.

토크나이저도 허깅페이스 모델 저장소 아이디를 통해 불러올 수 있다. 허깅페 이스 허브에서 **모델**(config.json)과 **토크나이저**((tokenizer_config.json - 토크나이저의 종류나 설정) (tokenizer.json - 실제 어휘 사전)를 불러오는 경우 동일한 모델 아이디로 맞춰야 한다.

> 예제 3.8. 토크나이저 불러오기
```
from transformers import AutoTokenizer
model_id = 'klue/roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_id)
```
> 예제 3.9. 토크나이저 사용하기

```
tokenized = tokenizer("토크나이저는 텍스트를 토큰 단위로 나눈다")
print(tokenized)
# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2],
#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))
# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']

print(tokenizer.decode(tokenized['input_ids']))
# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]

print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))
# 토크나이저는 텍스트를 토큰 단위로 나눈다
```

특수 토큰:
[CLS]: 주로 문장을 시작하는 토큰 , 주로 분류 작업에 사용된다.
[SEP]: 주로 문장을 끝내는 토큰, 두 문장을 토큰하면 구분하는 역할도 한다

### 3.3.3 데이터 셋 활용하기 

datasets 라이브러리를 사용하면 앞서 허깅페이스 허브에서 살펴봤던 데이터셋을 코드로 불러올 수 있다.

> 예제 3.15. KLUE MRC 데이터셋 다운로드

```
from datasets import load_dataset
klue_mrc_dataset = load_dataset('klue', 'mrc')
# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')
```

로컬 데이터 셋을 다운로드 받고 싶다면 예제에 있는 코드로 파일이나 파이썬 객체를 받아 사용 할 수 있다.
아래 코드를 실행하기 위해서는 구글 코랩에 csv 파일이 업로드 되어야 한다.
>예제 3.16. 로컬의 데이터 활용하기

```
from datasets import load_dataset
# 로컬의 데이터 파일을 활용
dataset = load_dataset("csv", data_files="my_file.csv")

# 파이썬 딕셔너리 활용
from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)

# 판다스 데이터프레임 활용
from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)
```

## 3.4 모델 학습시키기
한국어 기사 제목을 바탕으로 기사의 카테고리를 분류하는 텍스트 분류 모델을 학습하는 실습 진행

## 3.5 모델 추론하기 
### 3.5.1 파이프라인을 활용한 추론

허깅페이스는 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 pipeline을 제공한다.

>학습한 모델을 불러와 pipeline을 활용해 추론하기

```
from transformers import pipeline

model_id = "본인의 아이디 입력/roberta-base-klue-ynat-classification"

model_pipeline = pipeline("text-classification", model=model_id)

model_pipeline(dataset["title"][:5])
```

### 3.5.2 직접 추론하기 

>예제 3.31. 커스텀 파이프라인 구현


```
import torch
from torch.nn.functional import softmax
from transformers import AutoModelForSequenceClassification, AutoTokenizer

#CustomPipeline의 인스턴스를 호출할 때 내부적으로 _call_ 메서드를 사용하는데, tokenizer를 통해 토큰화를 수행
class CustomPipeline:
    def __init__(self, model_id):
        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model.eval()

    def __call__(self, texts):
        tokenized = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
#모델 추론을 수행
        with torch.no_grad():
            outputs = self.model(**tokenized)
            logits = outputs.logits
#가장 큰 예측 확률을 갖는 클래스를 추출해 결과로 반환
        probabilities = softmax(logits, dim=-1)
        scores, labels = torch.max(probabilities, dim=-1)
        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]

        return [{"label": label, "score": score.item()} for label, score in zip(labels_str, scores)]

custom_pipeline = CustomPipeline(model_id)
custom_pipeline(dataset['title'][:5])
```
# 04 말 잘 듣는 모델 만들기 
## 4.1 코딩 테스트 통과하기: 사전 학습과 미세 조정
### 4.1.1 코딩 개념 익히기: LLM의 사전 학습 

라마 모델 LLM의 사전 학습 방법: 코드,블로그, 기사, 광고 등 약 10TB 분량의 여러가지 글을 학습하게 한다. 

사전 학습 동안은 ILM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어 를 점점 더 잘 예측하게 된다.

다음 단어를 예측하는 언어 모델을 학습시킬 때는 학습 데이터의 일부를 입력으로 넣고 바로 다음에 나오는 정답 토큰을 맞추도록 학습한다. 그림에 보이듯이 수많은 학습 데이터에 대해 수행하면서 어떤 단어가 다음에 올지 학습하게 된다.

>언어 모델은 정답 토큰이 올 확률을 높이는 방식으로 학습한다

![그림 4.3 언어 모델은 정답 토큰이 올 확률을 높이는 방식으로 학습한다](./picture/photo_2024-09-11_23-02-42.jpg)   

### 4.1.2 연습문제 풀어보기: 지도 미세 조정
**지도 미세 조정** - 사전 학습한 모델링 토대로 요청의 형식을 적절히 해석하고, 응당의 형태를 적절히 작성하며, 요청과 응답이 잘 연결되도록 추가 학습하는 것 

- 지도 미세 조정의 **지도**는 학습 데이터에 정답이 포함되어 있다는 뜻이다.
- 정렬: 사용자의 요청과 LLM의 응답이 정렬되도록 하는 것

**지시 데이터 셋** - 사용자가 요청한 사항에 대한 적절한 응답을 정리한 데이터 셋

문제점: 딥러닝 모델은 학습 데이터에 의존해 행동을 학습하므로, 요청에 응답하는 데이터가 적으면 그 행동을 잘 학습하지 못한다.

이런 문제를 보완하기 위해 사용자의 요구사항과 그에 대한 응답(정답)을 구조화한 데 이터를 구축하고 언어 모델의 학습에 활용한다.

지도 미세 조정도 사전 학습처럼 다음 단어를 예측하는 인과적 언어 모델링 causal language modeling을 사용해 학습한다.

### 4.1.3 좋은 지시 데이터셋이 갖춰야 할 조건 
- 지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다.
- 가설: 적지만 가치가 높은 데이터셋이 많지만 품질이 떨어지는 데이터셋보다 높은 성능을 달성 할 수 있다.

피상적 정렬 가설: 모델의 지식이나 능력은 사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델이 능력과 지식을 어떻게  나열하는지 정도만 추가로 배움

 **좋은 지시 데이터셋이 갖춰야 할 조건 **
- 지시 데이터셋을 작은 규모로 구축하더라도 모델이 지시사항의 형식을 인식하고 답변하도록 만들 수 있다.
-지시사항이 다양한 형태이고 답변의 품질이 높을수록 모델의 답변 품질도 높아진다.
- 학습 데이터의 품질을 높이기 위해 모델의 목적에 맞춰 학습 데이터의 교육적 가 치를 판단하고 교육적 가치가 낮은 데이터를 필터링하는 방법을 사용할 수 있다.
- 교재의 예제 데이터와 같은 고품질의 데이터를 학습 데이터에 추가하면 성능을 크 게 높일 수 있다.

## 4.2 채점 모델로 코드 가독성 높이기
### 4.2.1 선호 데이터셋을 사용한 채점 모델 만들기
선호 데이터셋: 두 데이터 중 사람이 더 선호하는 데이터를 모은 데이터셋

선호 점수를 매기지 않는 이유: 코등서 직접 매긴 데이터셋은 구축하기 어려움.

![그림 4.10 챗GPT를 학습하는 과정에서의 선호 데이터셋 구축과 리워드 모델 학습](./picture/photo_2024-09-11_23-02-40.jpg)   

### 4.2.2 강화 학습: 높은 코드 가독성 점수를 향해 
**RLHF(Reinforcement Learning from Human Feedback)** - 사람의 피드백을 이용한 강화 학습

![그림 4.11 강화 학습에서 에이전트가 학습하는 방식](./picture/photo_2024-09-11_23-02-39.jpg)   
**에이전트**는 보상을 많이 받으려고 행동을 수정하고 학습한다. 이 **보상**을 받기 위해서는 **행동**을 하여 **환경**의 **상태**를 바꿔야 한다. 이 모든 행동을 연속적으로  수행하는 행동의 모음을 **에피소드**라고 한다. 

![그림 4.12 강화 학습의 관점에서 나타낸 코딩 테스트 문제 풀이와 채점 과정](./picture/photo_2024-09-11_23-02-38.jpg)   
![그림 4.13 강화 학습의 관점에서 언어 모델의 텍스트 평가 과정](./picture/photo_2024-09-11_23-02-37.jpg) 

### 4.2.3 PPO: 보상 해킹 피하기 
문제점: 강화 학습 중 그런데 이때 보상을 높게 받는 데에만 집중하는 보상 **해킹reward hacking** 이 발생할 수 있다. 
예를 들어, 코드 가독성 점수를 높게 받는 방법으로 깔끔한 코드를 작성하는 게 아니라 아예 코드를 작성하지 않거나 print("hello world") 같은 간단한 코드만 작성해서 코드 가득성 점수를 높게 받으려고 할 수 있다.

해결법: **PPO 강화 학습 ** - 지도 미세 조정 모델을 기준으로 학습하는 모델이 너 무 멀지 않게 가까운 범위에서 리워드 모델의 높은 점수를 찾도록 한다는 의미

 ### 4.2.4 RLHF: 멋지지만 피할 수 있다면...

LHF의 어려움
- 성능이 높고 일관성 있는 리워드 모델이 필요
- 참고 모델, 학습 모델, 리워드 모델 총 3개의 모델이 필요하기 때문에 GPU와 같은 리소스가 더 많이 필요하다.
- 하리퍼파라미터에 민감하다

## 4.3 강화 학습이 꼭 필요할까?
### 4.3.1 기각 샘플링: 단순히 가장 점수가 높은 데이터를 사용한다면?

기각 샘플링 - 지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아 다시 지도 미세 조정을 수행한다. 

장점: 강화 학습을 사용하지 않기 때문에 학습 이 비교적 안정적이고 간단하고 직관적인 방법임에도 효과가 좋아 많이 활용된다

> 그림 4.16 라마-2의 학습 과정에 사용된 기각 샘플링
![그림 4.16 라마-2의 학습 과정에 사용된 기각 샘플링](./picture/photo_2024-09-11_23-02-35.jpg)   

### 4.3.2 DPO: 선호 데이터셋을 직접 학습하기

DPO 방식 : 리워드 모델이나 강화 학습을 사용하지 않고 선호 데이터셋을 직접 학습한다
DPO 방 법은 RLHF에 비해 훨씬 단순하면서 효과적이어서 많은 연구자와 개발자들이 환호했다.

![그림 4.17 DPO와 RLHF의 차이](./picture/photo_2024-09-11_23-02-50.jpg)   

### 4.3.3 DPO를 사용해 학습한 모델들

# 2부 LLM 길들이기

# 05 GPU 효율적인 학습 



## 5.1 GPU에 올라가는 데이터 살펴보기 

### 5.1.1. 딥러닝 모델의 데이터 타입

**딥러닝 모델**이 입력 데이터를 처리해 결과를 내놓을 때까지 많은 **행렬 곱셈 연산**을 처리한다.

**GPU**는 이렇게 단순한 곱셈을 동시에 여러 개 처리하는 데 특화된 처 리 장치다. 따라서 딥러닝 모델의 연산을 빠르게 처리하기 위해 GPU를 많이 활용한다.

딥러닝 모델은 과거에 32비트 부동소수점 형식을 사용했지만, 모델의 파라미터가 늘어나면서 메모리 용량과 계산 시간 문제가 발생했다. 이를 해결하기 위해 성능을 유지하면서 더 적은 비트의 데이터 타입을 사용하는 방향으로 발전했다.
 
S - 부호 sign 
E -  지수 exponent - 수를 표현할 수 있는 범위의 크기를 결정한다
M - 가수 mantissa -표현할 수 있는 수의 촘촘함을 결정한다

![그림 5.1 데이터 형식에 따른 표현 범위](./picture/photo_2024-09-11_23-02-49.jpg)   

### 5.1.2. 양자화로 모델 용량 줄이기
**양자화** - 더 적은 비트로 모델을 표현하는 기술 (예: 32bit 를 16bit으로 줄임)

모델 파라미터의 데이터 타입이 더 많은 비트를 사용할수록 모델의 용량이 커지기 때문에 필요한 기술이다.

그러나 변환하면 정보가 정보가 소실되어,  <u>양자화를 수행하면 딥 러닝 모델의 성능이 저하된다</u>. 따라서 양자화 기술에서는 더 적은 비트를 사용하면서도 원본 데이터의 정보를 최대한 소실 없이 유지하는 것이 핵심 과제라고 할 수 있다.

**퀀타일 방식** - 입력 데이터를 크기 순으로 등수를 매겨 각 데이터에 양자화 하는 방식. 

### 5.1.3 GPU 메모리 분해하기

GPU 메모리에는 다음과 같은 데이터가 저장된다.   
- 모델 파라미터
- 그레이디언트
- 옵티마이저 상태
- 순전파 상태 

딥러닝 학습 과정:
1) 순전파 수행
2) 그때 계산한 손실로부터 역전파 수행
3) 옵티마이저를 통해 모델 업데이트
이때 역전파를 수행하기 위해 저장하고 있는 값들이 순전파 상태 값이다. 그레이디언트는 역전과 결과 생성된다.

배치 크기가 증가해도 모델, 그레이디언트, 옵티마이저 상태를 저장하는 데 필요한 GPU 메모리는 동일한다. 총 메모리가 증가하는 것을 통해 순전파 상태의 계산에 필요한 메모리가 증가한다. 

## 5.2 단일 GPU 효율적으로 활용하기
### 5.2.1 그레이디언트 누적 
**그레이디언트 누적** - 메모리 제약을 해결하기 위해 제한된 메모리 안에서 배치 크기를 키우는 것과 동일한 효과를 얻는 방법 (각 모델을 풀지 않고 모델들의 틀린점을 공톰점을 찾아 그레이디언트를 하는 것)

### 5.2.2 그레이디언트 체크포인팅 
**그레이디언트 체크포인팅** - 순전파의 계산 결과를 전 저장하지 않고 중간중간에 값들을 저장해 메모리 사용량을 줄이고 필요한 경우 체크포인트부터 다시 계산해 순전파 계산량을 줄이는 방식

![그림 5.9 그레이디언트 체크포인팅 방식을 사용했을 때의 순전파와 역전파 결과 저장 ](./picture/photo_2024-09-11_23-02-48.jpg)   

## 5.3 분산 학습과 ZeRo
### 5.3.1 분산 학습 

분산 학습 - GPU를 여러 개 활용해 딥러닝 모델을 학습

**데이터 병렬화** - 여러 GPU에 각각 모델을 올리고 학습 데이터를 병렬로 처리해 학습 속도를 높이는 방식

모델 병렬화 : 모델을 여러 개의 GPU에 나눠서 올리는 방식
1)파이프라인 병렬화 - 모델의 층별로 나눈다
2) 텐서 병렬화 -  한 층의 모델도 나눈다

그림 5.11에서 모델을 그림 의 상하로 나누면(머신 1, 2와 머신 3, 4로 구분) 파이프라인 병렬화이고 좌우로 나누면(머 신 1.3과 머신 2, 4로 구분) 텐서 병렬화에 해당한다.

![그림 5.11 하나의 모델을 나눠 여러 GPU에 올리는 모델 병렬화](./picture/photo_2024-09-11_23-02-47.jpg)   

데이터 병렬화 장점: 모델을 각 GPU에 올리고 학습 데이터를 나눠 동시에 학습 해서 학습 속도를 높인다
단점: 동일한 모델을 여러 GPU에 올려 중복으로 메모리를 차지하기 때문에 메모리 관 점에서는 비효율적이다

### 5.3.2 데이터 병렬화에서 중복저장 줄이기(ZeRO)

데이터 병렬화의 단점을 극복하기 위해  ZeRO 사용. 각 GPU가 모델을 부분적으로 가지고, 필요한 순간에만 모델 파라미터를 복사해 연산을 수행하는 방식으로 메모리를 효율적으로 사용한다.

## 효율적인 학습 방법(PEFT) LoRA 
### 5.4.1 모델 파라미터의 일부만 재구성해 학습하는 LoRA 

PEFT(Parameter Efficient Fine-Tuning) : 일부 파라미터만 학습 
**LoRA** - PEFT의 일종으로 모델 파라미터를 재구성해 더 적은 파라미터를 학습함으로써 GPU 메모리 사용량을 줄인다.

LoRA를 통해 GPU 메모리 사용량이 줄어드는 부분은 바로 그레이디언트와 올티마 이저 상태를 저장하는 데 필요한 메모리가 줄어들기 때문이다.

![그릴 5.16 파라미터 재구성을 통해 더 적은 파라미터 학습](./picture/photo_2024-09-11_23-02-46.jpg)  

![ 그림 5.17 전체 미세 조정과 LORA에서 저장하는 데이터 비교](./picture/photo_2024-09-11_23-02-45.jpg)  

## 5.5 효율적인 학습 방법(PEFT): QLoRA
### 5.5.1 4비트 양자화와 2차 양자화 

QLoRA - LoRA에 양자화를 추가해 메모리 효율성을 한 번 더 높인 학습 방법

QLORA는 대형 언어 모델을 최대한 효율적으로 다룰 수 있도록 여러 기법을 적용했기 때문에 메모리 효율화를 이해하기 위한 좋은 참고 자료이기도 하다. 

기존 데이터의 분포를 알고 있다면 많은 연산이나 메모리 사용 없이도 빠르게 데이터의 순위를 정할 수 있다. 

### 5.5.2 페이지 옵티마이저
그레이디언트 체크포인팅 과정에서 발생할 수 있는 OOM에러를 방지하기 위해 페이지 옵티마이저를 활용

**페이지 옵티마이저** - 엔비디아의 통합 메모리를 통해 GPU가 CPU 메모리(RAM)를 공유하는 것

# 06 sLLM 학습하기 
## 6.1 Text2SQL 데이터셋 
### 6.1.1 대표적인 Text2SQL 데이터셋

대표적인 Text2SQL 데이터셋 : WikiSQL, Spider 

SQL을 생성하기 위해서는 크게 두 가지 데이터가 필요:
1) 어떤 데이터가 있는지 알 수 있는 데이터베이스 정보(테이블과 컬럼)
2) 어떤 데이터를 추출하고 싶은지 나타낸 요청사항(request, question)

### 6.1.3 합성 데이터 활용

생성한 데이터셋은 db_ld, context, question, answer 4개의 컬럼으로 구성돼 있다.
- db_id : 테이블이 포함된 데이터베이스 아이디
- context : SQL 생성에 사용할 테이블정보
- question : 데이터 요청사항
- answer : 요청에 대한 SQL 정답 

## 6.2 성능 평가 파이프라인 준비하기 
### 6.2.1 Text2SQL 평가 방식 


일방적으로 사용하는 성능평가 방법:
- EM(Exact Match) 방식 : 생성한 SQL이 문자열 그대로 동일한지 확인하는 방식
- EX(Execution Accuracy) 방식 : 쿼리를 수행할 수 있는 데이터베이스를 만들고 프로그래밍 방식으로 SQL 쿼리를 수행해 정답과 일치하는지 확인하는 방식

최근에는 LLM을 활요해 LLM의 생성 결과를 평가하는 방식이 활발히 연구되고 있다

![그림 6.5 LLM을 활용한 Text2SQL 평가 방식](./picture/photo_2024-09-11_23-02-44.jpg)

### SQL 생성 프롬프트

>SQL 프롬프트 데이터 예시

```
#학습 데이터 예시

***당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question SQL 허리를 생성하세요.

DOL:

CREATE TABLE messages (

);

"message id" SERIAL PRIMARY KEY,

*conversation_id" INT NOT NULL,

"sender_ld" INT NOT NULL,

"content" TEXT,

"timestamp" TIMESTAMP WITH TIME ZONE DEFAULT CURRENT TIMESTAMP, "read" BOOLEAN DEFAULT FALSE,

FOREIGN KEY ("conversation_id") REFERENCES conversations("conversation_id"), FOREIGN KEY ("sender_id") REFERENCES users("user_id")

### Question:

messages 테이블에서 모든 데이터를 조회해 줘

###SQL:

SELECT FROM messages;"*"

#생성 프롬프트 예시

**"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.
### DDL:

CREATE TABLE messages (

"message id" SERIAL PRIMARY KEY, "conversation td" INT NOT NULL,

"sender (d" INT NOT NULL,

"content" TEXT,

"timestamp" TIMESTAMP WITH TIME ZONE DEFAULT CURRENT TIMESTAMP,

"read" BOOLEAN DEFAULT FALSE, FOREIGN KEY ("conversation td") REFERENCES conversations("conversation_td"), FOREIGN KEY ("sender (d") REFERENCES users("user_(d")

###Question:

messages 테이블에서 모든 데이터를 조회해 줘

###SQL:
"""
```
### GPT-4 평가 프롬프트와 코드 준비
GPT-4 API를 통해 평가 데이터셋으로를 수행하면 시간이 오래 걸려 OpenAI의 비동기 요청 코드(api_request_parallel_processor.py)를 활용해 요청 제한을 관리하며 처리 속도를 높일 수 있다. 이 코드는 jsonl 파일을 읽어 순차적으로 요청을 보내고, 에러나 제한에 걸리면 다시 요청을 보내 결과 누락을 방지한다.

> 예제 6.4. 평가를 위한 요청 jsonl 작성 함수


```
import json
import pandas as pd
from pathlib import Path

def make_requests_for_gpt_evaluation(df, filename, dir='requests'):
  if not Path(dir).exists():
      Path(dir).mkdir(parents=True)
  prompts = []
  for idx, row in df.iterrows():
      prompts.append("""Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return "yes" else return "no". Output JSON Format: {"resolve_yn": ""}""" + f"""

DDL: {row['context']}
Question: {row['question']}
gt_sql: {row['answer']}
gen_sql: {row['gen_sql']}"""
)

  jobs = [{"model": "gpt-4-turbo-preview", "response_format" : { "type": "json_object" }, "messages": [{"role": "system", "content": prompt}]} for prompt in prompts]
  with open(Path(dir, filename), "w") as f:
      for job in jobs:
          json_string = json.dumps(job)
          f.write(json_string + "\n")
```


## 6.3 실습: 미세조정 수행하기 
### 6.3.1 기초 모델 평가하기

> 예제 6.7. 기초 모델로 생성하기


```
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

def make_inference_pipeline(model_id):
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
  pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
  return pipe

model_id = 'beomi/Yi-Ko-6B'
hf_pipe = make_inference_pipeline(model_id)

example = """당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.

### DDL:
CREATE TABLE players (
  player_id INT PRIMARY KEY AUTO_INCREMENT,
  username VARCHAR(255) UNIQUE NOT NULL,
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  date_joined DATETIME NOT NULL,
  last_login DATETIME
);

### Question:
사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요.

### SQL:
"""

hf_pipe(example, do_sample=False,
    return_full_text=False, max_length=512, truncation=True)
#  SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';

# ### SQL 봇:
# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';

# ### SQL 봇의 결과:
# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%'; (생략)
```

### 6.3.2 미세 조정 수행
> 예제 6.10. 미세 조정 명령어

```
base_model = 'beomi/Yi-Ko-6B'
finetuned_model = 'yi-ko-6b-text2sql'

!autotrain llm \
--train \
--model {base_model} \
--project-name {finetuned_model} \
--data-path data/ \
--text-column text \
--lr 2e-4 \
--batch-size 8 \
--epochs 1 \
--block-size 1024 \
--warmup-ratio 0.1 \
--lora-r 16 \
--lora-alpha 32 \
--lora-dropout 0.05 \
--weight-decay 0.01 \
--gradient-accumulation 8 \
--mixed-precision fp16 \
--use-peft \
--quantization int4 \
--trainer sft
```
### 6.3.3 학습 데이터 정제와 미세정제

학습을 통해 사용하는 데이터셋이 커지면서 성능이 함께 향상되는 것을 확인할 수 있다. 이 결과를 통해 학습 데이터 정제 과정에서 데이터셋이 줄었음에도 성능이 유지 되는 것이 정제의 긍정적인 효과임을 짐작해 볼 수 있다.

# 모델 가볍게 만들기

## 7.1 언어 모델 추론 이해하기

### 7.1.1 언어 모델이 언어를 생성하는 방법

언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하고 그중에서 가장 확률이 높은 토큰을 입력 텍스트에 추가하면서 한 토큰씩 생성한다. 이 방식을 반복한다.

언어 모델이 텍스트 생성을 마치는 이유로 첫 번쨰, 생성 종료를 의미하는 특수 토큰(예: EOSEnd Of Sentence 토큰)을 생성하는 경우 생성을 종료한다. 두 번쨰로 사용자가 최대 길이로 설정 한 길이에 도달하면 더 이상 생성하지 않고 종료한다. 

토큰은 바로 다음의 토큰만 예측할 수 있다. 이를 자기희기적 특성이라고 부른다. 

추론 과정:
- 사전 계산 단계(prefill phase): 프롬프트를 처리하는 단계
- 디코딩 단계(decoding phase): 이후 한 토큰씩 생성하는 단계

셀프 어텐션처럼 똑같은 입력토큰을 반복해서 수행하지않기 위해 KV캐시에 대해 알아야 한다.

### 7.1.2 중복 연산을 줄이는 KV 캐시

**KV 캐시**는 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법. 키와 값 계산 결과를 저장하기 때문에 'KVKey-Value' 캐시라고 한다.

KV 캐시 없이는 다음 토큰을 예측하는 것 마다 이전에 수행했던 토큰을 키와 값 벡터로 변환하는 동일한 연산을 반복해야 한다.

![그림 7.4 KV 캐시를 사용할 때와 사용하지 않을 때의 차이](photo_2024-09-18_22-21-36.jpg)

KV 캐시 메모리 사용량 계산식

``KV 캐시 메모리 = 2바이트×2(키와 값) × (레이어 수) × (토큰 임베딩 차원)×(최 대 시퀀스 길이)×(배치 크기)

### 7.1.3 GPU 구조와 최적의 배치 크기

서빙이 효율적인지 판단하는 큰 기준:
1) 비용
2) 처리량 (throughput) - 시간당 처리한 요청 수(query/s)
3) 지연 시 간(latency) 하나의 토큰을 생성하는 데 걸리는 시간(token/s)

**적은 비용**으로 **더 많은 요청** 을 처리하면서 생성한 다음 **토큰을 빠르게 전달**할 수 있다면 효율적인 서빙이라고 할 수 있다.

- 스트리밍 멀티프로세서 Streaming Multiprocessors(이하 SM)
- SRAM (Static Random Access Memory) - 각각의 SM에는 연산을 수행하는 부분과 계산할 값을 저장
- 큰 고대역폭 메모리 High Bandwidth Memory - 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어렵기 때문에 여기에 큰 데이터를 저장한다. 

또한 메모리를 이동시키는 것도 시간을 잡아먹는다. 모델 이동 과정과 연산수행 과정은 함께 진행되기 때문에 두 가지 시간이 같을 때가 **최적의 배치 크기**가 된다. 

**메모리 바운드(memory bound)** - 최적의 배치 크기보다 배치 크기가 작으면 모델 파라미터를 이동시키느라 연산이 멈추는 비효율

**연산 바운드(compute bound)** - 크기가 최적 크기보다 더 커지면 연산에 오랜 시간이 걸리기 때문에 지연 시간이 길어 지는 비효율

최적의 배치 크기 계산식:

`` 2×P×배치 크기/하드웨어 연산 속도 = P / 메모리 대역폭``

`` 배치 크기 = 하드웨어 연산 속도/(2×메모리 대역폭) = (312×10¹²) / (2x 1555×10°) = 102.73``

### 7.1.4 KV 캐시 메모리 줄이기

멀티 헤드 어텐션을 할 시 많은 수의 키와 값 벡터를 저장하기 때문에 KV 캐시에 더 많 은 메모리를 사용하고 KV 캐시에서 더 많은 데이터를 불러와 계산하기 때문에 그만큼 속도가 느려진다.

이를 해결하기 위해 멀티 **쿼리 어텐션**(모든 쿼리 벡터가 하나의 키와 값 벡터를 공유)를 사용한다

하지만 이도 멀티 쿼리 어텐션은 키와 값 벡터를 공유하므로 메모리 사용량은 줄어들지만, 멀티 헤드 어텐션에 비해 성능이 떨어질 수 있습니다.

그래서 **그룹 쿼리 어텐션**(grouped-query attention, GQA)을 개발했다.

**그룹 쿼리 어텐션**은 2개의 쿼리 벡터당 1개의 키와 값 벡터를 사용해 결과적으로 4개의 키와 값을 사용한다. 그룹 쿼리 어텐션은 사용하는 키와 값 벡터의 수를 줄임으로써 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 메모리 사용량을 줄일 수 있었다.

![그림 7.11 멀티 헤드 어텐션, 그룹 쿼리 어텐션, 멀티 쿼리 어텐션 비교](photo_2024-09-18_22-21-39.jpg)

## 12 양자화로 모델 용량 줄이기

**양자화**란 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU 를 효율적으로 사용하는 방법

양자화:
- **학습 후 양자화** (Post-Training Quantization, PTQ) 
- **양자화 학습** (Quantization-Aware Training, QAT) 

주로  학습 후 양자화를 활용한다.

학습 후 양자화 방식:

- 비츠앤바이츠 bits-and-bytes

- GPTQ GPT Quantization

- AWQ Activation-aware Weight Quantization

현재 활발히 연구되고 있는 분야이므로 언제든지 새로운 방식이 추가될 수 있다

### 7.2.1 비츠앤바이츠

 **비츠앤바이츠** - 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리다.

1) 8비트 행렬 연산 - 8비트로 연산을 수행하면서도 성능 저하가 거의 없다
2) 4비트 정규 분포 양자화 - 4비트로  정규 분포를 따르는 값들을 효율적으로 압축

![그림 7.15 비츠앤바이츠의 8비트 양자화 방식](photo_2024-09-18_22-21-40.jpg)   

16비트 이상인 큰 값들은 그대로 계산한다. (중요한 정보를 갖고 있다고 예상하기 떄문)그 외에 정상 범위 의 있는 열은 양자화 할 때 벡터 단위로 절대 최대 값을 찾고 그 값을 기준으로 양자화 한다.  

### 7.2.2 GPTQ

**GPTQ**는 양자화 이전의 모델에 입력 X를 넣었을 때와 양자화 이후의 모델에 입력 X를 넣었을 때 오차가 가장 작아지도록 모델의 양자화를 수행한다. 직관적으로 봤을 때 양자화 전과 후의 결과 차이가 작다면 훌륭한 양자화라고 볼 수 있다.

![그림 7.16 GPTQ 양자화 진행 과정](photo_2024-09-18_22-21-41.jpg)

GPTQ의 양자화 과정은 전체 가중치 행렬에서 흰색으로 표시된 열을 양자화하고, 그 결과가 이전 상태와 최대한 가까워지도록 오른쪽 열의 파라미터를 업데이트하는 방식으로 진행된다. 블록은 오른쪽으로 이동하면서 점차 전체 모델의 파라미터를 업데이트한다.

### 7.2.3 AWQ

AWQ - 중요한 파라미터의 정보를 유지하며 수행하는 양자화 방식

중요한 파라미터란
- 모델 파라미터 값이 크다
- 데이터 활성화 값이 큰 채널의 파라미터

상위 1% 파라미터만 FP16으로 유지하고 나머지를 양자화 하였을 떄 성능이 거의 떨어지지 않았다. 그러나 모델 파라미터에 서로 다른 데이터 타입이 섞여 있는 경우 한 번에 일괄적으로 연산하기 어렵기 때문에 연산이 느려지고 하드웨어 효율성이 떨어지는 문제가 발생한다. 

![그림 7.19 양자화 과정에서 중요한 정보의 손실](photo_2024-09-18_22-21-43.jpg)

위 그림처럼 파라미터를 양자화 할 시 이전에는 서로 다른 값이었는데 양자화를 수행하면서 값이 같아지고 중요한 정보가 소실될 수 있다.

그래서 이를 막기 위해 붕요 파라미터만 1보다 큰 값을 곱하는 방식으로 이 문제를 해결했다. 이때 곱해 주는 값을 스케일러scaler라고 부른다.

![그림 7.20 스케일러를 곱했을 때 중요한 정보의 소실을 막음](photo_2024-09-18_22-21-44.jpg)

## 7.3 지식 증류 활용하기

**지식 증류** (knowledge distillation)란 더 크고 성능이 높은 선생 모델(teacher model)의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델(student model)을 만드는 방법

지식 증류는 학생 모델이 선생 모델을 모방하는 것을 넘어서, 새로운 데이터셋 구축이나 개발 속도 향상 등 다양한 방식으로 활용을 한다.

# sLLM 서빙하기

## 8.1 효율적 배치 전략

### 8.1.1 일반 배치(정적 배치)

**일반 배치** - 입력 데이터를 배치 처리할 때, 가장 기본적인 방식은 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식

일반 배치의 문제점:
- 입력 중 먼저 생성이 종류 된 입력은 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 대기하게 된다. GPU를 효율적으로 사용 못한다.
- 처음에는 모든 문장에 대해 수행하게 되지만 문장의 길이 차이로 인해 추론을 끝낸 문장이 생기고 추론하는 문장이 적어져 배치 크기가 작아지고 GPU를 비효율적으로 사용하게 된다.   

### 8.1.2 동적 배치

**동적 배치**dynamic batching는 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략이다.

![그림 8.2 일정한 시간 동안의 요청을 묶어서 처리하는 동적 배치 전략](photo_2024-09-18_22-21-45.jpg)

### 8.1.3 연속 배치

**연속 배치**continuous batching는 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가한다. 

![그림 8.3 생성이 끝나면 새로운 문장을 추가하는 연속 배치 전략](photo_2024-09-18_22-21-46.jpg)

생성이 끝나고도 다른 문장의 생성이 끝나길 기다리면서 대기 시간이 길어지는 문제를 줄이고 배치 크기가 줄면서 GPU를 비효율적으로 사용하는 문제도 해결할 수 있다.

## 8.2 효율적인 트랜스포머 연산

### 8.2.1 플래시어텐션

**플래시어텐션** FlashAttention - 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발 됐다. 플래시어텐션은 어텐션 연산 과정을 변경해 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하도록 개선했다. 

> 셀프 어텐션 연산에 드는 시간

![그림 8.5 셀프 어텐션의 각 연산에 걸리는 시간](photo_2024-09-18_22-21-48.jpg)

마스크, 소프트맥스, 드롭아웃 처리에 드는 시간이 행렬 곱셈에 드는 시간보다 더 길다. 하지만 연산량 자체는 행렬 곱셈이 다른 연산에 비해 훨씬 크다.

플래시어텐션에서는 데이터 이동 속도가 느린 고대역포 메모리에 큰 어텐션 행렬을 쓰고 읽으면서 걸리는 시간을 줄이기 위해 블록 단위로 어텐션 연산을 수행하고 전체 어텐션 행렬을 쓰거나 읽지 않는 방식으로 속도를 높였다. 또 작은 블록 단위로 연산을 수행하기 때문에 SRAM에 데이터를 읽고 쓰면서 더 빠르게 연산을 수행한다.

![그림 8.7 플래시어텐션에서 어텐션 연산 과정을 개선한 방식](photo_2024-09-18_22-33-14.jpg)

### 8.2.2 플래시어텐션 2

**플래시어텐션 2**는 플래시어텐션을 개선해 2배 정도 속도를 향상했다.

개선한 부분:
-행렬 곱셈이 아닌 연산 줄이기
- 시퀀스 길이 방향의 병렬화 추가

### 8.2.3 상대적 위치 인코딩

사인파 위치 인코딩 - 최초의 트랜스포머 아키텍처에 쓰인 위치 인코딩. 수식에 따라 정해지는 값을 위치 임베딩 값으로 더해준다.

절대적 위치 인코딩의 한계점인 학습 데이터보다 더 긴 입력이 들어오면 언어 모델의 생성 품질이 빠르게 떨어지는 것을 상대적 위치 인코딩이 대신해준다.

상대적 위치 인코딩 - 절대적인 위치에 따라 임베딩을 더하는 것이 아니라 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 인코딩.

대표적인 상대적 위치 인코딩 방식:
- RoPE: 각각의 토큰 임베딩을 토큰 위치에 회전시킨다. 토큰 사이의 위치 정보가 두 임베딩 사이의 각도를 통해 모델에 반영된다.

![그림 8.18 RoPE의 상대적 위치 인코딩 방식](photo_2024-09-18_22-33-15.jpg)

- ALiBI - 쿼리, 키, 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식

![그림 8.19 ALiBi의 상대적 위치 인코딩 방식](photo_2024-09-18_22-33-17.jpg)

>오른쪽의 행렬은 현재 쿼리의 위치 를 의미하는 0을 기준으로 앞에 있을수록 더 작은 값을 더해 상대적인 위치를 나타낸다고 볼 수 있다. 

## 8.3 효율적인 추론 전략

### 8.3.1 커널 퓨전

GPU 연산은 커널 단위로 수행되며, 연산을 수행하기 위해서 전후에 추가적인 작업을 위한 오버헤드가(ex: 고대역폭 메모리에서 데이터를 읽어오고 연산 결과를 쓰는 작업) 발생한다. 여러 커널을 사용하는 경우 오버헤드가 증가하여 연산 시간이 길어질 수 있으므로, **커널 퓨전**(kernel fusion)을 통해 연산을 하나로 묶어 오버헤드를 줄이고 효율성을 높일 수 있다.

### 8.3.2 페이지어텐션

KV의 단점인 GPU의 메모리를 많이 차지하는 현상을 **페이지 어텐션**과 함께 사용하면 보완 할 수 있다.
페이지어텐션 - 운영체제의 가상 메모리 개념을 빌려와 중간에서 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 관리해서 실제로는 물리적으로 연속된 메모리를 사용하지 않으면서도 논리적 메모리에서는 서로 연속적이도록 만들었다.

또한 다양한 디코딩 방식에서 메모리를 절약할 수 있는데 동일한 입력 프롬프트에서 여러 개의 출력을 생성하는 병렬 샘플링에서 입력 프롬프트에 대한 메모리를 공유함으로써 메모리를 절약한다.

참조 카운트 - 물리적 블록을 공유하고 있는 논리적 블록 수를 의미

### 8.3.3 추측 디코딩

**추측 디코딩** speculative decoding - 쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식


디코딩은 작은 **드래프트 모델**draft model과 큰 **타깃 모델**target model 이라는 2개의 모델을 사용해 추론을 수행한다. 


![그림 8.28 추측 디코딩 방식의 추론 과정](photo_2024-09-18_22-33-19.jpg)

먼저 드래프트 모델이 토큰을 생산 한다. 그 후 타깃 모델과 비교하여 동일하다면 승인하고 동일하지 않다면 거절한다.

## 8.4 실습: LLM 서빙 프레임워크

### 8.4.1 오프라인 서빙

**오프라인 서빙**이란 정해진 입력 데이터에 대해 배치 추론을 수행한다.

### 8.4.2 온라인 서빙

**온라인 추론**이란 사용자의 요청에 따라 모델 추론을 수행한다.

대표적인 LLM 추론 프레임워크인 vLLM은 허깅페이스 라이브러리를 활용한 추론 대비 더 빠른 오프라인 추론이 가능하고 온라인 서빙에 사용할 수 있는 편리한 API 서버를 제공한다.

# LLM 애플리케이션 개발하기

## 9.1 검색 증강 생성(RAG)

LLM의 답변의 근거나 출처가 불명확하고 부정확한 정보를 지어내는 환각 현상을 해결 하기 위해 **RAG**라는 기법이 활용된다. **검색 증강 생성**이란, LLM에게 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법을 말한다.

이때 답변에 필요한 정보를 '검색retrieval 을 통해 선택하기 때문에 '검색을 통해 보충한 생성'이라는 의미로 붙은 이름이다.

### 9.1.1 데이터 저장

![그림 9.3 데이터 저장 작업 과정](photo_2024-09-18_22-33-20.jpg)

> 데이터 소스의 텍스트를 임베딩 모델을 사용해 임베딩 벡터로 변환한다. 변환한 임베딩 벡터는 벡터 사이의 거리를 기준으로 검색하는 특수한 데이터베이스인 벡터 데이터베이스에 저장한다.

벡터 데이터베이스는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공한다.

특정 문장(검색 쿼리)으로 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리도 벡터로 변환해 벡터 데이터베이스에서 위치를 찾고 쿼리 임베 딩과 가장 가까운 벡터를 찾는다. 

### 9.1.2 프롬프트에 검색 결과 통합
LLM은 결과를 생성할 때 프롬프트만 입력으로 받기 때문에 사용자가 앞서 저장한 켁스트를 LLM에게 전달하기 위해서 요청과 관련이 큰 문서를 벡터 데이터베이스에서 찾고 검색 결과를 프롬프트에 통합해야 한다.

![그림 9.6 검색 결과를 프롬프트에 통합](photo_2024-09-18_22-33-21.jpg)

## 9.2 LLM 캐시
### 9.2.1 LLM 캐시 작동 원리

**LLM 캐시**는 추론을 수행할 때 사용자의 요청과 생성 결과를 기록하고 이후에 동일하거나 비슷한 요청이 들어오면 새롭게 텍스트를 생성하지 않고 이전의 생성 결과를 가져와 바로 응답한다

LLM 캐시를 사용하지 않는다면 통합한 프롬프트를 바로 LLM에 전달하고 결과를 생성한다. 하지만 LLM 캐시를 사용한다면 캐시 요청을 통해 이전에 동일하거나 유사한 요청이 있었는지 확인하고 만약 있었다면 LLM 캐시에 저장된 답변을 전달하고 없었다면 LLM에 프롬프트를 전달해 새롭게 텍스트를 생성해서 LLM 오케스트레이션 도구로 전달한다.

![그림 9.12 프롬프트 통합과 LLM 사이에 LLM 캐시 확인 과정 추가](photo_2024-09-18_22-33-22.jpg)

LLM 캐시:
_ 일치 캐시: 요청이 완전히 일치 할 경우 저장된 응답을 발휘한다.
- 유사 검색 캐시: 유사한 요청이 있는지 확인하고 만약 있다면 저장된 텍스츠를 반환한다. 만약 없다면 LLM으로 새롭게 텍스트를 생성해 응답하면서 벡터 데이터베이스에 요청의 임베딩 벡터와 생성 결과를 저장한다.

### 9.2.2 실습: OpenAI API 캐시 구현

## 9.3 데이터 검증

### 9.3.1 데이터 검증 방식

**데이터 검증**이란, 벡터 검색 결과나 LLM 생성 결과에 포함되지 않아야 하는 데이터를 필터링하고 답변을 피해야 하는 요청을 선별함으로써 LLM 애플리케이션이 생성한 텍스트로 인해 생길 수 있는 문제를 줄이는 방법

- 규칙 기반: 문자열 매칭이나 정규 표현식을 활용해 데이터를 확인
- 분류 또는 회귀 모델: 명확한 문자열 패턴이 없는 경우 별도의 분류 또는 희귀 모델을 만들어 활용
- 임베딩 유사도 기반: 민담한 의견을 물었을 떄 임베딩과 유사할 떄 답변을 피할 수 있다.
- LLM 활용: LLM을 활용해 텍스트 내의 부적절한 내용이 섞여 있어 적절하지 않은 경우 이를 다시 생성하거나 삭제

### 9.3.2 데이터 검증 실습

## 9.4 데이터 로깅
**데이터 로깅** - 사용자의 입력과 LLM이 생성한 출력을 기록한다
LLM의 경우 입력이 동일해도 출력이 달라질 수 있기 때문에 어떤 입력에서 어떤 출력을 반환했는지 반드시 기록해야 하기 때문에 데이터 로깅이 필요하다.

대표적인 로깅 도구 중 하나인 W&B(Weight adn Bias)에서 제공하는 Trace 기능을 활용하면 요청과 응답을 기록하고 확인 할 수 있다.

### 9.4.1 OpenAl API 로깅

대표적인 상업용 LLM API인 OpenAI의 API를 로깅하기 위해서는 예제 9.16과 같은 코 드를 사용하면 된다.

> 예제 9.16 OpenAI API 로깅하기
```
import datetime
from openai import OpenAI
from wandb.sdk.data_types.trace_tree import Trace

client = OpenAI()
system_message = "You are a helpful assistant."
query = "대한민국의 수도는 어디야?"
temperature = 0.2
model_name = "gpt-3.5-turbo"

response = client.chat.completions.create(model=model_name,
                                        messages=[{"role": "system", "content": system_message},{"role": "user", "content": query}],
                                        temperature=temperature
                                        )

root_span = Trace(
      name="root_span",
      kind="llm",
      status_code="success",
      status_message=None,
      metadata={"temperature": temperature,
                "token_usage": dict(response.usage),
                "model_name": model_name},
      inputs={"system_prompt": system_message, "query": query},
      outputs={"response": response.choices[0].message.content},
      )

root_span.log(name="openai_trace")
```

### 9.4.2 라마인덱스 로깅


>예제 9.17 라마인덱스 W&B 로깅


```
from datasets import load_dataset
import llama_index
from llama_index.core import Document, VectorStoreIndex, ServiceContext
from llama_index.llms.openai import OpenAI
from llama_index.core import set_global_handler
# 로깅을 위한 설정 추가
llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
set_global_handler("wandb", run_args={"project": "llamaindex"})
wandb_callback = llama_index.core.global_handler
service_context = ServiceContext.from_defaults(llm=llm)

dataset = load_dataset('klue', 'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

print(dataset[0]['question']) # 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?

query_engine = index.as_query_engine(similarity_top_k=1, verbose=True)
response = query_engine.query(
    dataset[0]['question']
)
```

# 10 임베딩 모델로 데이터 의미 함축하기

데이터의 의미를 함축 해 숫자로 표현하는 것은 오랜 기간 데이터 분야에 과제였다. 이를 해결하기 위해 어떤 다양한 시도를 했었는지 알아본다.

## 10.1 텍스트 임베딩 이해하기

**임베딩** (embedding)이란 '데이터의 의미를 압축한 숫자 배열(벡터)'을 말한다.


**텍스트 임베딩** 또는 **문장 임베딩** - 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식. 

### 10.1.1 문장 임베딩 방식의 장점

문장 임베딩 방식을 사용하면 단어나 문장 사이에 관계를 계산 할 수 있다.

데이터를 숫자로 변환하기 위해 원핫 인코딩을 활발히 사용한다.

### 10.1.2 원핫 인코딩

원핫 인코딩은 다음 예시와 같이 표현한다.
>학교 - 1, 공부 - 2, 운동 - 3

↓

> 학교 - [1,0,0], 공부 - [0,1,0], 운동 - [0,0,1]

원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만, 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 단점이 있다. 

### 10.1.3 백오브워즈
**백오브워즈** 는 '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환한다. 백오브워즈는 단어의 순서에 관계없이 해당 문서에 등장한 단어와 그 등장 횟수를 집계한다.

백오브워즈의 단점: 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는 데 크게 도움이 되지 않는 경우가 있다는 점이다. (ex: 한국어 은/는, 이가; AI)

### 10.1.4 TF-IDF
앞서 단점을 보완 하기 위해 TF-IDF는 다음 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만든다.

> TF-IDF(w) = TF(w) x log(N/DF(w))

- TF(w): 특정 문서에서 특정 단어 w가 등장한 횟수
- DF(w): 특정 단어 w가 등장한 문서의 수

그러나 만약 100000 차원에 벡터를 사용하게 되면 이러한 경우 대부분이 0인 벡터가 되어 이를 '희소(sparse)'하다고 한다. **희소한 벡터**는 의미를 '압축'해서 담고 있지 못하기 때문에 벡터 사이의 관계를 활용하기 어렵다. 이를 대비하기 위해 **밀집 임베딩**(dense embedding)을 사용한다.

### 10.1.5 워드투벡

**워드투벡**word2vec은 단어가 '함께 등장하는 빈도' 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법. 특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델이 생성할 수 있다

![그림 10.1 워드투벡의 두 가지 학습 방식](photo_2024-09-18_22-33-23.jpg)

- CBOW: 주변 단어로 가운데 단어를 예측하는 방식
- 스킵그램: 중간 단어로 주변 단어를 예측하는 방식

## 10.2 문장 임베딩 방식

### 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법

**문장 임베딩**: 텍스트를 밀집 벡터로 표현하여 문장 간 유사도나 관련성을 쉽게 계산할 수 있게 해준다. BERT 모델은 이러한 문장 임베딩 변환에서 뛰어난 성능을 보인다.

BERT 모델을 사용해 문장 관계 계산 방법:
- **바이 인코더** (bi-encoder): 두 문장을 독립적으로 BERT 모델에 입력하고, 출력된 문장 임베딩 벡터 간의 유사도를 별도로 계산한다.

- **교차 인코더** (cross-encoder): 두 문장을(쿼리 문장, 검색 대상 문장) 함께 BERT 모델에 입력하여 모델이 직접 문장 간의 유사도를 계산한다. 0-1사이 값으로 출력한다. 이 방법은 계산량이 많지만 더 정확한 관계 예측이 가능하다. 하지만 입력으로 넣은 두 문장의 유사도만 계산하기 때문에 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 연산을 반복해야 한다는 단점이 있다.

![그림 10.3 문장 사이 관계를 계산하는 두 가지 방법](photo_2024-09-18_22-33-24.jpg)

바이 인코더가 교차 인코더를 해결하는 방법:
1) 검색 쿼리 문장과 검색 대상 문장을 독립적으로 모델에 입력하여 각각의 문장 임베딩을 생성한다.
2) 생성된 문장 임베딩을 사용해 유사도를 계산하며, 이렇게 하면 교차 인코더와 달리 바이 인코더는 각 문장의 독립적인 임베딩을 결과로 반환 하기 때문에 유사도를 계산하고 싶은 문장이 바뀌더라도 추가적인 BERT 연산 없이 문장 유사도 계산이 가능하다.

### 10.2.2 바이 인코더 모델 구조

바이 인코더에서는 BERT 모델은 입력 문장의 각 토큰에 대해 출력 임베딩을 생성한다. 문장 길이에 따라 임베딩 수가 달라질 수 있다. 문장의 길이가 다를 때 서로 다른 개수의 임베딩이 반환된다면, 문장과 문장 사이의 유사도를 쉽게 계산하기 어렵다.

풀링 층의 역할: 문장 길이가 달라도 동일한 크기의 고정된 임베딩으로 통합해준다. 이는 문장을 대표하는 1개의 임베딩을 통합되며, 문장 간 유사도를 쉽게 계산할 수 있게 한다.

> 예제 10.3 Sentence-Transformers 라이브러리로 바이 인코더 생성하기


```
from sentence_transformers import SentenceTransformer, models
# 사용할 BERT 모델
word_embedding_model = models.Transformer('klue/roberta-base')
# 풀링 층 차원 입력하기
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
# 두 모듈 결합하기
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
```

**폴링 모드**란, 언어 모델이 출력한 결과 임베딩을 고정된 크기의 문장 임베딩으로 통합 할 때 통합하는 방식

세가지 방식이있다:
- **클래스 모드**(pooling_mode_cls_tokens): BERT 모델의 첫 번째 토큰인 [CLS] 토큰의 출력 임베딩을 문장 임베딩으로 사용한다.
- **평균 모드**(pooling_mode_mean_tokens): BERT 모델에서 모든 입력 토큰의 출력 임베딩을 평균한 값을 문장 임베딩으로 사용한다
- **최대 모드**(pooling_mode_max_tokens): BERT 모델의 모든 입력 토큰의 출력 임베딩에서 문장 길이sequence 방향에서 최댓값을 찾아 문장 임베딩으로 사용한다.

세 가지 풀링 모드 중에서는 평균 모드를 일반적으로 많이 활용한다.

### 10.2.3 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해 보기

**Sentence-Transformers 라이브러리**: 허깅페이스 모델을 불러와 텍스트와 이미지 모델을 쉽게 사용할 수 있다.

문장 임베딩 예시:
1) snunlp/KR-SBERT-V40K-klueNLI-augSTS 모델을 사용하여 한국어 문장을 임베딩으로 변환한다.
2) 예시에서는 "잠이 안 옵니다", "졸음이 옵니다", "기차가 옵니다"라는 세 문장을 입력한다.
3) util.cos_sim 함수를 통해 문장 임베딩 사이의 코사인 유사도를 계산한다.
4) 유사도 결과: "잠"과 "졸음" 사이의 유사도는 0.641로 높고, "잠"과 "기차"는 0.1887로 낮으며, "졸음"과 "기차"는 0.273으로 상대적으로 낮다.

> 예제 10.6 한국어 문장 임베딩 모델로 입력 문장 사이의 유사도 계산


```
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

embs = model.encode(['잠이 안 옵니다',
                     '졸음이 옵니다',
                     '기차가 옵니다'])

cos_scores = util.cos_sim(embs, embs)
print(cos_scores)
# tensor([[1.0000, 0.6410, 0.1887],
#         [0.6410, 1.0000, 0.2730],
#         [0.1887, 0.2730, 1.0000]])
```

이 방법을 이미지에서도 이미지 임베딩으로 쉡게 변환 가능하다. 

> 예제 10.7 CLIP 모델을 활용한 이미지와 텍스트 임베딩 유사도 계산


```
from PIL import Image
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('clip-ViT-B-32')

img_embs = model.encode([Image.open('dog.jpg'), Image.open('cat.jpg')])
text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])

cos_scores = util.cos_sim(img_embs, text_embs)
print(cos_scores)
# tensor([[0.2771, 0.1509],
#         [0.2071, 0.3180]])
```

### 10.2.4 오픈소스와 상업용 임베딩 모델 비교하기

임베딩 모델을 활용하는 방법:
- **상업용 모델**: 대량의 데이터로 학습된 만큼 성능이 뛰어나고 LLM 텍스트 생성에 비해 훨씬 낮은 비용으로 사용이 가능하다. 그러나 일부 생성 모델에는 임베딩 모델에 대한 미세 조정 기능이 없어 사용자가 자신의 데이터에 특화된 임베딩 모델을 만들 수 없다.
- **오픈소스 모델**: 자신의 데이터에 맞춰 미세 조정을 수행할 수 있다

## 10.3 실습: 의미 검색 구현하기

**의미 검색**이란, 단순히 키워드 매칭을 통한 검색이 아니라 밀집 임베딩을 이용해 문장이나 문서의 의미를 고려한 검색을 수행하는 것을 말한다. 

실습에 사용할 faiss 라이브러리는 메타(구 페이스북)에서 개발한 벡터 연산 라이브러리다.

### 10.3.1 의미 검색 구현하기
### 10.3.2 라마인덱스에서 Sentence-Transformers 모델 사용하기

## 10.4 검색 방식을 조합해 성능 높이기

키워드 검색 - 동일한 키워드가 많이 포함 될수록 유사도를 높게 평가하는 검색 방식

장점: 키워드 검색은 동일한 키워드가 등장한 문서를 상위 검색 결과로 반환하기 때문에 관련성이 떨어지는 검색 결과가 나타날 가능성이 낮다

단점: 동일한 키워드를 사용하지 않으면 의미가 유사하더라도 검색하지 못한다


### 10.4.1 키워드 검색 방식: BM25

**BM25**는 TF-IDF와 유사한 통계 기반 스코어링 방법으로, TF-IDF에 문서의 길이에 대한 가중치를 추가한 알고리즘이다.

![그림 10.13 BM25 수식](photo_2024-09-18_22-33-26.jpg)

IDF(qi) - 문서 빈도 부분, 특정 단어가 전제 문서에 얼마나 등장 했는 지 알 수 있다.
TF - 단어 빈도,특정 문서(D) 내의서 단어가 얼마나 등장했는지 나타낸다.

![그림 10.14 BM25의 IDF 계산식](photo_2024-09-18_22-33-27.jpg)

n(qi) -  쿼리 단어의 토큰이 등장하는 문서의 수

N - 전체 문서의 수

만약 모든 문서에 등장하는 토큰이라면 IDF 부분은 최솟값이 되고 적게 등장해 n(qi) 가 작아지면 IDF 부분의 값은 커진다. 직관적으로 설명하면, 여러 문서에 등장하는 단어라면 덜 중요한 단어이기 때문에 값이 작아지도록 한 것이다.

### 10.4.2 상호 순위 조합 이해하기

**하이브리드 검색**을 위해서는 통계 기반 점수와 임베딩 유사도 점수를 결합할 때, 각 점수의 분포 차이로 인해 직접 합치면 한 점수의 영향이 더 클 수 있다.

문제 해결 방법: **상호 순위 조합 (Reciprocal Rank Fusion, RRF)** 을 사용하여 문제를 해결한다.

각 점수의 순위를 기반으로 점수를 산출한다. 예를 들어, BM25와 임베딩 점수를 순위로 변환한다.
- 순위에 따라 점수를 계산하고, 만약 한쪽 점수에서 순위에 들지 않았으면 0점으로 처리한다.
- 두 점수를 합쳐 최종 순위를 결정하며, 높은 순위를 받은 문서가 최종적으로 더 높은 순위를 차지한다.

## 10.5 실습: 하이브리드 검색 구현하기

이번 실습에서는 앞서 구현한 밀집 임베딩 기반의 의미 검색에 BM25 검색을 더해 하이 브리드 검색을 구현한다

# 11 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기

## 11.1 검색 성능을 높이기 위한 두 가지 방법

바이 인코더와 교차 인코더를 결합하여 사용할 수 있다.

1) 바이 인코더를 사용해 대규모의 문서에서 검색 쿼리와 유사한 소수의 문서(예: 상위 100개)를 선별한다. 
2) 의미 검색을 통해 선별한 소수의 문서는 유사도를 더 정확히 계산할 수 있는 교차 인코더를 사용해 유사한 순서대로 재정렬한다. 

![그림 11.1 바이 인코더와 교차 인코더를 결합해 검색 성능 높이기](photo_2024-09-18_22-33-29.jpg)

검색 성능을 높이기 위한 방법:
- 바이 인코더를 추가 학습해 검색 성능을 높인다. (문장 임베딩 모델을 사용하려는 데이터셋으로 추가 학습한다.)
- 교차 인코더를 추가해 검색 성능을 높인다. (검색 쿼리와 관련이 높은 문서가 상위 검색 결과에 포함되어야 검색 증강 생성이 효과적)

## 11.2 언어 모델을 임베딩 모델로 만들기

![그림 11.2 문장 임베딩 모델의 구조](photo_2024-09-18_22-33-30.jpg)

사전 학습된 언어 모델을 불러 오고 그 위에 풀링 층을 추가하고 문장의 의미를 잘 담을 수 있도록 학습해야 한다.


### 11.2.1 대조 학습

문장 임베딩 모델을 학습시킬 때는 대조 학습contrastive learning을 사용한다. 

**대조 학습**이란, 관련이 있거나 유사한 데이터는 더 가까워지도록 만들고 관련이 없거나 유사하지 않은 데이터는 더 멀어지도록 하는 학습 방식

일반적으로는 서로 유사한 데이터를 수집한 데이터셋이나 서로 이어지는 문장을 수집 해 임베딩 모델의 학습에 사용한다.

### 11.2.2 실습: 학습 준비하기

이번 실습에서는 다음 세 가지 데이터 전처리를 수행한다.
- 학습 데이터의 일부를 검증을 위한 데이터셋으로 분리한다.
- 유사도 점수를 0~1 사이로 정규화한다
- torch.utils.data. DataLoader를 사용해 배치 데이터로 만든다.

> 예제 11.3 학습 데이터에서 검증 데이터셋 분리하기
```
# 학습 데이터셋의 10%를 검증 데이터셋으로 구성한다.
klue_sts_train = klue_sts_train.train_test_split(test_size=0.1, seed=42)
klue_sts_train, klue_sts_eval = klue_sts_train['train'], klue_sts_train['test']
```

> 예제 11.4 label 정규화하기

```
from sentence_transformers import InputExample
# 유사도 점수를 0~1 사이로 정규화 하고 InputExample 객체에 담는다.
def prepare_sts_examples(dataset):
    examples = []
    for data in dataset:
        examples.append(
            InputExample(
                texts=[data['sentence1'], data['sentence2']],
                #원본 데이터셋에는 0~5 점 척도로 되어 있는 label 점수를 5로 나눠 0~1 범위로 정규화 
                label=data['labels']['label'] / 5.0)
            )
    return examples
#앞서 준비한 3개 의 데이터셋(학습, 검증, 평가)을 prepare_sts_examples 함수를 사용해 전처리한다
train_examples = prepare_sts_examples(klue_sts_train)
eval_examples = prepare_sts_examples(klue_sts_eval)
test_examples = prepare_sts_examples(klue_sts_test)
```

> 예제 11.5 학습에 사용할 배치 데이터셋 만들기

```
from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
```

### 11.2.3 실습: 유사한 문장 데이터로 임베딩 모델 학습하기

> 예제 11.8 임베딩 모델 학습

```
from sentence_transformers import losses

num_epochs = 4
model_name = 'klue/roberta-base'
model_save_path = 'output/training_sts_' + model_name.replace("/", "-")
train_loss = losses.CosineSimilarityLoss(model=embedding_model)

# 임베딩 모델 학습
embedding_model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    evaluator=eval_evaluator,
    epochs=num_epochs,
    evaluation_steps=1000,
    warmup_steps=100,
    output_path=model_save_path
)
```

## 11.3 임베딩 모델 미세 조정하기

임베딩 모델 미세 조정: 검색 쿼리와 관련된 문서를 찾기 위해 임베딩 모델을 활용한다.

좋은 임베딩 모델은 관련 문서와의 유사도를 높게 (1에 가까운 값으로) 계산하고, 관련이 없는 문서와의 유사도는 낮게 (0에 가까운 값으로) 계산해야 한다.

### 11.3.1 실습: 학습 준비

KLUE의 MRC 데이터셋 - 기사 본문 및 해당 기사와 관 련된 질문을 수집한 데이터셋

KLUE의 MRC 데이터셋을 사용하여 사용자의 질문에 적절히 응답하는 챗봇을 만들어보자. 검색 증강 생성을 통해 관련 기사 본문을 찾고 이를 LUM의 프롬프트에 추가한다.
검색 증강 생성이 잘 작동하려면 관련 있는 질문과 기사 쌍에 높은 유사도 점수를, 관련 없는 쌍에는 낮은 유사도 점수를 부여해야 한다.

문장 임베딩 모델은 학습 데이터와 유사한 데이터에서 가장 잘 작동한다. 따라서, 사전 학습된 임베딩 모델이 MRC 데이터셋에 맞지 않을 경우 성능이 낮아질 수 있다. 이 문제를 해결하기 위해 MRC 데이터셋에 임베딩 모델을 활용하려는 경우 그 목적 에 맞게 MRC 데이터셋으로 미세 조정해야 한다.

> 예제 11.13 데이터 전처리

```
from datasets import load_dataset
klue_mrc_train = load_dataset('klue', 'mrc', split='train')
klue_mrc_test = load_dataset('klue', 'mrc', split='validation')

df_train = klue_mrc_train.to_pandas()
df_test = klue_mrc_test.to_pandas()

df_train = df_train[['title', 'question', 'context']]
df_test = df_test[['title', 'question', 'context']]
```

> 예제 11.14 질문과 관련이 없는 기사를 irrelevant_context 컬럼에 추가


```
def add_ir_context(df):
  irrelevant_contexts = []
  for idx, row in df.iterrows():
    title = row['title']
    irrelevant_contexts.append(df.query(f"title != '{title}'").sample(n=1)['context'].values[0])
  df['irrelevant_context'] = irrelevant_contexts
  return df

df_train_ir = add_ir_context(df_train)
df_test_ir = add_ir_context(df_test)
```

irrelevant_context 컬럼에 질문과 관련 없는 기사 본문을 추가한 데이터셋을 활용해 임베딩 모델의 성능 평가에 사용할 데이터를 만든다.

> 예제 11.15 성능 평가에 사용할 데이터 생성


```
from sentence_transformers import InputExample

examples = []
for idx, row in df_test_ir[:100].iterrows():
  examples.append(
      InputExample(texts=[row['question'], row['context']], label=1)
  )
  examples.append(
      InputExample(texts=[row['question'], row['irrelevant_context']], label=0)
  )
```

### 11.3.2 MNR 손실을 활용해 미세 조정하기

MNR 학습 - MRC 데이터셋과 같이 데이터셋에 서로 관련이 있는 문장만 있는 경우 사용하기 좋은 손실 함수

이전 기사 예제와 달리 MNR 손실을 사용하면 하나의 배치 데이터 안에서 다른 데이터의 기사 본문을 관련이 없는negative 데이터로 사용해 모델을 학습시킬 수 있다.

> 예제 11.19 MNR 손실 함수 불러오기

```
from sentence_transformers import losses

loss = losses.MultipleNegativesRankingLoss(sentence_model)
```

> 예제 11.20 MRC 데이터셋으로 미세 조정


```
epochs = 1
save_path = './klue_mrc_mnr'

sentence_model.fit(
    train_objectives=[(loader, loss)],
    epochs=epochs,
    warmup_steps=100,
    output_path=save_path,
    show_progress_bar=True
)
```


## 11.4 검색 품질을 높이는 순위 재정렬

효율적으로 바이 인코더로 후보군을 압축하고 후보군 내에서 교차 인코더로 순위를 재정렬하는 방법을 사용한다. 실습을 위해 sentence- transformers에서 제공하는 CrossEncoder와 미세 조정 메서드를 사용한다.

교차 인코더가 문장의 관련성을 자 계산 하기 위해 미세 조정이 필요하다. 수행 하기 전에 교차 인코더 학습 데이터셋을 생성해야 한다.

> 예제 11.25 교차 인코더 학습 데이터셋 준비


```
train_samples = []
for idx, row in df_train_ir.iterrows():
    train_samples.append(InputExample(
        texts=[row['question'], row['context']], label=1
    ))
    train_samples.append(InputExample(
        texts=[row['question'], row['irrelevant_context']], label=0
    ))
```

> 예제 11.26 교차 인코더 학습 수행


```
train_batch_size = 16
num_epochs = 1
model_save_path = 'output/training_mrc'

train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)

cross_model.fit(
    train_dataloader=train_dataloader,
    epochs=num_epochs,
    warmup_steps=100,
    output_path=model_save_path
)
```

## 11.5 바이 인코더와 교차 인코더로 개선된 RAG 구현하기

이번 장에는 검색 증강 생성을 다음과 같은 세 가지 케이스로 나눠 비교한다.

• 기본 임베딩 모델로 검색하기

• 미세 조정한 임베딩 모델로 검색하기

• 미세 조정한 모델과 교차 인코더를 결합해 검색하기

>예제 11.33 기본 임베딩 모델 평가

```
from sentence_transformers import SentenceTransformer
base_embedding_model = SentenceTransformer('shangrilar/klue-roberta-base-klue-sts')
base_index = make_embedding_index(base_embedding_model, klue_mrc_test['context'])
evaluate_hit_rate(klue_mrc_test, base_embedding_model, base_index, 10)
# (0.88, 13.216430425643921)
```

>예제 11.34 미세 조정한 임베딩 모델 평가


```
finetuned_embedding_model = SentenceTransformer('shangrilar/klue-roberta-base-klue-sts-mrc')
finetuned_index = make_embedding_index(finetuned_embedding_model, klue_mrc_test['context'])
evaluate_hit_rate(klue_mrc_test, finetuned_embedding_model, finetuned_index, 10)
# (0.946, 14.309881687164307)
```

> 예제 11.36 임베딩 모델과 교차 인코드를 조합해 성능 평가


```
hit_rate, cosumed_time, predictions = evaluate_hit_rate_with_rerank(klue_mrc_test, finetuned_embedding_model, cross_model, finetuned_index, bi_k=30, cross_k=10)
hit_rate, cosumed_time
# (0.973, 1103.055629491806)
```

결과:
![표 11.1 세 가지 검색 방식의 성능 비교](photo_2024-09-18_22-33-32.jpg)

성능 향상 분석:
- 기본 임베딩 모델에 비해 미세 조정된 임베딩 모델을 사용했을 때, 거의 동일한 시간에 6.6% 성능 향상이 있었다.
- 임베딩 모델과 교차 인코더를 모두 사용해 순위 재정렬을 했을 때, 추가로 **2.7%**의 성능 향상이 있었다.

미세 조정된 임베딩 모델이 실패한 5.4%의 데이터 중 절반인 2.7%에서 추가로 정답을 포함시켰다.
즉, 틀린 문제를 기준으로 보면, **오답률이 절반으로 줄어들었다.**
