# 07. 모델 가볍게 만들기

## 7.1 언어 모델 추론하기

### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델이 텍스트 생성을 마치는 이유
- 생성 종료를 의미하는 특수 토큰 등장
- 사용자가 최대 길이로 설정한 길이에 도달
> Auto-regressive: 한 번에 한 토큰만 생성 가능

> 추론 과정:
- 사전 계산 단계: 프롬프트를 동시에 병렬적으로 처리
- 디코딩 단계: 이후 한 토큰씩 생성
> 추론 과정에서 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산 반복하므로 비효율적임.


### 7.1.2 중복 연산을 줄이는 KV 캐시
> KV(Key-Value) 캐시: 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법
- 반복 부분은 KV 캐시에 저장해두고, 새로운 부분만 계산
- (KV 캐시 메모리) = (모델 단위 비트) * 2(key, value) * (레이어 수) * (토큰 임베딩 차원) * (최대 시퀀스 길이) * (배치 크기)
- 메모리 많이 차지함

### 7.1.3 GPU 구조와 최적의 배치 크기
> 서빙의 효율성 판단하는 기준: 같은 GPU로 처리량 높이고 지연 시간 낮춰야 함
- 비용
- 처리량: 시간당 처리한 요청 수(query/s)
- 지연 시간: 하나의 토큰 생성하는 데 걸리는 시간
> GPU 구조
- HBM: 큰 데이터 저장하는 고대역폭 메모리
- SM: 스트리밍 멀티프로세서. 하나의 GPU에 여러개 존재
    - 연산부 + SRAM(계산할 값 저장)
    - SRAM을 L1캐시, 공유 메모리로 부르기도 함
> GPU가 추론할 때
- 배치 크기만큼의 토큰 한 번에 생성
- 계산한 결과는 KV 캐시에 저장, 이후 새로운 계산 수행
    - (계산 시간) = (모델 단위 비트) * (모델 파라미터 메모리) * (배치 크기) /  (하드웨어 연산 속도)
- HBM의 모델을 SRAM으로 이동하여 계산
    - (모델 파라미터 이동 시간) = (모델 파라미터 메모리) / (메모리 대역폭)
- 최적점: (계산 시간) = (모델 타라미터 이동 시간)
    - 메모리 바운드: (계산 시간) < (모델 파라미터 이동 시간)
    - 계산 바운드: (계산 시간) > (모델 파라미터 이동 시간)
    - 최적 배치 크기(B*) = (하드웨어 연산 속도) / (모델 단위 비트) * (메모리 대역폭)

### 7.1.4 KV 캐시 메모리 줄이기
> 멀티 헤드 어텐션 방식의 비효율성
- KV 캐시 메모리 증가하므로 계산 속도 느려짐
> 멀티 쿼리 어텐션
- 여러 헤드의 쿼리 벡터가 하나의 키와 값 벡터 사용
- 성능 떨어질 가능성 있음. 이 문제 해결하기 위해 그룹 쿼리 어텐션 방식 사용
    - 2개의 쿼리 벡터당 1개의 키와 값 벡터 사용
- 해당 방법 도입 이점: 추론 속도 향상, KV 캐시 메모리 감소
    - 성능이 떨어지는 멀티 쿼리 어텐션도 추가 학습 수행하면 성능 개선됨.
## 7.2 양자화로 모델 용량 줄이기
> 양자화: 부동소수점 데이터를 정수 형식으로 변환하는 것.
- fp32 -> fp16
- 최근에는 W4A16 방식(4비트로 모델 파라미터 양자화, 계산은 16비트로)
- 수행 시점에 따라 Post-Training Quantization / Quantization-Aware Training(양자화 학습)
    - 주로 PTO방식 사용. 아래 사례는 모두 PTO방식.
### 7.2.1 비츠앤바이츠
> 양자화 라이브러리
- 8비트 행렬 연산
    - 입력 값 중 이상치가 포함된 열은 별도 분리해 16비트 그대로 계산
    - 정상 범위일 때 벡터 단위로 absmax를 찾고, 그 값을 기준으로 양자화 // 이게 뭔 말이지.. 양자화 상수끼리 연산한다는 건가?
- 4비트 정규 분포 양자화(QLoRA 방식)
### 7.2.2 GPTQ
> 양자화를 위한 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하면서 
<br>
양자화 이전의 유사한 결과가 나오도록 모델 업데이트

### 7.2.3 AWQ
> 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 
<br>
양자화를 수행하면서도 성능 저하를 막을 수 있다.
> 특별한 파라미터를 찾는 방법
- 모델 파라미터의 값이 큰 것
- 입력 데이터 활성화 값이 큰 채널의 파라미터
    - 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델 성능 유지됨.
> 양자화하면서도 중요 파라미터의 정보를 지키는 법
- float type을 4비트 정수로 양자화할 때
    - 4비트 정수 범위가 -8~7이므로 모델 파라미터에 2배 해준 후 반올림
    - 중요한 파라미터에만 1보다 큰 값 곱해준 후 양자화 진행
        - 곱해주는 값(scaler)이 2일 때까지는 성능 향상
        - 이유는 2보다 큰 값이면, 중요값 기준으로 양자화하기 때문에 정보 소실 가능

## 7.3 지식 증류 활용하기
> 더 크고 성능 좋은 선생 모델의 생성 결과를 활용해 더 작고 성능 낮은 학생 모델 만드는 방법
- 학생 모델이 선생 모델의 생성 결과 모방하는 방식으로 학습.
- 데이터셋 구축에 선생 모델을 사용


# 08. sLLM 서빙하기

## 8.1 효율적인 배치 전략

### 8.1.1 일반 배치(정적 배치) 
> 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식
### 8.1.2 동적 배치
> 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 방식
### 8.1.3 연속 배치
> 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장 제거하고 새로운 문장 추가
## 8.2 효율적인 트랜스포머 연산

### 8.2.1 플래시어텐션
> 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하게 만듦.
- 블록 단위로 어텐션 연산 수행
- SRAM 데이터 읽고 씀->속도 증가
### 8.2.2 플래시어텐션 2
> 개선점
- 행렬 곱셈이 아닌 연산 줄이기
- 시퀀스 길이 방향의 병렬화 추가
### 8.2.3 상대적 위치 인코딩
> 토큰과 토큰 사이의 상대적인 위치 정보 추가
- RoPE: 각각의 토큰 임베딩을 토큰 위치에 따라 회전, 토큰 사이의 위치 정보가 두 벡터 사이의 각도로 반영됨
- ALiBi: 어텐션 행렬에 오른쪽에서 왼쪽으로 갈 수록 더 작은 값 더함
## 8.3 효율적인 추론 전략

### 8.3.1 커널 퓨전
> 전후 추가적 작업을 위한 오버헤드 줄이는 압업
- 플래시 어텐션
### 8.3.2 페이지어텐션
> KV 캐시는 공간 효율이 떨어짐. 미리 공간을 예약해두기 때문. 
- 가상 메모리 개념 차용하여 논리적 메모리와 물리적 메모리 연결하는 블록 테이블 관리.
- 병렬 샘플링에서 입력 프롬프트에 대한 메모리 공유.
### 8.3.3 추측 디코딩
> 쉬운 단어는 작고 효율적인 모델이, 어려운 단어는 크고 성능 좋은 모델이 예측
- 드래프트 모델: 작은 모델로 빠른 추론
- 타깃 모델: 크고 성능 좋은 모델. 생성된 내용 검증
## 8.4 실습: LLM 서빙 프레임워크 

### 8.4.1 오프라인 서빙
> 정해진 입력 데이터에 대해 배치 추론 수행하는 것.
- 배치 크기가 증가하면 추론 시간 줄어듦.
### 8.4.2 온라인 서빙
> 사용자의 요청이 올 때 모델 추론 수행

# 3부 LLM을 활용한 실전 애플리케이션 개발

# 09. LLM 애플리케이션 개발하기

## 9.1 검색 증강 생성(RAG)
LLM의 환각 현상 해결하기 위해 답변에 필요한 정보를 검색하여 선택하는 방법
- 검색할 데이터를 벡터 데이터베이스에 저장
- 사용자의 요청에 관련된 정보를 벡터 데이터베이스에서 검색
- 사용자의 요청과 결합해 프롬프트 완성
### 9.1.1 데이터 저장
> 데이터 소스, 임베딩 모델, 벡터 데이터베이스
- 데이터 소스의 텍스트를 임베딩 모델을 통해 임베딩 벡터로 변환. 임베딩 벡터를 벡터 DB에 저장
- 임베딩 모델: OpenAI text-embedding-ada-002
- 특정 문장으로 검색 수행할 때 임베딩 벡터로 전환 후 DB에서 유사도 높은 벡터를 찾음.
### 9.1.2 프롬프트에 검색 결과 통합
> LLM은 결과 생성 시 프롬프트만 입력으로 받음.
- 검색 결과를 프롬프트에 통합해야 함. 
- 맥락 정보를 문서 임베딩으로 변환, 벡터 DB 저장
- 질문을 쿼리 임베딩으로 변환해 검색
- 두 임베딩이 가까우면 자동으로 관련 정보를 찾아 프롬프트에 추가
## 9.2 LLM 캐시
LLM 추론을 수행할 때 사용자의 요청과 생성 결과 기록하고, 비슷한 요청 들어오면 이전 결과를 가져와 바로 응답

### 9.2.1 LLM 캐시 작동 원리
> LLM 캐시 방식
- 일치 캐시: 요청이 완전히 일치하는 경우 저장된 응답 반환.
- 유사 검색 캐시: 문자열의 임베딩 벡터 비교.

## 9.3 데이터 검증
### 9.3.1 데이터 검증 방식
> 데이터 검증: 검색 결과 혹은 LLM 생성 결과에 포함되지 않아야 하는 데이터 필터링하고 답변 피해야하는 요청 선별
- 규칙 기반: 문자열 매칭 혹은 정규 표현식
- 분류 혹은 회귀 모델: 명확한 문자열 패턴이 없는 경우
- 임베딩 유사도 기반: 부적절 카테고리와 임베딩 유사도 계산
- LLM 활용: LLM 이용해 텍스트 상 부적절한 내용 있는지 판단
## 9.4 데이터 로깅
사용자의 입력과 LLM이 생성한 출력 기록
-W&B, MLflow, PromptLayer, etc.

# 10. 임베딩 모델로 데이터 의미 압축하기
## 10.1 텍스트 임베딩 이해하기
임베딩: 데이터의 의미를 압축한 벡터
<br>
문장 임베딩: 여러 문장의 텍스트를 임베딩으로 변환하는 방식

### 10.1.1 문장 임베딩 방식의 장점
> 서로 다른 텍스트의 유사도를 판단할 수 있음
### 10.1.2 원핫 인코딩
> n개의 단어가 있는 딕셔너리에서, k번째 단어 'A'를 n차원 벡터
<br>
A=[0,0,0,.....,0,1,0,....,0](1은 k번째에 위치) 로 표현하는 것
- 모든 벡터끼리의 유사도가 0이므로 관계 표현 불가.

### 10.1.3 백오브워즈
> Lemma: 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서
- 순서 고려 없이 해당 문서에 등장한 단어와 등장 횟수 집계
- 한계: 어떤 단어가 많이 나왔다고, 그 문서의 의미를 파악하는 데 도움이 되지 않는 경우도 있음 

### 10.1.4 TF-IDF
> TF-IDF(w) = TF(w) * log(N/DF(w))
- TF(w): 특정 문서에서 특정 단어 w가 등장한 횟수(Term Frequency)
- N: 전체 문서 수
- DF(w): 특정 단어 w가 등장한 문서의 수(Document Frequency)
- 많은 문서에 등장하는 단어의 중요도를 작게 만듦

### 10.1.5 워드투벡
> CBOW(Continuous Bag of Words): 주변의 단어 정보로 중간에 있을 단어 예측하는 방식
- Skip-gram: 가운데 단어 정보로 주변 단어 예측
## 10.2 문장 임베딩 방식
### 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법
BERT 모델 사용해 문장과 문장 사이 관계 계산하는 방법
- 바이 인코더: 각 문장을 임베딩으로 바꾼 뒤 코사인 유사도 계산
- 교차 인코더: 두 문장을 함꼐 BERT에 넣으면 두 문장 사이의 관계를 0-1 사이의 값으로 출력
    - 값은 정확하나, 모든 쌍에 대하여 BERT연산 해야되므로 비효율적.
### 10.2.2 바이 인코더 모델 구조
> BERT 모델의 출력을 풀링 층을 통해 고정된 크기의 문장 임베딩으로 만듦. 입력 토큰마다 출력 임베딩 생성
> 풀링 모드 세 가지
- 클래스 모드: 첫번째 토큰의 출력 임베딩
- 평균 모드: 모든 입력 토큰의 출력 임베딩을 평균한 값
- 최대 모드: 문장 길이 방향에서 최댓값

### 10.2.3 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해보기
> 이미지를 이미지 임베딩으로 변환하는 라이브러리
- OpenAI가 개발한 text-image 멀티 모달 모델 사용해 변환 가능
### 10.2.4 오픈소스와 상업용 임베딩 모델 비교하기
> 상업용 모델: 학습 성능이 뛰어나고 낮은 비용으로 사용 가능. 단, 임베딩 모델에 대한 미세 조정 지원 안 함.

> 오픈소스 모델: 자신의 데이터에 맞춰 미세 조정 수행 가능

## 10.3 실습: 의미 검색 구현하기
### 10.3.1 의미 검색 구현하기
> 밀집 임베딩을 이용해 문장이나 문서의 의미를 고려한 검색
- 데이터셋을 split하고 문장 임베딩으로 변환
- faiss IndexFlatL2 클래스를 사용해 인덱스 만듦.
- 쿼리를 임베딩으로 변환한 후 검색 수행
## 10.4 검색 방식을 조합해 성능 높이기

키워드 검색: 동일한 키워드가 많이 포함될수록 유사도 높게 평가하는 검색

### 10.4.1 키워드 검색 방식: BM25
> TF-IDF에 문서 길이 가중치 추가한 알고리즘

### 10.4.2 상호 순위 조합 이해하기
> 하이브리드 검색에서 점수 분포가 다른 문제 해결하기 위해 각 점수에서의 순위를 활용해 점수 산출

# 11. 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기

## 11.1 검색 성능을 높이기 위한 두 가지 방법
> 바이 인코더: 가벼운 벡터 연산, 유사도 검색 정확도 떨어짐
> 교차 인코더: 높은 유사도 검색 정확도, 무거운 연산
- 일단 바이 인코더로 대략적인 유사도 계산 후, 유사한 소수 문서에 대해 교차 인코더로 정밀한 계산

## 11.2 언어 모델을 임베딩 모델로 만들기

### 11.2.1 대조 학습
> 관련이 있거나 유사한 데이터는 더 가까워지도록 만들고, 관련이 없거나 유사하지 않은 데이터는 더 멀어지도록 학습

### 11.2.2 실습: 학습 준비하기
> 과정
- 사전 학습 언어 모델 불러와 문장 임베딩 모델 만들기
- 실습 데이터셋 다운로드 및 확인
- 학습 데이터에서 검증 데이터셋 분리
- label 정규화하기
- batch 데이터셋 만들기

### 11.2.3 유사한 문장 데이터로 임베딩 모델 학습하기
> 과정
- 임베딩 모델 학습
    - 에포크, 언어 모델 지정
    - 손실 함수 지정
    - 학습 데이터셋, 손실 함수, 평가 객체 인자 전달

## 11.3 임베딩 모델 미세 조정하기

### 11.3.1 실습: 학습 준비
> 과정
- 실습 데이터 다운로드
- 기본 임베딩 모델 볼러오기
- 데이터 전처리
- 성능 평가에 사용할 데이터 생성 및 성능 평가

### 11.3.2 MNR 손실을 활용해 미세조정하기
> 데이터셋이 서로 관련이 있는 문장만 있는 경우 사용하기 좋은 손실 함수

> 과정
- 서로 관련 있는 데이터만으로 학습 데이터 구성
- MNR 손실 함수 불러오기
- MRC 데이터셋으로 파인튜닝

## 11.4 검색 품질을 높이는 순위 재정렬
> 과정
- 교차 인코더로 사용할 학습 모델 불러오기
- 교차 인코더 학습 데이터셋 준비
- 교차 인코더 학습 수행
- 수행 전, 후 교차 인코더 성능 비교
    - 0.0xx -> 0.8XX

## 11.5 바이 인코더와 교차 인코더로 개선된 RAG 구현하기
> 과정
- 평가를 위한 데이터셋 불러와 1000개 선별
- 임베딩 저장하고 검색하는 함수 구현
- 교차 인코더를 활용한 순위 재정렬 함수 정의
- 성능 지표와 평가에 걸린 시간 반환하는 함수 정의

### 11.5.1 기본 임베딩 모델로 검색하기
> 히트율 88%, 평가 시간 13.21초

### 11.5.2 미세 조정한 임베딩 모델로 검색하기
> 히트율 94.6%, 평가 시간 14.30초

### 11.5.3 미세 조정한 임베딩 모델과 교차 인코더 조합하기
> 히트율 97.3%, 평가 시간 1.1초
