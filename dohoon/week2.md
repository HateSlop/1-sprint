# 07. 모델 가볍게 만들기

## 7.1 언어 모델 추론하기

### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델이 텍스트 생성을 마치는 이유
- 생성 종료를 의미하는 특수 토큰 등장
- 사용자가 최대 길이로 설정한 길이에 도달
> Auto-regressive: 한 번에 한 토큰만 생성 가능

> 추론 과정:
- 사전 계산 단계: 프롬프트를 동시에 병렬적으로 처리
- 디코딩 단계: 이후 한 토큰씩 생성
> 추론 과정에서 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산 반복하므로 비효율적임.


### 7.1.2 중복 연산을 줄이는 KV 캐시
> KV(Key-Value) 캐시: 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법
- 반복 부분은 KV 캐시에 저장해두고, 새로운 부분만 계산
- (KV 캐시 메모리) = (모델 단위 비트) * 2(key, value) * (레이어 수) * (토큰 임베딩 차원) * (최대 시퀀스 길이) * (배치 크기)
- 메모리 많이 차지함

### 7.1.3 GPU 구조와 최적의 배치 크기
> 서빙의 효율성 판단하는 기준: 같은 GPU로 처리량 높이고 지연 시간 낮춰야 함
- 비용
- 처리량: 시간당 처리한 요청 수(query/s)
- 지연 시간: 하나의 토큰 생성하는 데 걸리는 시간
> GPU 구조
- HBM: 큰 데이터 저장하는 고대역폭 메모리
- SM: 스트리밍 멀티프로세서. 하나의 GPU에 여러개 존재
    - 연산부 + SRAM(계산할 값 저장)
    - SRAM을 L1캐시, 공유 메모리로 부르기도 함
> GPU가 추론할 때
- 배치 크기만큼의 토큰 한 번에 생성
- 계산한 결과는 KV 캐시에 저장, 이후 새로운 계산 수행
    - (계산 시간) = (모델 단위 비트) * (모델 파라미터 메모리) * (배치 크기) /  (하드웨어 연산 속도)
- HBM의 모델을 SRAM으로 이동하여 계산
    - (모델 파라미터 이동 시간) = (모델 파라미터 메모리) / (메모리 대역폭)
- 최적점: (계산 시간) = (모델 타라미터 이동 시간)
    - 메모리 바운드: (계산 시간) < (모델 타라미터 이동 시간)
    - 계산 바운드: (계산 시간) > (모델 타라미터 이동 시간)
    - 최적 배치 크기(B*) = (하드웨어 연산 속도) / (모델 단위 비트) * (메모리 대역폭)

### 7.1.4 KV 캐시 메모리 줄이기
> 멀티 헤드 어텐션 방식의 비효율성
- KV 캐시 메모리 증가하므로 계산 속도 느려짐
> 멀티 쿼리 어텐션
- 여러 헤드의 쿼리 벡터가 하나의 키와 값 벡터 사용
- 성능 떨어질 가능성 있음. 이 문제 해결하기 위해 그룹 쿼리 어텐션 방식 사용
    - 2개의 쿼리 벡터당 1개의 키와 값 벡터 사용
- 해당 방법 도입 이점: 추론 속도 향상, KV 캐시 메모리 감소
    - 성능이 떨어지는 멀티 쿼리 어텐션도 추가 학습 수행하면 성능 개선됨.
## 7.2 양자화로 모델 용량 줄이기
> 양자화: 부동소수점 데이터를 정수 형식으로 변환하는 것.
- fp32 -> fp16
- 최근에는 W4A16 방식(4비트로 모델 파라미터 양자화, 계산은 16비트로)
- 수행 시점에 따라 Post-Training Quantization / Quantization-Aware Training(양자화 학습)
    - 주로 PTO방식 사용. 아래 사례는 모두 PTO방식.
### 7.2.1 비츠앤바이츠
> 양자화 라이브러리
- 8비트 행렬 연산
    - 입력 값 중 이상치가 포함된 열은 별도 분리해 16비트 그대로 계산
    - 정상 범위일 때 벡터 단위로 absmax를 찾고, 그 값을 기준으로 양자화 // 이게 뭔 말이지.. 양자화 상수끼리 연산한다는 건가?
- 4비트 정규 분포 양자화(QLoRA 방식)
### 7.2.2 GPTQ
> 양자화를 위한 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하면서 
<br>
양자화 이전의 유사한 결과가 나오도록 모델 업데이트

### 7.2.3 AWQ
> 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 
<br>
양자화를 수행하면서도 성능 저하를 막을 수 있다.
> 특별한 파라미터를 찾는 방법
- 모델 파라미터의 값이 큰 것
- 입력 데이터 활성화 값이 큰 채널의 파라미터
    - 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델 성능 유지됨.
> 양자화하면서도 중요 파라미터의 정보를 지키는 법
- float type을 4비트 정수로 양자화할 때
    - 4비트 정수 범위가 -8~7이므로 모델 파라미터에 2배 해준 후 반올림
    - 중요한 파라미터에만 1보다 큰 값 곱해준 후 양자화 진행
        - 곱해주는 값(scaler)이 2일 때까지는 성능 향상
        - 이유는 2보다 큰 값이면, 중요값 기준으로 양자화하기 때문에 정보 소실 가능

## 7.3 지식 증류 활용하기
> 더 크고 성능 좋은 선생 모델의 생성 결과를 활용해 더 작고 성능 낮은 학생 모델 만드는 방법
- 학생 모델이 선생 모델의 생성 결과 모방하는 방식으로 학습.
- 데이터셋 구축에 선생 모델을 사용


# 08. sLLM 서빙하기

## 8.1 효율적인 배치 전략

### 8.1.1 일반 배치(정적 배치) 

### 8.1.2 동적 배치

### 8.1.3 연속 배치

## 8.2 효율적인 트랜스포머 연산

### 8.2.1 플래시어텐션

### 8.2.2 플래시어텐션 2

### 8.2.3 상대적 위치 인코딩

## 8.3 효율적인 추론 전략

### 8.3.1 커널 퓨전

### 8.3.2 페이지어텐션

### 8.3.3 추측 디코딩

## 8.4 실습: LLM 서빙 프레임워크 

### 8.4.1 오프라인 서빙

### 8.4.2 온라인 서빙