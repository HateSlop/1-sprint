# 07. 모델 가볍게 만들기

## 7.1 언어 모델 추론하기

### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델이 텍스트 생성을 마치는 이유
- 생성 종료를 의미하는 특수 토큰 등장
- 사용자가 최대 길이로 설정한 길이에 도달
> Auto-regressive: 한 번에 한 토큰만 생성 가능

> 추론 과정:
- 사전 계산 단계: 프롬프트를 동시에 병렬적으로 처리
- 디코딩 단계: 이후 한 토큰씩 생성
> 추론 과정에서 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산 반복하므로 비효율적임.


### 7.1.2 중복 연산을 줄이는 KV 캐시
> KV(Key-Value) 캐시: 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법
- 반복 부분은 KV 캐시에 저장해두고, 새로운 부분만 계산
- (KV 캐시 메모리) = (모델 단위 비트) * 2(key, value) * (레이어 수) * (토큰 임베딩 차원) * (최대 시퀀스 길이) * (배치 크기)
- 메모리 많이 차지함

### 7.1.3 GPU 구조와 최적의 배치 크기
> 서빙의 효율성 판단하는 기준: 같은 GPU로 처리량 높이고 지연 시간 낮춰야 함
- 비용
- 처리량: 시간당 처리한 요청 수(query/s)
- 지연 시간: 하나의 토큰 생성하는 데 걸리는 시간
> GPU 구조
- HBM: 큰 데이터 저장하는 고대역폭 메모리
- SM: 스트리밍 멀티프로세서. 하나의 GPU에 여러개 존재
    - 연산부 + SRAM(계산할 값 저장)
    - SRAM을 L1캐시, 공유 메모리로 부르기도 함
> GPU가 추론할 때
- 배치 크기만큼의 토큰 한 번에 생성
- 계산한 결과는 KV 캐시에 저장, 이후 새로운 계산 수행
    - (계산 시간) = (모델 단위 비트) * (모델 파라미터 메모리) * (배치 크기) /  (하드웨어 연산 속도)
- HBM의 모델을 SRAM으로 이동하여 계산
    - (모델 파라미터 이동 시간) = (모델 파라미터 메모리) / (메모리 대역폭)
- 최적점: (계산 시간) = (모델 타라미터 이동 시간)
    - 메모리 바운드: (계산 시간) < (모델 타라미터 이동 시간)
    - 계산 바운드: (계산 시간) > (모델 타라미터 이동 시간)
    - 최적 배치 크기(B*) = (하드웨어 연산 속도) / (모델 단위 비트) * (메모리 대역폭)

### 7.1.4 KV 캐시 메모리 줄이기
> 멀티 헤드 어텐션 방식의 비효율성
- KV 캐시 메모리 증가하므로 계산 속도 느려짐
> 멀티 쿼리 어텐션
- 여러 헤드의 쿼리 벡터가 하나의 키와 값 벡터 사용
- 성능 떨어질 가능성 있음. 이 문제 해결하기 위해 그룹 쿼리 어텐션 방식 사용
    - 2개의 쿼리 벡터당 1개의 키와 값 벡터 사용
- 해당 방법 도입 이점: 추론 속도 향상, KV 캐시 메모리 감소
    - 성능이 떨어지는 멀티 쿼리 어텐션도 추가 학습 수행하면 성능 개선됨.
## 7.2 양자화로 모델 용량 줄이기
> 양자화: 부동소수점 데이터를 정수 형식으로 변환하는 것.
- fp32 -> fp16
- 최근에는 W4A16 방식(4비트로 모델 파라미터 양자화, 계산은 16비트로)
- 수행 시점에 따라 Post-Training Quantization / Quantization-Aware Training(양자화 학습)
    - 주로 PTO방식 사용. 아래 사례는 모두 PTO방식.
### 7.2.1 비츠앤바이츠
> 양자화 라이브러리
- 8비트 행렬 연산
    - 입력 값 중 이상치가 포함된 열은 별도 분리해 16비트 그대로 계산
    - 정상 범위일 때 벡터 단위로 absmax를 찾고, 그 값을 기준으로 양자화 // 이게 뭔 말이지.. 양자화 상수끼리 연산한다는 건가?
- 4비트 정규 분포 양자화(QLoRA 방식)
### 7.2.2 GPTQ
> 양자화를 위한 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하면서 
<br>
양자화 이전의 유사한 결과가 나오도록 모델 업데이트

### 7.2.3 AWQ
> 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 
<br>
양자화를 수행하면서도 성능 저하를 막을 수 있다.
> 특별한 파라미터를 찾는 방법
- 모델 파라미터의 값이 큰 것
- 입력 데이터 활성화 값이 큰 채널의 파라미터
    - 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델 성능 유지됨.
> 양자화하면서도 중요 파라미터의 정보를 지키는 법
- float type을 4비트 정수로 양자화할 때
    - 4비트 정수 범위가 -8~7이므로 모델 파라미터에 2배 해준 후 반올림
    - 중요한 파라미터에만 1보다 큰 값 곱해준 후 양자화 진행
        - 곱해주는 값(scaler)이 2일 때까지는 성능 향상
        - 이유는 2보다 큰 값이면, 중요값 기준으로 양자화하기 때문에 정보 소실 가능

## 7.3 지식 증류 활용하기
> 더 크고 성능 좋은 선생 모델의 생성 결과를 활용해 더 작고 성능 낮은 학생 모델 만드는 방법
- 학생 모델이 선생 모델의 생성 결과 모방하는 방식으로 학습.
- 데이터셋 구축에 선생 모델을 사용


# 08. sLLM 서빙하기

## 8.1 효율적인 배치 전략

### 8.1.1 일반 배치(정적 배치) 
> 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식
### 8.1.2 동적 배치
> 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 방식
### 8.1.3 연속 배치
> 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장 제거하고 새로운 문장 추가
## 8.2 효율적인 트랜스포머 연산

### 8.2.1 플래시어텐션

### 8.2.2 플래시어텐션 2

### 8.2.3 상대적 위치 인코딩

## 8.3 효율적인 추론 전략

### 8.3.1 커널 퓨전

### 8.3.2 페이지어텐션

### 8.3.3 추측 디코딩

## 8.4 실습: LLM 서빙 프레임워크 

### 8.4.1 오프라인 서빙

### 8.4.2 온라인 서빙

# 3부 LLM을 활용한 실전 애플리케이션 개발

# 09. LLM 애플리케이션 개발하기

## 9.1 검색 증강 생성(RAG)
LLM의 환각 현상 해결하기 위해 답변에 필요한 정보를 검색하여 선택하는 방법
- 검색할 데이터를 벡터 데이터베이스에 저장
- 사용자의 요청에 관련된 정보를 벡터 데이터베이스에서 검색
- 사용자의 요청과 결합해 프롬프트 완성
### 9.1.1 데이터 저장
> 데이터 소스, 임베딩 모델, 벡터 데이터베이스
- 데이터 소스의 텍스트를 임베딩 모델을 통해 임베딩 벡터로 변환. 임베딩 벡터를 벡터 DB에 저장
- 임베딩 모델: OpenAI text-embedding-ada-002
- 특정 문장으로 검색 수행할 때 임베딩 벡터로 전환 후 DB에서 유사도 높은 벡터를 찾음.
### 9.1.2 프롬프트에 검색 결과 통합
> LLM은 결과 생성 시 프롬프트만 입력으로 받음.
- 검색 결과를 프롬프트에 통합해야 함. 
- 맥락 정보를 문서 임베딩으로 변환, 벡터 DB 저장
- 질문을 쿼리 임베딩으로 변환해 검색
- 두 임베딩이 가까우면 자동으로 관련 정보를 찾아 프롬프트에 추가
## 9.2 LLM 캐시
LLM 추론을 수행할 때 사용자의 요청과 생성 결과 기록하고, 비슷한 요청 들어오면 이전 결과를 가져와 바로 응답

### 9.2.1 LLM 캐시 작동 원리
> LLM 캐시 방식
- 일치 캐시: 요청이 완전히 일치하는 경우 저장된 응답 반환.
- 유사 검색 캐시: 문자열의 임베딩 벡터 비교.

## 9.3 데이터 검증
### 9.3.1 데이터 검증 방식
> 데이터 검증: 검색 결과 혹은 LLM 생성 결과에 포함되지 않아야 하는 데이터 필터링하고 답변 피해야하는 요청 선별
- 규칙 기반: 문자열 매칭 혹은 정규 표현식
- 분류 혹은 회귀 모델: 명확한 문자열 패턴이 없는 경우
- 임베딩 유사도 기반: 부적절 카테고리와 임베딩 유사도 계산
- LLM 활용: LLM 이용해 텍스트 상 부적절한 내용 있는지 판단
## 9.4 데이터 로깅
사용자의 입력과 LLM이 생성한 출력 기록
-W&B, MLflow, PromptLayer, etc.

# 10. 임베딩 모델로 데이터 의미 압축하기
## 10.1 텍스트 임베딩 이해하기
임베딩: 데이터의 의미를 압축한 벡터
<br>
문장 임베딩: 여러 문장의 텍스트를 임베딩으로 변환하는 방식

### 10.1.1 문장 임베딩 방식의 장점
> 서로 다른 텍스트의 유사도를 판단할 수 있음
### 10.1.2 원핫 인코딩
> n개의 단어가 있는 딕셔너리에서, k번째 단어 'A'를 n차원 벡터
<br>
A=[0,0,0,.....,0,1,0,....,0](1은 k번째에 위치) 로 표현하는 것
- 모든 벡터끼리의 유사도가 0이므로 관계 표현 불가.

### 10.1.3 백오브워즈
> Lemma: 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서
- 순서 고려 없이 해당 문서에 등장한 단어와 등장 횟수 집계
- 한계: 어떤 단어가 많이 나왔다고, 그 문서의 의미를 파악하는 데 도움이 되지 않는 경우도 있음 
