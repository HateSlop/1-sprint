# 12. 벡터 데이터베이스로 확장하기: RAG 구현하기
## 12.1 벡터 데이터베이스란
### 12.1.1 딥러닝과 벡터 데이터베이스
> 표현 학습: 데이터가 충분하면 모델이 알아서 특징 추출

> 벡터 데이터베이스 활용 방법
- 저장: 저장할 데이터를 벡터로 변환 후 벡터 DB에 저장
- 검색: 검섹할 데이터를 벡터로 변환하고 벡터 DB에서 검색
- 결과 반환: 벡터 DB에서 검색 쿼리의 임베딩과 거리가 가까운 벡터 찾아 반환
    - 유사도 계산: 유클리드 거리, 코사인 유사도, 내적

### 12.1.2 벡터 데이터베이스 지형 파악하기
> 벡터 데이터베이스 소프트웨어 종류
- 벡터 라이브러리
- 벡터 전용 데이터베이스
- 벡터 기능 추가 데이터베이스
> 벡터 데이터베이스 기능
- 메타 데이터의 저장 및 필터링
- 데이터의 백업 및 관리
- 모니터링, 관련 AI도구 등 에코시스템과의 통합
- 데이터 보안과 엑세스 관리

## 12.2 벡터 데이터베이스 작동 원리
### 12.2.1 KNN 검색과 그 한계
> 검색하려는 벡터와 가장 가까운 K개의 이웃 벡터 찾는 방식
- 직관적, 모든 데이터 조사하여 정확
- 데이터 많으면 연산량 증가, 확장성 떨어짐.
> 과정
- 인덱스 만들기
- 검색 하기(검색 시간과 재현율이 중요)

### 12.2.2 ANN 검색이란
> 대용량 데이터셋에서 주어진 쿼리 항목과 가장 유사한 항목을 효율적으로 찾는 방법
- 임베딩 벡터 빠르게 탐색할 수 있는 구조로, 검색 시 탐색 범위 좁히는 데 집중.
- IVF, HNSW
### 12.2.3 탐색 가능한 작은 세계(NSW)
> 완전히 랜덤한 그래프와 완전히 규칙적인 그래프 사이에 적당히 랜덤하게 연결된 그래프 상태
- 완전히 규칙적인 그래프보다 탐색 단계를 줄여줌.
- 지역 최솟값에서 탐색을 종료할 문제가 생김
### 12.2.4 계층 구조
> 연결 리스트
- 새로운 데이터 추가 및 삭제가 자유로움
- 탐색 속도 느림
> 탐색 속도 높이기 위해
- 데이터 정렬
    - 스킵 리스트
        - for n to 1;
            level(n) is included in level(n-1)
        - for n to 0:
            search(level(n));
## 12.3 실습: HNSW 인덱스의 핵심 파라미터 이해하기

### 12.3.1 파라미터 m 이해하기
> m: 추가하는 임베딩 벡터에 연결하는 간선의 수
- m값이 커지면 검색 품질 개선, m값이 감소하면 검색 시간 증가
### 12.3.2 파라미터 ef_construction 이해하기
> M개의 가장 가까운 벡터를 선택할 후보군
- M증가: 성능 증가, 색인 시간 증가. 메모리 사용량, 검색 시간은 영향 안받음.
### 12.3.3 파라미터 ef_search 이해하기
> 검색 단계에서 후보군의 크기 결정

## 12.4 실습: 파인콘으로 벡터 검색 구현하기

### 12.4.1 파인콘 클라이언트 사용법
> 과정
- 인덱스 이름과 입력할 임베딩 벡터 차원수 설정해 인덱스 설정
- 생성한 인덱스의 이름을 입력해 사용할 인덱스 불러오기
- 임베딩 모델로 데이터를 임베딩으로 변환
- numpy라이브러리 데이터 타입을 리스트로 변환
- 파인콘 형식에 맞게 데이터 변환
- 데이터 검색 시 query 메서드 사용
- 데이터 수정 시 update 메서드, 데이터 삭제 시 delete 메서드 사용
- 이 과정을 CRUD(create, read, update, delete)라고 함.

## 12.5 실습: 파인콘을 활용해 멀티 모달 검색 구현하기
이미지 생성 기능을 파인콘 벡터 데이터베이스로 구현하기

### 12.5.1 데이터셋
> 데이터: 이미지 칼럼, 프롬프트 칼럼만 사용

### 12.5.2 실습 흐름
- GPT 설명 프롬프트: GPT가 생성한 이미지 설명문
- 원본 프롬프트: 이미지에 대응되는 프롬프트
- 유사 프롬프트: 원본 이미지를 이미지 임베딩으로 변환 후 검색해 찾은 프롬프트

# 13. LLM 운영하기

## 13.1 MLOps
> DevOps 개념을 머신러닝, 데이터 과학 분야로 확장한 방법론
- 데이터 수집, 전처리, 모델 학습, 평가, 배포, 모니터링 등 머신 러닝 프로젝트의 전 과정 자동화 & 효율화.
- 모델의 재현성을 보장하는 것이 매우 중요(이전에 수행된 ML 워크플로를 그대로 반복했을 때 동일한 모델을 얻을 수 있는지 여부)
- 모델 학습을 자동으로 트리거하여 새로운 데이터로 지속적으로 모델 업데이트.
- 모델 최적화

### 13.1.1 데이터 관리
> 포함시킬 데이터의 범위를 선택하고 어떤 전처리 방식을 포함시킬지, 특성 공학을 통해 어떤 특성을 추가할지에 따라 학습 데이터셋이 달라짐.
> DVC(DataVersionControl)을 통해 데이터셋 버전 관리 모델 학습 진행한 데이터셋 기록

### 13.1.2 실험 관리
> 모델 성능에 영향을 주는 다양한 하이퍼파라미터 값을 조정해 최적값 찾기.
- MLflow, W&B로 실험 관리 및 추적

### 13.1.3 모델 저장소
> 머신러닝 모델을 체계적으로 관리하고 버전 제어하는 데 필수적인 요소. 
- 모델 개발 과정에서 여러 파이프라인에서 다양한 버전의 모델이 나오는데, 이를 통합 관리
- MLflow 모델저장소, AWS 세이지메이커

### 13.1.4 모델 모니터링
> 모델이 반환하는 값의 퀄리티를 평가하는 모니터링 수행
- 모델을 배포한 후 데이터 분포가 학습 데이터와 차이가 커지면 성능 떨어짐.
- 머신러닝 시스템의 기본적인 지표 기록해 문제 여부, 추가 리소스 확보 필요 여부, 리소스 낭비 여부 모니터링.

## 13.2 LLMOps는 무엇이 다를까?
> 머신러닝, LLM 모델 차이점
- LLM은 기존 머신러닝 모델에 비해 훨씬 크고 일부 기업이 API 기반으로 상업용 모델 제공
- 머신러닝은 평가 지표가 명확한 작업에, LLM은 정량적 평가가 어려운, 생성 작업에 사용
### 13.2.1 상업용 모델과 오픈소스 모델 선택하기
> LLMOps는 MLOps보다 훨씬 크고 다양한 일을 할 수 있는 모델을 다룸
- 사용하려는 목적과 문제의 난이도에 따라 상업용 모델/ 오픈소스 모델 중 적절한 모델 선택

### 13.2.2 모델 최적화 방법의 변화
> 기본적으로 LLM은 사전 학습된 모델을 가져와 미세 조정 수행.
- 미세 조정 과정에서 모델 학습 최적화
> 프롬프트 구조화 작업
- 프롬프트를 기록해두고, 성능 평가 진행

## 13.3 LLM 평가하기

### 13.3.1 정량적 지표
> 텍스트 생성 작업 평가 시 사용하는 세 가지 정량 지표
- BLEU: 기계 번역 결과와 사람이 번역한 결과의 유사도를 측정해 번역 성능 평가
- ROUGE: 모델이 생성한 요약문과 사람이 작성한 참조 요약문 사이의 n-gram 중복도 측정
- PPL: 단어 생성 시 불확실성을 수치화
### 13.3.2 벤치마크 데이터셋을 활용한 평가
> 벤치마크 데이터셋: 다양한 모델의 성능을 비교하기 위해 공통으로 사용하는 데이터셋

### 13.3.3 사람이 직접 평가하는 방식
> 정성 평가 가능하나 시간 오래 걸리고 비용 많이 듦.

### 13.3.4 LLM을 통한 평가
> 80개의 선별한 멀티 턴 질문 데이터인 MT-Bench와 챗봇 아레나 데이터 활용하여 선호도 평가
### 13.3.5 RAG 평가
> 평가 기준
- 신뢰성: 생성된 응답이 검색된 맥락 데이터에 얼마나 사실적으로 부합하는지
- 답변 관련성: 생성된 답변이 요청과 얼마나 관련성이 있는지
- 맥락 관련성: 검색 결과인 맥락 데이터가 요청과 얼마나 관련 있는지

# 4부 멀티 모달, 에이전트 그리고 LLM의 미래
# 14. 멀티 모달 LLM
## 14.1 멀티 모달 LLM이란
### 14.1.1 멀티 모달 LLM의 구성 요소
> 구성 요소
- 모달리티 인코더: 텍스트 외 데이터 처리 위해 사전 학습된 모델
    - 비전 트랜스포머가 가장 많이 사용됨. 이미지를 패치 단위로 자른 후 일렬로 나열해 입력해 처리.
    - CLIP 모델: 이미지와 텍스트를 같은 벡터 공간에 임베딩.
- 입력 프로젝터: 이미지 임베딩을 텍스트로 변환
- LLM 백본: 텍스트 처리
- 출력 프로젝터: 텍스트 출력
- 모달리티 생성기: 텍스트 출력 결과 바탕으로 이미지 생성

### 14.1.2 멀티 모달 LLM 학습 과정
> 과정
- 추가 훈련 단계(멀티 모달 입출력 지원): 사전 학습 + 지시 학습
- 멀티 모달 지시 튜닝: 특정 멀티 모달 작업을 수행하도록 학습

## 14.2 이미지와 텍스트를 연결하는 모델: CLIP
### 14.2.1 CLIP 모델이란?
> 텍스트와 이미지 데이터의 관계 계산을 위해 텍스트 모델과 이미지 모델을 함께 학습시킨 모델.
### 14.2.2 CLIP 모델의 학습 방법
> 데이터: 서로 관련 있는 이미지와 텍스트 쌍
> 학습 방법: 
- 대조 학습을 통해 모델 학습
- 이미지 인코더(ViT, ResNet), 텍스트 인코더(Transformer Architecture)
### 14.2.3 CLIP 모델의 활용과 뛰어난 성능
> 제로샷 추론: 사전 학습 데이터 이외에 특정 작업을 위한 데이터로 미세 조정하지 않은 상태에서 추론을 수행하는 것

> 이미지 검색: 이미지와 텍스트의 유사도를 기반으로 텍스트 입력 시 유사한 이미지 찾는 기능.

> 전문성 요구되는 이미지 인식에서는 ResNet에 비해 성능이 낮음.

## 14.3 텍스트로 이미지를 생성하는 모델: DALL-E
LLM 백본의 텍스트 출력을 기반으로 이미지 생성하는 DALL-E.
얘가 쓰는 디퓨전 모델
### 14.3.1 디퓨전 모델 원리
> 확산 현상에서 영감을 받아 만들어진 생성 모델.
- 확산: 데이터 관점에서는 데이터 분포가 랜덤하게 변하는 것.
- 원본 이미지를 완전히 랜덤한 노이즈로 바꾸고, 다시 어떤 부분이 노이즈인지 예측하는 방식으로 학습
> 디퓨전 모델로는 U-Net이라는 인코더-디코더 모델을 많이 사용. 
- 인코더 디코더
    - 인코딩: 입력 데이터 차원을 낮추기
    - 디코더: 입력 데이터 차원을 높이기
    - U-Net은 인코딩 단계의 고차원 정보를 활용하여 이미지 위치 정보 손실 방지.
### 14.3.2 DALL-E 모델
> 과정
- CLIP 모델로 텍스트 임베딩 생성(텍스트 임베딩과 이미지 임베딩 거리 가깝게 만드는 방향으로 CLIP 학습)
- 프라이어 모델로 CLIP 이미지 임베딩 만들고, 디코더로 이미지 생성
    - 프라이어란 텍스트 임베딩을 입력으로 받아 이미지 임베딩 예측하는 디퓨전 모델
    - 프라이어, 디코더 모두 디퓨전 모델. 얘네 같이 사용하면 성능 올라감

## 14.4 LLaVA
앞선 두 모델은 이미지에 대해 글을 생성하거나 대화를 나눌 수는 없다.
<br>
LLaVA: CLIP + LLM -> 이미지 인식 및 텍스트 생성

### 14.4.1 LLaVA의 학습 데이터
> GPT-4 모델에 이미지에 대한 설명과 위치 정보를 입력해 다음 세 가지 유형의 텍스트 생성하도록 했다(마치 이미지를 본 것 처럼)
- 대화: 사람이 이미지에 대해 질문했을 때 어시스턴트가 이미지를 보고 답변하는 형식의 데이터
- 자세한 설명: 이미지 설명을 읽고 이미지에 대해 자세히 설명하도록
- 복잡한 추론: 어려운 질문 생성하고 답변하게
### 14.4.2 LLaVA 모델 구조
> Xv(이미지)와 Xq(텍스트 쿼리)를 텍스트 임베딩으로 변환하여 Xa(텍스트 응답) 생성

### 14.4.3 LLaVA 1.5
> 버전 1에서 CLIP의 버전을 높이고, 2층의 선형 층으로 토큰화함.
- 적은 데이터를 통해 학습 가능, 성능 올라감
### 14.4.4 LLaVA NeXT
> 버전 1.5 발전시킨 모델. 개선 사항:
- 입력 이미지 해상도 4배 증가
- 고품질 지시 데이터셋 구축, 시각적 추론 능력과 OCR 성능 개선
- 더 많은 시나리오에서 응답 가능. 다양한 애플리케이션에 활용
- SGLang 프레임워크 사용해 추론 성능 증가