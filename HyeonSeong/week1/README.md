# LLM을 활용한 실전 AI 애플리케이션 개발
------------------
# 7 모델 가볍게 만들기

## 7.1 언어 모델 추론 이해하기

### 7.1.1 언어 모델이 언어를 생성하는 방법
언어 모델이 텍스트 생성을 마치는 이유는 크게 두가지
- 다음 토큰으로 생성 종료를 의미하는 특수 토큰을 생성하는 경우
- 사용자가 최대 길이로 설정한 길이에 도달하면 더 이상 생성하지 않고 종료

그림 7.1

언어모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적(auto-regressive)인 특성을 보유
동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복적으로 수행하기 때문에 비효율적

그림 7.3

### 7.1.2 중복 연산을 줄이는 KV 캐시
KV 캐시 - 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법   

그림 7.4

KV 캐시 메모리의 계산식

KV 캐시 메모리 = 2바이트 X 2(키의 값) X (레이어 수) X (토큰 임베딩 차원) X (최대 시퀀스 길이) X (배치 크기)

처음 2는 fp16의 데이터 형식이기 때문에 2바이트
두번째 2는 키 캐시와 값 2개를 저장하기 때문에 2    
어텐션의 연산 결과는 레이어 수 만큼 생기기 때문에 레이어 수   
토큰 임베딩을 표현하는 차원의 수만큼 수를 저장하기 때문에 차원 수   
최대로 생성하려는 시퀀스 길이만큼의 메모리를 미리 확보하기 때문에 시퀀스 길이   
배치 크기에 따라 저장하는 데이터가 달라지므로 배치 크기   

### 7.1.3 GPU 구조와 최적의 배치 크기
서빙이 효율적인지 판단하는 큰 기준 3 가지
1. 비용
2. 처리량(throughput) - 시간당 처리한 요청(쿼리) 수
3. 지연 시간(latency) - 하나의 토큰을 생성하는 데 걸리는 시간   

그림 7.6
GPU는 여러 스트리밍 멀티프로세서(streaming multiprocessors)로 구성   
각각의 SM에는 연산을 수행하는 부분과 값을 저장하는 SRAM(Static Random Access Memory)가 존재   
연산을 수행하는 부분과 가까운 SRAM에는 큰 메모리를 갖기 어렵기 때문에 큰 고대역폭 메모리(High Bandwidth Memory)에 큰 데이터를 저장
추론을 수행할 때는 연산을 수행하는 시간과 메모리를 이동시키는 시간이 걸림   
모델의 이동 과정과 연산 수행 과정은 함께 진행되기 때문에 두 가지 시간이 같을 때가 최적의 배치 크기   
- 메모리 바운드: 배치 크기가 작아서 모델 파라미터를 이동시키느라 연산이 멈추는 비효율 발생
- 연산 바운드: 배치 크기가 커서 연산에 더 오랜 시간이 발생   

그림 7.9

### 7.1.4 KV 캐시 메모리 줄이기
멀티 쿼리 어텐션(multi-query attention) - 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유하는 방식   
그룹 쿼리 어텐션(grouped-query attention) - 멀티 쿼리 어텐션의 성능 감소로 인해 그보다는 키와 값의 수를 늘린 방식   

그림 7.11

키의 값의 수를 줄이면서 크게 추론 속도 향상과 KV 캐시 메모리의 감소에 대한 효과가 있음   
멀티 쿼리 어텐션의 경우 멀티 헤드 어텐션과 비교했을 때 성능 저하가 뚜렷하기 때문에 키와 값을 줄인 이후에 기존의 학습 데이터로 추가 학습을 수행   
그에 반해 그룹 쿼리 어텐션은 멀티 헤드 어텐션과 성능 차이가 적음   

## 7.2 양자화로 모델 용량 줄이기
양자화를 수행하는 시점에 따라 학습 후 양자화 / 양자화 학습으로 나눔   
LLM의 경우 학습에 많은 자원이 들기 때문에 새로운 학습이 필요한 양자화 학습보다는 학습 후 양자화를 주로 활용   
### 7.2.1 비츠앤바이츠
비츠앤바이츠 - 8비트 연산을 수행하면서도 성능 저하가 거의 없는 8비트 행렬 연산과 4비트 정규 분포 양자화 방식을 제공하는 양자화 라이브러리   
8비트 행렬 연산 - 입력값 중 크기가 큰 이상치가 포함된 열은 별도로 분리해서 16비트 그대로 계산

그림 7.15

### 7.2.2 GPTQ
GPTQ(GPT Quantization) - 모델에 입력 X를 넣었을 때와 양자화 이후의 모델에 입력 X를 넣었을 때 오차가 가장 작아지도록 모델의 양자화를 수행   

그림 7.16

흰색 열의 양자화를 수행하고 양자화를 위해 준비한 데이터를 입력한 결과가 이전과 최대한 가까워지도록 아직 양자화하지 않은 오른쪽 부분의 파라미터를 업데이트   

### 7.2.3 AWQ
AWQ(Activation-aware Weight Quantization) - 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있다는 아이디어에서 출발   
어떤 파라미터가 중요한 지 크게 두가지로 판단
1. 모델 파라미터의 값이 크다
2. 입력 데이터 활성화 값이 큰 채널

위의 기준을 통해 상위 1%에 해당하는 모델 파라미터를 찾고 해당 파라미터는 기존 모델의 데이터 타입인 FP16을 유지하고 나머지는 양자화 진행하니 성능 저하가 거의 발생하지 않음   

그림 7.20

중요한 파라미터에만 1보다 큰 스케일러 값을 곱하는 방식으로 양자화 진행   
그러나 스케일러 S가 2일 때까지는 성능이 향상되지만 2를 넘어가는 경우 성능이 다시 하락하는 사실 확인   

그림 7.21

스케일러가 큰 경우 가장 큰 수의 값이 변화하기 때문에 다른 파라미터에 영향을 주어 정보 소실이 발생   

## 7.3 지식 증류 활용하기
지식 증류(knowledge distillation) - 더 크고 성능이 높은 선생 모델(teacher model)의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델(student model)을 만드는 방법   
그림 7.22

제퍼-7B-베타 모델의 경우 개발의 지시 데이터셋의 구축과 선호 데이터셋의 구축에 모두 LLM을 사용   
그림 7.23   
사람의 리소스가 필요한 작업에 모델을 활용해 개발 속도를 높이고 자원을 아낄 수 있음   