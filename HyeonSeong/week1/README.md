# LLM을 활용한 실전 AI 애플리케이션 개발
------------------
# 1.LLM 지도

## 1.1 딥러닝과 언어 모델링
**LLM(Large Language Model)** - 딥러닝에 기반
사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 **자연어 처리(natural language processing)**   
**언어 모델** - 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩 만들어 가는 방식으로 텍스트 생성

### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝   
문제를 해결하는 방법   
>문제의 유형에 따라 일반적으로 사용되는 모델을 준비   
>풀고자 하는 문제에 대한 학습 데이터 준비   
>학습 데이터를 반복적으로 모델에 입력   

딥러닝 / 머신러닝 - 데이터의 특징을 누가 뽑는가?   
머신러닝은 데이터의 특징을 연구자 또는 개발자가 추출 / 딥러닝은 모델이 스스로 데이터의 특징을 찾고 분류   

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식   
**임베딩** - 데이터의 의미와 특징을 포착해 숫자의 집합으로 표현하는 것   
거리를 계산할 수 있기 때문에 다음과 같은 작업에 적합 

    검색 및 추천: 검색어와 관련이 있는 상품을 추천
    클러스터링및 분류: 유사하고 관련이 있는 데이터를 하나로 분류
    이상치 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 표현   

그림 1.6

### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법   
**전이 학습** - 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다룬 문제를 풀 때 사용하는 방식   
대량의 데이터로 모델을 학습시키는 **사전학습** / 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 **미세 조정**   
특정한 데이터만으로 학습한 모델보다 사전 학습 모델의 일부를 가져와 활용했을 때 더 성능이 좋음   
그림 1.9   

## 1.2 언어 모델이 챗GPT가 되기까지
### 1.2.1 RNN에서 트랜스포머 아키택처로   
**RNN** - 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측   
그림 1.12   
**트랜스포머 아키텍처**는 순차적인 처리 방식이 아닌, 맥락을 모두 참조하는 어탠션(attention)연산을 사용   
그림 1.14   
맥락을 압축하지 않고 그대로 활용하기 때문에 성능은 높아지는 대신 무겁고 비효율적인 연산을 사용   

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계
GPT-1 = 1억 1,700만개의 파라미터 > GPT-2 = 15억개 > GPT-3 = 1,750억개   
언어 모델이 학습하는 과정을 학습 데이터로 압축   
그림 1.18   
하지만 모델이 계속해서 커진다고 성능이 비례하진 않고 학습 데이터의 크기가 최대 모델 크기의 상한   

### 1.2.3 챗GPT의 등장   
GPT-3는 그저 사용자의 말을 이어서 작성하는 능력밖에 없음   
**지도 미세 조정**(supervised fine-tuning)과 **RLHF**(Reinforcement Learning from Human Feedback)을 통해 사용자의 요청을 해결할 수 있는 텍스트를 생성 가능   
정렬(alignment) - LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것   
지도 미세 조정 - 언어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋(instruction dataset)으로 추가 학습하는 것   
지시 데이터셋 - 사용자가 요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터셋   
OpenAI는 두 가지 답변 중 사용자가 더 선호하는 답변의 데이터셋을 선호 데이터셋(preference dataset)으로 정리하고 이를 답변으로 평가하는 리워드 모델(reward model)을 생성하여 더 높은 점수를 받을 수 있도록 추가 학습 진행   
위와 같은 강화 학습을 진행하는 것 - RLHF   

## 1.3 LLM 애플리케이션의 시대가 열리다
### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM   
기존의 자연어 처리 접근 방식에는 언어 이해 모델과 언어 생성 모델을 각각 개별해 연결   
LLM은 하나로 연결되어 더 빠르고 다양한 작업에 활용 가능   
그림 1.22 23

### 1.3.2 sLLM: 더 작고 효율적으로 모델 만들기
LLM을 활용하는 방법
1. OpenAI의 GPT-4와 같이 상업용 API를 사용
2. 오픈소스 LLM을 활용해 직접 생성

**sLLM** - 추가 학습을 하는 경우 모델의 크기가 작아서 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델

### 1.3.3 더 효율적인 학습과 추론을 위한 기술
많은 연산량을 처리하기 위해서 GPU를 사용 - 상당 부분의 비용이 이에 발생   
더 적은 비트로 표현하는 **양자화**(quantization)과 모델의 일부만 학습하는 **LoRA**(Low Rank Adaptation)을 활용하여 연산을 개선   

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 생성(RAG) 기술
**환각 현상** - LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상   
**검색 증강 생성**(Retrieval Augmented Generation) - 프롬프트에 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제 해결   

## 1.4 LLM의 미래: 인식과 행동의 확장
**멀티 모달**(multi modal) - 더 다양한 형식의 데이터를 입출력하도록 발전시킨 LLM   

## 1.5 정리
그림 1.27

# 2 LLM의 중추, 트랜스포머 아키텍처 살펴보기
## 2.1 트랜스포머 아키텍처란
기존의 RNN은 학습 속도가 느리고, 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어짐   
트랜스포머는 셀프 어텐션(self-attention)이라는 개념을 도입   
셀프 어텐션 - 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정   
트랜스포머의 장점   
>확장성: 더 깊은 모델을 만들어도 학습이 잘됨. 동일한 블록을 반복해 사용하기 때문에 확장이 용이   
>효율성: 학습할 때 병렬 연산이 가능하기 때문에 학습 시간이 단축   
>더 긴 입력 처리: 입력이 길어져도 성능이 거의 떨어지지 않음   

 그림 2.2   
공통적으로 입력을 임베딩(embedding)층을 통해 숫자 집합인 임베딩으로 변환
위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더함

## 2.2 텍스트를 임베딩으로 변환하기
### 2.2.1 토큰화
**토큰화**(tokenization) - 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여   
큰 단위를 기준으로 토큰화할수록 텍스트의 의미가 잘 유지되지만 사전의 크기가 커짐   
그림 2.4   
데이터의 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드(subword) 토큰화 방식을 사용   
그림 2.5   

### 2.2.2 토큰 임베딩으로 변환하기
토큰이 의미를 담기 위해서는 최소 2개 이상의 숫자 집합인 벡터(vector)여야 함   
딥러닝에서는 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습   

### 2.2.3 위치 인코딩
트랜스포머는 모든 입력을 동시에 처리하기 때문에 텍스트에서 순서 정보를 추가하기 위해 위치 인코딩을 진행   
**절대적 위치 인코딩**(absolute position encoding) - 모델로 추론을 수행하는 시점에서는 입력 토큰의 위치에 따라 고정된 임베딩을 더함   
**상대적 위치 인코딩**(relative position encoding) - 긴 텍스트를 추론하는 경우 절대적 위치 인코딩의 성능이 떨어져 상대적인 위치 정보를 더함   
그림 2.7   

## 2.3 어텐션 이해하기
### 2.3.1 사람이 글을 읽는 방법과 어텐션
**어텐션** - 사람이 단어 사이의 관계를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방한 연산   
단어와 단어 사이의 관계를 계산해서 그 값에 따라 관련이 깊은 단어와 그렇지 않은 단어를 구분   

### 2.3.2 쿼리, 키, 값 이해하기
위와 같은 과정을 처리하기 위해 정보 검색의 개념에서 쿼리, 키, 값이라는 개념을 도입   
>쿼리: 우리가 입력하는 검색어   
>키: 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징   
>값: 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬한 문서   

그림 2.9

원하는 결과를 얻기 위해 쿼리와 키 토큰을 토큰 임베딩을 변환하여 계산   
- 같은 단어끼리는 무조건 임베딩이 동일하게 발생한다는 문제   
- 간접적인 관련성은 반영되기 어려움

토큰 임베딩을 변환하는 가중치를 도입   
쿼리, 키, 값 세 가지 가중치를 통해 내부적으로 토큰과 토큰 사이의 관계를 계산해서 적절히 주변 맥락을 반영   
그림 2.14   

### 2.3.3 코드로 보는 어텐션
예제 2.7

### 2.3.4 멀티 헤드 어텐션
멀티 헤드 어텐션 - 한번에 여러 어텐션 연산을 동시에 적용하여 성능을 더 향샹   
그림 2.17   

## 2.4 정규화와 피드 포워드 층
**정규화** - 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법   
과거에는 배치 입력 데이터 사이에 정규화를 수행하는 **배치 정규화**(batch 
트랜스포머는 특징 차원에서 정규화를 수행하는 **층 정규화**(layer normalization)사용   
전체 입력 문장을 이해하는 연산을 위해 **완전 연결 층**(fully connected layer)인 피드 포워드 층을 사용   

### 2.4.1 층 정규화 이해하기
**층 정규화** - 데이터 분포가 서로 다르면 정확한 예측을 어렵게 만드는데, 이 데이터를 정규화하여 모든 입력 변수가 비슷한 범위와 분포를 갖도록 하는 것   
이미지 처리에서는 배치 정규화를 사용하고 자연어 처리에서는 층 정규화를 사용   
그림 2.18
그림 2.19   
**사전 정규화**(pre-norm) - 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통과했을 때 학습이 더 안정적   

### 2.4.2 피드 포워드 층
피드 포워드 층(feed forward layer) - 데이터의 특징을 학습하는 완전 연결 층(fully connected layer)을 말함   
선형 층, 드롭아웃 층, 층 정규화, 활성 함수로 구성됨   

## 2.5 인코더
멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태   
예제 2.11   

## 2.6 디코더
디코더 블록에서는 **마스크 멀티 헤드 어텐션**과 **크로스 어텐션**(cross attention) 사용   
앞에서 생성한 토큰을 기반으로 다음 토큰을 생성 - 인과적(causal) 또는 자기 회귀적(auto-regressive)   
학습할 때는 인코더와 디코더 모두 완성된 텍스트를 입력받기 때문에 특정 시점에서는 그 이전에 생성된 토큰까지만 확인할 수 있도록 마스크를 추가   
## 2.7 BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
표 2.1

### 2.7.1 인코더를 활용한 BERT
BERT(Bidirectional Encoder Representations from Transformers) - 구글에서 개발한 트랜스포머의 인코더만을 활용해 자연어 이해 태스크에 집중한 대표적인 모델   
- 입력 토큰의 일부를 마스크 토큰으로 대체하고 그 마스크 토큰을 맞추는 마스크 언어 모델링을 통해 사전 학습
- 양방향 문맥을 이해할 수 있다는 특징이 있어 자연어 이해 작업에서 뛰어난 성능

### 2.7.2 디코더를 활용한 GPT
GPT(Generative Pre-trained Transformer) - OpenAI에서 개발한 생성 작업을 위해 만든 모델
- 디코더만을 사용하여 생성 작업의 경우 입력 토큰이나 이전까지 생성한 토큰만을 문맥으로 활용하는 인과적 언어 모델링(Casual Language Modeling)을 사용하기 때문에 단방향 방식임

### 2.7.3 인코더와 디코더를 모두 사용하는 BART, T5
BART - 메타가 개발한 BERT와 GPT의 장점을 결합한 모델
- BERT보다 다양한 사전 학습 과제를 도입했고 더 자유로우누 변형 추가가 가능하다는 점에 차이가 있음   
T5 - 구글이 개발하였으며 모든 자연어 처리 작업이 결국 '텍스트에서 텍스트(Text to Text)로의 변환'이라는 아이디어를 바탕으로 함

## 2.8 주요 사전 학습 메커니즘
## 2.8.1 인과적 언어 모델링
인과적 언어 모델링 - 문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식   

## 2.8.2 마스크 언어 모델링
마스크 언어 모델링 - 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습   

# 3 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리
## 3.1 허깅페이스 트랜스포머란
허깅페이스(Huggingface)팀이 개발한 트랜스포머(Transformers) 라이브러리는 공통된 인터페이스로 트랜스포머 모델을 활용할 수 있도록 지원함으로써 현재는 딥러닝 분야의 핵심 라이브러리가 됨

## 3.2 허깅페이스 허브 탐색하기
### 3.2.1 모델 허브
모델 허브에는 어떤 작업에 사용하는지, 어떤 언어로 학습된 모델인지 등 다양한 기준으로 모델이 분류되어 있음   

### 3.2.2 데이터셋 허브
모델 허브와 비슷하지만 데이터셋 크기, 데이터 유형 등이 추가로 있음   
**한국어 언어 이해 평가**(Korean Language Understanding Evaluation) - 대표적인 한국어 데이터셋 중 하나로 텍스트 분류, 기계 독해, 문장 유사도 판단 등 다양한 작업에서 모델의 성능을 평가하기 위해 개발된 벤치마크 데이터셋

### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스
스페이스는 사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능   
다양한 오픈소스 LLM과 그 성능 정보를 게시하는 리더보드가 존재   

## 3.3 허깅페이스 라이브러리 사용법 익히기
### 3.3.1 모델 활용하기
허깅페이스에서는 모델을 **바디**(body)와 **헤드**(head)로 구분   
같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위함   
라이브러리에서는 제한없이 바디만, 헤드와 함께, 헤드가 함께 있는 모델의 바디만 불러올 수도 있음   

### 3.3.2 토크나이저 활용하기
**토그나이저** - 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환   
예제 3.9   
토근화 결과 중 **token_type_ids**는 문장을 구분하는 역할   
**attention_mask**는 해당 토큰이 패딩 토큰인지 실제 데이터인지에 대한 정보

### 3.3.3 데이터셋 활용하기
예제 3.16   

## 3.4 모델 학습시키기
### 3.4.1 데이터 준비
예제 3.20   

### 3.4.2 트레이너 API를 사용해 학습하기
허깅페이스는 학습에 필요한 다양한 기능을 학습 인자(Training Arguments)만으로 쉽게 활용할 수 있는 트레이너 API를 제공   
예제 3.23   
데이터셋을 준비하고 학습 인자를 설정하는데 필요한 몇 줄의 코드만으로도 모델 학습 가능   

### 3.4.3 트레이너 API를 사용하지 않고 학습하기
예제 3.28   
Trainer를 사용하면 간편하다는 장점이 있고, 사용하지 않으면 내부 동작을 명확히 할 수 있고 직접 학습 과정을 조절할 수 있음   

### 3.4.4 학습한 모델 업로드하기   
예제 3.29   

## 3.5 모델 추론하기
### 3.5.1 파이프라인을 활용한 추론
허깅페이스는 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 pipeline을 제공   
예제 3.30   

### 3.5.2 직접 추론하기
예제 3.31
