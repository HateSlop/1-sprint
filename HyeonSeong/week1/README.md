# LLM을 활용한 실전 AI 애플리케이션 개발
------------------
# 7 모델 가볍게 만들기

## 7.1 언어 모델 추론 이해하기

### 7.1.1 언어 모델이 언어를 생성하는 방법
언어 모델이 텍스트 생성을 마치는 이유는 크게 두가지
- 다음 토큰으로 생성 종료를 의미하는 특수 토큰을 생성하는 경우
- 사용자가 최대 길이로 설정한 길이에 도달하면 더 이상 생성하지 않고 종료

그림 7.1

언어모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적(auto-regressive)인 특성을 보유
동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복적으로 수행하기 때문에 비효율적

그림 7.3

### 7.1.2 중복 연산을 줄이는 KV 캐시
KV 캐시 - 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법   

그림 7.4

KV 캐시 메모리의 계산식

KV 캐시 메모리 = 2바이트 X 2(키의 값) X (레이어 수) X (토큰 임베딩 차원) X (최대 시퀀스 길이) X (배치 크기)

처음 2는 fp16의 데이터 형식이기 때문에 2바이트
두번째 2는 키 캐시와 값 2개를 저장하기 때문에 2    
어텐션의 연산 결과는 레이어 수 만큼 생기기 때문에 레이어 수   
토큰 임베딩을 표현하는 차원의 수만큼 수를 저장하기 때문에 차원 수   
최대로 생성하려는 시퀀스 길이만큼의 메모리를 미리 확보하기 때문에 시퀀스 길이   
배치 크기에 따라 저장하는 데이터가 달라지므로 배치 크기   

### 7.1.3 GPU 구조와 최적의 배치 크기
서빙이 효율적인지 판단하는 큰 기준 3 가지
1. 비용
2. 처리량(throughput) - 시간당 처리한 요청(쿼리) 수
3. 지연 시간(latency) - 하나의 토큰을 생성하는 데 걸리는 시간   

그림 7.6
GPU는 여러 스트리밍 멀티프로세서(streaming multiprocessors)로 구성   
각각의 SM에는 연산을 수행하는 부분과 값을 저장하는 SRAM(Static Random Access Memory)가 존재   
연산을 수행하는 부분과 가까운 SRAM에는 큰 메모리를 갖기 어렵기 때문에 큰 고대역폭 메모리(High Bandwidth Memory)에 큰 데이터를 저장
추론을 수행할 때는 연산을 수행하는 시간과 메모리를 이동시키는 시간이 걸림   
모델의 이동 과정과 연산 수행 과정은 함께 진행되기 때문에 두 가지 시간이 같을 때가 최적의 배치 크기   
- 메모리 바운드: 배치 크기가 작아서 모델 파라미터를 이동시키느라 연산이 멈추는 비효율 발생
- 연산 바운드: 배치 크기가 커서 연산에 더 오랜 시간이 발생   

그림 7.9

### 7.1.4 KV 캐시 메모리 줄이기
멀티 쿼리 어텐션(multi-query attention) - 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유하는 방식   
그룹 쿼리 어텐션(grouped-query attention) - 멀티 쿼리 어텐션의 성능 감소로 인해 그보다는 키와 값의 수를 늘린 방식   

그림 7.11

키의 값의 수를 줄이면서 크게 추론 속도 향상과 KV 캐시 메모리의 감소에 대한 효과가 있음   
멀티 쿼리 어텐션의 경우 멀티 헤드 어텐션과 비교했을 때 성능 저하가 뚜렷하기 때문에 키와 값을 줄인 이후에 기존의 학습 데이터로 추가 학습을 수행   
그에 반해 그룹 쿼리 어텐션은 멀티 헤드 어텐션과 성능 차이가 적음   

## 7.2 양자화로 모델 용량 줄이기
양자화를 수행하는 시점에 따라 학습 후 양자화 / 양자화 학습으로 나눔   
LLM의 경우 학습에 많은 자원이 들기 때문에 새로운 학습이 필요한 양자화 학습보다는 학습 후 양자화를 주로 활용   
### 7.2.1 비츠앤바이츠
비츠앤바이츠 - 8비트 연산을 수행하면서도 성능 저하가 거의 없는 8비트 행렬 연산과 4비트 정규 분포 양자화 방식을 제공하는 양자화 라이브러리   
8비트 행렬 연산 - 입력값 중 크기가 큰 이상치가 포함된 열은 별도로 분리해서 16비트 그대로 계산

그림 7.15

### 7.2.2 GPTQ
GPTQ(GPT Quantization) - 모델에 입력 X를 넣었을 때와 양자화 이후의 모델에 입력 X를 넣었을 때 오차가 가장 작아지도록 모델의 양자화를 수행   

그림 7.16

흰색 열의 양자화를 수행하고 양자화를 위해 준비한 데이터를 입력한 결과가 이전과 최대한 가까워지도록 아직 양자화하지 않은 오른쪽 부분의 파라미터를 업데이트   

### 7.2.3 AWQ
AWQ(Activation-aware Weight Quantization) - 모든 파라미터가 동등하게 중요하지는 않으며 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있다는 아이디어에서 출발   
어떤 파라미터가 중요한 지 크게 두가지로 판단
1. 모델 파라미터의 값이 크다
2. 입력 데이터 활성화 값이 큰 채널

위의 기준을 통해 상위 1%에 해당하는 모델 파라미터를 찾고 해당 파라미터는 기존 모델의 데이터 타입인 FP16을 유지하고 나머지는 양자화 진행하니 성능 저하가 거의 발생하지 않음   

그림 7.20

중요한 파라미터에만 1보다 큰 스케일러 값을 곱하는 방식으로 양자화 진행   
그러나 스케일러 S가 2일 때까지는 성능이 향상되지만 2를 넘어가는 경우 성능이 다시 하락하는 사실 확인   

그림 7.21

스케일러가 큰 경우 가장 큰 수의 값이 변화하기 때문에 다른 파라미터에 영향을 주어 정보 소실이 발생   

## 7.3 지식 증류 활용하기
지식 증류(knowledge distillation) - 더 크고 성능이 높은 선생 모델(teacher model)의 생성 결과를 활용해 더 작고 성능이 낮은 학생 모델(student model)을 만드는 방법   
그림 7.22

제퍼-7B-베타 모델의 경우 개발의 지시 데이터셋의 구축과 선호 데이터셋의 구축에 모두 LLM을 사용   
그림 7.23   
사람의 리소스가 필요한 작업에 모델을 활용해 개발 속도를 높이고 자원을 아낄 수 있음   

# 8 sLLM 서빙하기
## 8.1 효율적인 배치 전략
### 8.1.1 일반 배치(정적 배치)
일반/정적 배치(naive/static batching) - 가장 기본적인 방식으로 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식   
단점
- 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 대기하는 비효율 발생   
- 생성이 일찍 종료되는 문장이 있으면 결과적으로 배치 크기가 작아지는 리소스 낭비

### 8.1.2 동적 배치
동적 배치(dynamic batching) - 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략   

그림 8.2

그러나 생성하는 토큰 길이 차이로 인해 처리하는 배치 크기가 점차 줄어들어 GPU를 비효율적으로 사용   

### 8.1.3 연속 배치
연속 배치(continuous batching) - 일반 배치와 달리 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가   

그림 8.3

사전 연산과 디코딩은 처리 방식이 다르기 때문에 처리 중인 문장과 대기 중인 문장의 비율을 지켜보고 특정 조건을 달성했을 때 추가   

## 8.2 효율적인 트랜스포머 연산
기존의 절대적 위치 인코딩의 경우 학습 데이터보다 긴 입력 데이터가 들어올 때 성능이 크게 저하되는 단점이 존재
### 8.2.1 플래시어텐션
플래시어텐션(FlashAttention) - 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발   
마스크, 소프트맧, 드롭아웃과 같이 큰 메모리를 사용하는 연산이 오래 걸리는 이유는 GPU에서 메모리를 읽고 쓰는 데 오랜 시간이 걸리기 때문   
SRAM이 빠르지만 메모리의 크기가 작기 때문에 대부분의 읽기 쓰기 작업은 HBM에서 진행   
블록 단위로 어텐션 연산을 수행하고 전체 어텐션 행렬을 쓰거나 읽기 않는 방식을 도입   
이를 활용하면 HBM이 아닌 SRAM에 데이터를 읽고 쓰면서 더 빠르게 연산 수행 가능   

그림 8.7

연산량이 증가하지만 메모리를 읽고 쓰는 양이 크게 줄어들면서 실행시간은 오히려 1/5정도로 감소   

표 8.1

### 8.2.2 플래시어텐션 2
플래시어텐션2는 플래시어텐션에 비해 크게 두 가지를 개선해 2배 정도 속도를 향상함
1. 행렬 곱셈이 아닌 연산 줄이기
2. 시퀀스 길이 방향의 병렬화 추가

그림 8.14

GPU를 효율적으로 활용하기 위해서는 충분한 수의 스레드 블록이 있어야 하는데 아래 그림과 같이 스퀀스 길이 방향으로 여러 개의 묶음으로 나눠 사용하는 스레드 블록 수를 늘리는 식으로 작동   

그림 8.15

### 8.2.3 상대적 위치 인코딩
최초의 트랜스포머 아키텍처에서는 토큰의 위치에 따라 사인과 코사인 수식으로 정해진 값을 더해줌   
이렇게 사인과 코사인을 사용해 위치 인코딩을 더하는 방식을 사인파(sinusoidal)위치 인코딩이라고 부름   
그러나 학습 데이터보다 더 긴 입력이 들어오면 언어 모델의 생성 품질이 빠르게 떨어진다는 한계가 있음   
이를 극복하기 위해 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 상대적 위치 인코딩(relative positional encoding) 방식이 활발히 연구됨   
RoPE(Rotary Positional Encoding) - 각각의 토큰 임베딩을 토큰 위치에 따라 회전시키는 방식   
토큰 사이의 위치 정보가 두 임베딩 사이의 각도를 통해 모델에 반영

그림 8.18

ALiBi(Attention with Linear Biases) - 쿼리와 키 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식   
간단한 인코딩 방식을 사용하기 떄문에 학습과 추론에도 별도로 처리 시간을 추가하지 않음   

그림 8.21

## 8.3 효율적인 추론 전략
### 8.3.1 커널 퓨전
GPU에서 연산은 커널 단위로 이루어지는데 커널마다 고대역폭 메모리에서 데이터를 읽어오고 연산 결과를 쓰는 오버헤드가 발생   
커널 퓨전(kernel fusion) - 연산을 하나로 묶어 오버헤드를 줄이는 방식   

그림 8.22

### 8.3.2 페이지어텐션
기존의 KV 캐시는 앞으로 사용할 수도 있는 메모리를 미리 잡아두면서 GPU 메모리를 많이 낭비한다는 문제 발생   

그림 8.25

'연속적인 물리 메모리'를 사용하기 위해 미리 메모리를 준비하기 때문에 발생   
페이지어텐션 - 운영체제의 가상 메모리 개념을 빌려와 중간에서 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 관리   

그림 8.26

다양한 디코딩 방식에서 동일한 입력의 프롬프트에 대한 메모리를 공유하는 방식인 병렬 샘플링(parallel sampling)을 활용해서 메모리를 절약   
다른 토큰을 생성하기 때문에 이를 분리하기 위해서 참조 카운트(reference count)라는 개념을 활용   
참조 카운트 - 물리적 블록을 공유하고 있는 논리적 블록 수를 의미   

### 8.3.3 추측 디코딩
추측 디코딩(speculative decoding) - 쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식   
작은 드래프트 모델(draft model)과 큰 타깃 모델(target model)이라는 2개의 모델을 활용   
먼저 드래프트 모델이 토큰을 생성하고 타깃 모델은 생성했을 결과와 동일한지 계산해 승인하거나 비승인하는 방식

그림 8.28

2개의 모델을 사용하기 때문에 시스템 복잡도가 올라가지만 이를 해결하기 위해 하나의 원본 모델 내에서 여러 토큰을 예측하는 메두사(Medusa)와 같은 방식도 존재   

## 8.4 실습: LLM 서빙 프레임워크
- 오프라인 추론: 대량의 입력 데이터에 대해 추론을 수행해 충분히 큰 배치 크기를 활용할 수 있는 추론
- 온라인 추론: 사용자의 요청에 따라 모델 추론을 수행하는 방식   
### 8.4.1 오프라인 서빙
예제 8.5

표 8.3
### 8.4.2 온라인 서빙
예제 8.10

# 9 LLM 애플리케이션 개발하기
## 9.1 검색 증강 생성(RAG)
LLM의 답변은 근거나 출처가 불명확하고 부정확한 정보를 지어내는 환각 현상이 존재   
RAG(Retrieval Augmented Generation) - 질문이나 요청만 전달하고 생성하는 것이 아니라 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법   

그림 9.2

LLM 오케스트레이션 도구 - 인터페이스, 임베딩 모델, 벡터 데이터베이스 등 LLM 애플리케이션을 위한 다양한 구성요소를 연결하는 프레임워크   
### 9.1.1 데이터 저장
데이터 소스 - 텍스트, 이미지와 같은 비정형 데이터가 저장된 데이터 저장소   
임베딩 모델 - 비정형 데이터를 입력했을 때 그 의미를 담은 임베딩 벡터로 변환하는 모델   
벡터 데이터베이스 - 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능 제공   

그림 9.5

### 9.1.2 프롬프트에 검색 결과 통합
LLM은 결과를 생성할 때 프롬프트만 입력으로 받기 때문에 사용자의 요청과 관련이 큰 문서를 벡터 데이터베이스에서 찾고 검색 결과를 프롬프트에 통합해야 함   
결과의 정확도를 위해 매번 질문에 관련된 정보를 수동으로 찾아 입력해 줄 수는 없기 때문에 프로그래밍 방식으로 관련된 정보를 찾아 프롬프트에 넣을 수 있어야 함   

그림 9.9

### 9.1.3 실습: 라마인덱스로 RAG 구현하기
대표적인 LLM 오케스트레이션 라이브러리인 라마인덱스를 사용해 진행   

예제 9.4

다양한 구성요소와 과정이 필요하지만, 라마인덱스를 사용하면 단 두 줄의 코드만으로 유사한 텍스트를 검색하고 생성하는 과정을 전부 수행 가능   

예제 9.5

## 9.2 LLM 캐시
LLM 캐시는 추론을 수행할 때 사용자의 요청과 생성 결과를 기록하고 이후에 동일하거나 비슷한 요청이 들어오면 새롭게 텍스트를 생성하지 않고 이전의 생성 결과를 가져와 바로 응답   

### 9.2.1 LLM 캐시 작동 원리
LLM 캐시는 프롬프트 통합과 LLM 생성 사이에 위치하여 캐시 요청을 통해 이전에 동일하거나 유사한 요청이 있었는지 확인   
크게 두 가지 방식으로 나눌 수 있음   

1. 일치 캐시(exact cash) - 요청이 완전히 일치하는 경우 저장된 응답을 반환
2. 유사 검색 캐시(similar search) - 문자열 그대로가 아닌 문자열의 임베딩 벡터를 비교하여 유사한 요청이 있었는지 확인   

### 9.2.2 실습: OpenAI API 캐시 구현
파이썬 딕셔너리와 오픈소스 벡터 데이터베이스 크로마(Chroma)를 사용해 기능 구현   

예제 9.8

입력받은 prompt가 self.cache에 없다면 새롭게 저장하고 동일한 프롬프트가 있다면 저장된 응답을 반환   

## 9.3 데이터 검증
### 9.3.1 데이터 검증 방식
데이터 검증 - 벡터 검색 결과나 LLM 생성 결과에 포함되지 않아야 하는 데이터를 필터링하고 답변을 피해야 하는 요청을 선별함으로써 새롭게 생성된 텍스트로 인해 생길 수 있는 문제를 줄이는 방법   
일종의 가이드라인으로 사용할 수 있는 방법은 크게 네 가지

1. 규칙 기반 - 문자열 매칭이나 정규 표현식을 활용해 데이터를 확인   
2. 분류 또는 회귀 모델 - 명확한 문자열 패턴이 없는 경우 별도의 모델(긍부정 분류 모델)을 만들어 활용   
3. 임베딩 유사도 기반 - 민감한 임베딩 벡터를 만들어 이와 관련된 답변을 피하도록 만들 수 있음   
4. LLM 활용 - LLM을 활용해 응답이 적절하지 않은 경우 이를 다시 생성하거나 삭제   

### 9.3.2 데이터 검증 실습
엔비디아에서 개발한 NeMo-Guardrails 라이브러리를 활용해 특정 주제에 대한 답변을 피하는 기능 구현   

예제 9.13

## 9.4 데이터 로깅
데이터 로깅 - 사용자의 입력과 LLM이 생성한 출력을 기록   
LLM의 경우 입력이 동일해도 출력이 달라질 수 있기 때문에 어떤 입력에서 어떤 출력을 반환했는지 반드시 기록해야 함   
대표적인 로깅 도구 중 하나인 W&B(Weight adn Bias)에서 제공하는 Trace 기능을 활용하면 요청과 응답을 기록할 수 있음   

### 9.4.1 OpenAI API 로깅

예제 9.16

### 9.4.2 라마인덱스 로깅

예제 9.17

# 10 임베딩 모델로 데이터 의미 압축하기
## 10.1 텍스트 임베딩 이해하기
텍스트 임베딩 / 문장 임베딩 - 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식   
### 10.1.1 문장 임베딩 방식의 장점
문장 임베딩 방식을 사용하면 서로 다른 텍스트를 마치 사람이 이해하는 것처럼 서로 유사한지, 관련이 있는지 판단할 수 있다는 장점이 있음   

### 10.1.2 원핫 인코딩
데이터를 그대로 숫자로 변환한다고 하면 숫자를 비교하게 되어 오해가 생길 수 있기 때문에 원핫 인코딩(one-hot encoding)이 표현되었음   
원핫 인코딩 - 숫자를 0,1의 행렬로 표시하여 데이터 사이의 의도하지 않은 관계가 담기는 것을 방지   
그러나 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 치명적인 단점이 있음   

### 10.1.3 백오브워즈
백오브워즈(Bag of Words) - '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환   

표 10.1   

그러나 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는 데 크게 도움이 되지 않는 경우가 존재 (조사, 접속사)   

### 10.1.4 TF-IDF
TF-IDF(Term Frequency-Inverse Document Frequency) - 앞의 문제를 보완하기 위해 다음 수식을 활용해 많은 문서에 등장하는 단어의 중요도를 작게 만듦   

표 10.2

그러나 이러한 경우 대부분의 수가 0인 벡터가 되는데 이를 '희소(sparse)'하다고 함   
희소한 벡터는 의미를 '압축'해서 담고 있지 못하기 때문에 벡터 사이의 관계를 활용하기 어려워 이를 대비한 것을 밀집 임베딩(dense embedding)이라고 부름   

### 10.1.5 워드투백
워드투백(word2vec) - 단어가 '함께 등장하는 빈도'정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법   
1. CBOW(Continuous Bag of Words) - 주변 단어로 가운데 단어를 예측하는 방식
2. 스킵그램(skip-gram) - 중간 단어로 주변 단어를 예측하는 방식   

그림 10.1

이 경우 유사한 벡터가 거리와 방향이 나오기 때문에 단어와 단어 사이의 관계나 의미에 대해 확인할 수 있음   

## 10.2 문장 임베딩 방식
### 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법
바이 인코더            