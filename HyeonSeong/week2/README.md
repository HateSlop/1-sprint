# LLM을 활용한 실전 AI 애플리케이션 개발
------------------
# 12 벡터 데이터베이스로 확장하기: RAG 구현하기
## 12.1 벡터 데이터베이스란
**벡터 데이터베이스** - 벡터 임베딩(데이터의 의미를 담은 숫자 배열)을 키로 사용하는 데이터베이스   

### 12.1.1 딥러닝과 벡터 데이터베이스
**표현 학습** - 기존의 머신러닝에서는 매번 새롭게 특징을 정의하는 단계가 필요했지만 딥러닝에서는 데이터만 충분하다면 모델이 알아서 이런 특징을 뽑는 과정도 학습   

그림 12.4   

비슷한 데이터는 가깝게 있고, 다른 데이터는 멀리 위치하게 되는 임베딩 벡터의 특징을 이용해 서로 비슷한 데이터를 찾을 수 있음   
벡터 데이터 베이스를 활용하기 위한 3단계   
1. 저장: 저장할 데이터를 임베딩 모델을 거쳐 벡터로 변환하고 벡터 데이터베이스에 저장
2. 검색: 검색할 데이터를 임베딩 모델을 거쳐 벡터로 변환하고 벡터 데이터베이스에서 검색
3. 결과 반환: 벡터 데이터베이스에서는 검색 쿼리의 임베딩과 거리가 가까운 벡터를 찾아 반환

벡터 사이의 거리를 측정하는 다양한 방법이 있는데 일반적으로 유클리드 거리, 코사인 유사도, 점곱(dot product)을 가장 많이 활용   
### 12.1.2 벡터 데이터베이스 지형 파악하기
벡터 임베딩을 저장하고 검색하는 기능을 구현할 때 크게 아래와 같은 소프트웨어를 접함   
1. **벡터 라이브러리** - 메타의 Faiss, 스포티파이의 Annoy 와 같이 벡터를 저장하고 검색하는 핵심 기능을 구현   
2. **벡터 전용 데이터베이스** - 파인콘(Pinecone), 위비에이트(Weaviate)
3. **벡터 기능 추가 데이터베이스** - 일래스틱서치(Elasticsearch), PostgreSQL과 같이 기존의 데이터베이스에 벡터 저장과 검색 기능을 추가   
벡터 데이터베이스는 벡터 라이브러리와 다르게 아래와 같은 기능을 제공   
1. 메타 데이터의 저장 및 필터링 가능
2. 데이터의 백업 및 관리
3. 모니터링, 관련 AI 도구 등 에코시스템과의 통합
4. 데이터 보안과 엑세스 관리   

그림 12.7

고급 벡터 검색이 필요하고 워크로드가 큰 경우 그림 왼쪽의 벡터 전용 데이터베이스를 선택하는 것이 좋음   
벡터 데이터베이스에 대한 이해도가 있고 직접 오픈소스 서비스를 활용해 시스템을 구축할 수 있고 선호한다면 그림 왼쪽 위의 오픈소스 벡터 데이터베이스가 좋음   

## 12.2 벡터 데이터베이스 작동 원리
### 12.2.1 KNN 검색과 그 한계
**KNN(K-Nearest Neighbor)검색** - 검색하려는 벡터와 가장 가까운 K개 이웃 벡터를 찾는 검색 방식   
모든 데이터를 조사하기 때문에 정확하지만 모든 벡터를 조사하기 때문에 연산량이 데이터 수에 비례하게 늘어남   
벡터 검색을 위해서는 먼저 **인덱스**(관계형 데이터베이스의 테이블과 비슷한 레벨)를 만들어야 함   
인덱스를 벡터에 저장하는데, 이렇게 벡터를 저장하는 과정을 "**색인한다**"라고 함   
색인 단계에서는 인덱스의 메모리 사용량과 색인 시간이 중요하고 검색 단계에서는 검색 시간과 재현율이 중요   
**재현율** - 실제로 가장 가까운 K개의 정답 데이터 중 몇 개가 검색 결과로 반환됐는지 그 비율을 나타낸 값   
KKN 검색의 경우 재현율이 100%   

그림 12.8   

### 12.2.2 ANN 검색이란
**근사 최근접 이웃(Approximate Nearest Neighbor)검색** - 대용량 데이터셋에서 주어진 쿼리 항목과 가장 유사한 항목을 효율적으로 찾는 데 사용되는 기술   
대표적인 ANN 알고리즘   
1. **IVF(Inverted File Index)** - 검색 공간을 제한하기 위해 데이터셋 벡터들을 클러스터로 그룹화
2. **HNSW(Hierarchical Navigable Small World)** - 효율적인 ANN 검색을 위한 그래프 기반 인덱싱 구조   
HNSW가 가장 많이 활용되는 ANN 검색 알고리즘   
```
ANN 검색의 재현율 = (KNN으로 찾은 실제 가장 가까운 K개 중 ANN이 찾은 개수) / K
```

### 12.2.3 탐색 가능한 작은 세계(NSW)
HNSW의 그래프는 노드(node)와 간선(edge)로 이루어짐   
1. 노드 - 저장하는 데이터를 의미하고 벡터 데이터베이스에서는 벡터 임베딩이 노드
2. 간선 - 노드와 노드를 연결하는 선으로, 간선을 통해 서로 연결된 노드끼리만 탐색이 가능   

그림 12.9   

탐색 가능한 작은 세계 - 완전히 랜덤한 그래프와 완전히 규칙적인 그래프 사이에 '적당히 랜덤하게' 연결된 그래프 상태   
규칙적인 연결을 통해 정확한 탐색이 가능하면서도 랜덤한 성질을 통해 빠른 탐색이 가능   

그림 12.10   

하지만 랜덤으로 저장하다 보니 아래 그림과 같이 진입점에서 출발했을 때 찾으려는 검색 벡터(Q)와 가장 가까운 점(E)이 아닌 점 A에서 탐색을 멈추는 **지역 최솟값(local mininum)** 문제 발생   

그림 12.13   

### 12.2.4 계층 구조
연결 리스트(linked list) - 새로운 데이터를 추가하거나 삭제할 때 서로를 연결하는 주소 정보를 추가하거나 삭제하면 되기 때문에 데이터 추가/삭제가 자유롭지만 탐색을 할 때는 앞에서부터 순차적으로 확인해야 하기 때문에 탐색 속도가 느림   

그림 12.14   

아래 그림과 같이 데이터가 크기 순으로 정렬되어 있다면 레벨을 나누어 데이터를 듬성듬성 배치하고 탐색은 가장 위층부터 시작   

그림 12.15   

HNSW는 이런 계층 구조를 NSW에 접목해 벡터를 저장하는데 아래와 같은 기준으로 벡터를 저장   
1. 최대가 6인 주사위를 굴려서 6이 나오면 0,1,2층 모두 배치   
2. 주사위를 굴려서 4~5가 나오면 0,1층에 배치
3. 주사위를 굴려서 1,2,3이 나오면 0층에만 배치   

그림 12.16   

## 12.3 실습: HNSW 인덱스의 핵심 파라미터 이해하기
### 12.3.1 파라미터 m 이해하기
HNSW에서 파라미터 m은 추가하는 임베딩 벡터에 연결하는 간선의 수   
벡터에 연결되는 간선이 많을수록 그래프가 더 촘촘하게 연결되기 때문에 검색의 품질이 좋음   

예제 12.4   

표 12.2   

### 12.3.2 파라미터 ef_construction 이해하기
ef_construction은 M개의 가장 가까운 벡터를 선택할 후보군의 크기로, 이 값이 크면 더 많은 후보를 탐색하기 때문에 실제로 추가한 벡터와 가장 가까운 벡터를 선택할 가능성이 높음   

예제 12.5   

표 12.3   

### 12.3.3 파라미터 ef_search 이해하기
ef_search는 ef_construction이 색인 단계에서 후보군의 크기를 결정한 것과 동일하게 검색 단계에서 후보군의 크기를 결정   

예제 12.6   

표 12.4   

## 12.4 실습: 파인콘으로 벡터 검색 구현하기
### 12.4.1 파인콘 클라이언트 사용법
예제 12.8   
파인콘 인덱스에 저장할 수 있도록 tolist() 메서드를 사용해 형태를 변경   
예제 12.9   
예제 12.11   

### 12.4.2 라마인덱스에서 벡터 데이터베이스 변경하기
예제 12.13   

## 12.5 실습: 파인콘을 활용해 멀티 모달 검색 구현하기
### 12.5.1 데이터셋
예제 12.14   

### 12.5.2 실습 흐름
1. 원본 이미지와 세 가지 프롬프트로 생성한 3개의 합성 이미지를 비교   
2. 원본 이미지에 대응되는 '원본 프롬프트'를 입력   
3. 전체 프롬프트 텍스트를 텍스트 임베딩 모델로 저장한 벡터 데이터베이스에 원본 이미지를 이미지 임베딩 모델로 변환한 이미지 임베딩으로 검색해 찾은 '유사 프롬프트'를 사용해 이미지를 생성   

그림 12.19   

### 12.5.3 GPT-4o로 이미지 설명 생성하기
예제 12.15   

### 12.5.4 프롬프트 저장
아래 예제를 통해 프롬프트 임베딩을 저장하고 이미지 임베딩으로 검색할 인덱스를 생성   
예제 12.18   
아래 예제를 사용해 생성한 임베딩 벡터를 벡터 데이터베이스에 저장   
예제 12.20   

### 12.5.5 이미지 임베딩 검색
예제 12.21   

### 12.5.6 DALL-E 3로 이미지 생성
아래의 코드를 사용해 3개의 프롬프트에 대한 이미지를 생성
1. GPT-4o가 원본 이미지를 설명해서 작성한 GPT 설명 프롬프트로 이미지 생성
2. 원본 프롬프트를 사용해 이미지 생성
3. 이미지 임베딩으로 검색한 유사 프롬프트를 사용해 이미지 생성   

예제 12.24

출력결과는 아래와 같이 이미지 생성 파라미터에 따라 생성 결과가 달라지고 랜덤성이 있음   

그림 12.20   

# 13 LLM 운영하기
## 13.1 MLOps
MLOps(Machine Learning Operations) - 데브옵스(DevOps)의 개념을 머신러닝과 데이터 과학 분야로 확장한 방법론으로 데이터 수집, 전처리, 모델 학습, 평가, 배포, 모니터링 등 머신러닝 프로젝트의 전 과정을 자동화하고 효율화하는 것   

그림 13.1

MLOps는 특히 이전에 수행된 ML 워크플로를 그대로 반복했을 때 동일한 모델을 얻을 수 있는지 여부를 의미하는 재현성(reproducibility)를 보장하는 것이 매우 중요   

그림 13.2

### 13.1.1 데이터 관리
모델 학습을 위한 데이터 준비 과정에는 여러가지 중요한 의사결정이 포함되고 그 의사 결정에 따라 다양한 형태의 데이터셋이 생성   
포함시킬 데이터의 범위를 선택하고 어떤 전처리 방식을 포함시킬지, 특성 공학을 통해 어떤 특성을 추가할지에 따라 학습 데이터셋이 달라짐   

그림 13.3   

모델 학습 결과를 재현하기 위해서는 데이터셋의 버전을 관리하고 어떤 학습 데이터셋으로 모델을 학습시켰는지 기록해야 함   

### 13.1.2 실험 관리
머신러닝 모델을 학습시킬 때는 어떤 모델을 사용할지 결정해야 함   

그림 13.4   

### 13.1.3 모델 저장소
MLOps에서 모델 저장소(model registry)는 머신러닝 모델을 체계적으로 관리하고 버전 제어하는 데 필수   
여러 머신러닝 파이프라인에서 다양한 실험을 통해 여러 버전의 모델이 생성되는데, 이를 활용하면 다양한 모델을 통합해서 관리할 수 있음   

그림 13.5   

### 13.1.4 모델 모니터링
머신러닝 모델은 학습 데이터를 통해 학습한 패턴을 바탕으로 예측을 수행하기 때문에 정상적으로 요청에 응답하고 있더라도 엉뚱한 값을 반환한 것은 아닌지 확인   

## 13.2 LLMOps는 무엇이 다를까?
### 13.2.1 상업용 모델과 오픈소스 모델 선택하기
LLMOps에서는 MLOps보다 훨씬 크고 다양한 일을 할 수 있는 모델을 다룬다는 점에서 큰 차이   

표 13.1   

### 13.2.2 모델 최적화 방법의 변화
LLMOps에서 다루는 LLM은 모델의 크기가 크기 때문에 일반적으로 사전 학습시키는 경우는 거의 없음   
오픈소스 모델을 선택했다면 미세 조정을 자유롭게 수행할 수 있지만, 상업용 모델을 선택했다면 미세 조정 기능을 지원하는 모델만 제한적으로 미세 조정할 수 있음   

표 13.2   

LLMOps에서 다루는 언어 모델은 모델의 크기가 크고 처음부터 학습시킬 때 들어가는 계산량이 크기 때문에 일반적으로 사전 학습하지 않고 사전 학습된 모델을 가져와 미세 조정하는 전이 학습을 기본으로 사용   
모델 개발 과정에서 학습할 때 설정한 하이퍼파라미터를 기록해 두고 이후 동일한 성능의 모델을 다시 만들 수 있도록 관리   

### 13.2.3 LLM 평가의 어려움
LLM은 다양한 작업이 가능하기 때문에 특정 작업의 성능 평가 방식으로 모두 평가할 수 없고 프롬프트에 따라 성능이 달라지기도 해서 명확한 기준을 잡기 어려움   

## 13.3 LLM 평가하기
### 13.3.1 정량적 지표
텍스트 생성 작업을 평가할 때 사용할 수 있는 대표적인 세 가지 정량 지표   
1. BLEU(Bilingual Evaluation Understudy Score) - 기계 번역 결과와 사람이 번역한 결과의 유사도를 측정하여 평가   
2. ROUGE(Recall-Oriented Understudy for Gisting Evaluation)- 모델이 생성한 요약문과 사람이 작성한 참조 요약문 사이의 n그램 중복도를 재현율 관점에서 측정   
3. 펄플렉시티(Perplexity) - 모델이 새로운 단어를 생성할 때의 불확실성을 수치화한 것으로, 값이 낮을수록 모델의 예측 성능이 우수하다는 의미   

세 가지 정량 지표 모두 빠르게 언어 모델의 성능을 평가할 수 있다는 장점이 있지만 문장의 의미, 문법, 유창성 등 질적인 측면의 평가에는 한계가 있고 실제 사람의 주관적 판단과 불일치하는 경우가 많음   

### 13.3.2 벤치마크 데이터셋을 활용한 평가
벤치마크 데이터셋 - 다양한 모델의 성능을 비교하기 위해 공통으로 사용하는 데이터셋, 대표적으로 (ARC, HellaSwag, MMLU 등)   

표 13.3   

W&B에서 새로운 한국어 LLM 리더보드인 호랑이(Horangi)가 공개   
문장의 생성 확률이 아니라 실제로 생성한 텍스트 결과가 A,B,C,D와 같이 정답과 일치하는지를 비교해 성능을 평가   

### 13.3.3 사람이 직접 평가하는 방식   
사람이 직접 평가하는 방식은 언어의 유창성과 같이 정량적인 지표로 평가하기 어려운 사항을 평가할 수 있다는 장점이 있지만 시간이 오래 걸리고 비용이 많이 든다는 단점이 있음   

### 13.3.4 LLM을 통한 평가
표 13.4   
첫 번째 턴에서 하나의 요청을 하고 응답 이후에 다시 두 번째 요청을 함   
어러 턴에 걸쳐 LLM이 사용자의 요구사항에 맞춰 대응하는지 확인하기 위해서임   

사람과 LLM의 평가가 80% 이상 일치했기 때문에 사람이 직접 평가할 때 드는 시간과 비용을 생각하면 LLM을 활용해 비교적 적은 비용으로 빠르고 정확하게 평가를 수행해서 사람이 직접 평가하는 양을 줄일 수 있음   

### 13.3.5 RAG 평가
그림 13.9   
1. 신뢰성(faithfulness) - 생성된 응답이 검색된 맥락 데이터에 얼마나 사실적으로 부합하는 지 평가   
2. 답변 관련성(answer relevancy) - 생성된 답변이 요청과 얼마나 관련성 있는지 평가
3. 맥락 관련성(context relevancy) - 검색 결과인 맥락 데이터가 요청과 얼마나 관련 있는지 평가   

# 14 멀티 모달 LLM
## 14.1 멀티 모달 LLM이란
멀티 모달 LLM - 텍스트 뿐만 아니라 이미지, 비디오, 오디오, 3D 등 다양한 형식의 데이터를 이해하고 생성할 수 있는 LLM   
### 14.1.1 멀티 모달 LLM의 구성요소
멀티 모달 LLM은 일반적으로 다섯 가지 구성요소로 이뤄짐   
1. LLM은 뛰어난 이해 능력과 추론 능력을 갖고 있기 때문에 이미지 형식의 데이터를 모달리티 인코더(modality encoder)와 입력 프로젝터(input projector)를 통해 텍스트로 변환해 입력
2. LLM의 출력은 기본적으로 텍스트인데, 출력 프로젝터(output projector)를 통해 이미지 형태의 데이터 출력이 필요한지 판단   
3. 모달리티 생성기(modality generator)를 통해 특정 데이터 형식의 출력을 생성   

그림 14.2   

요소들의 오른쪽 아래에 있는 그림들은 학습 과정에서의 파라미터 업데이트 여부를 나타냄   

모달리티 인코더 - 이미지, 비디오, 오디오 같이 텍스트 이외의 데이터 형식을 처리하기 위해 학습된 사전 학습 모델   
비전 트랜스포머(vision transformer) - 텍스트를 처리하기 위해 개발된 트랜스포머 아키텍처를 이미지에 적용한 모델   

그림 14.3   

이미지를 패치 단위로 자른 후 마치 텍스트에서 단어를 처리하는 것과 같이 일렬로 나열해 입력해 처리   
모달리티 인코더가 이미지 데이터를 처리해서 이미지 임베딩으로 변환했다면 입력 프로젝터는 이미지 임베딩을 LLM 백본이 이해할 수 있는 텍스트로 변환   
이미지를 생성하기 위해서는 크게 이미지 생성이 필요한지 판단하는 단계와 만약 필요하다면 어떤 이미지를 생성할지 정하는 단계로 나눌 수 있음   

### 14.1.2 멀티 모달 LLM 학습 과정
LLM과 마찬가지로 사전 학습과 지시 데이터셋을 활용한 지시 학습(instruction tuning)으로 나눔   
사전 학습 단계에서는 멀티 모달 데이터에 대한 전체적인 이해력을 습득하고 높이는 데 집중   
사전 학습이 끝난 후에는 멀티 모달 지시 튜닝 단계를 진행   
지시 - 모델이 이미지 캡션을 생성하거나 입력 이미지에 대한 질문 응답 같은 특정 멀티 모달 작업을 수행하도록 학습 시키는 것을 말함   

## 14.2 이미지와 텍스트를 연결하는 모델: CLIP
### 14.2.1 CLIP 모델이란
CLIP(Contrastive Language_Image Pre-training) - OpenAI에서 개발한 모델로 인터넷상에서 수집한 이미지와 캡션 데이터를 활용하여 이미지와 텍스트를 같은 벡터 공간에 임베딩하도록 만들어짐    

### 14.2.2 CLIP 모델의 학습 방법
CLIP은 이미지와 이미지에 대한 설명이 대응된 데이터인 이미지-텍스트 쌍을 활용   

그림 14.4   

유사한 데이터 쌍은 더 가까워지도록 하고 유사하지 않은 데이터 쌍은 더 멀어지도록 학습시키는 대조 학습을 통해 모델을 학습   

그림 14.5  

### 14.2.3 CLIP 모델의 활용과 뛰어난 성능
제로샷 추론(Zero-shot prediction) - 사전 학습 데이터 이외에 특정 작업을 위한 데이터로 미세 조정하지 않은 상태에서 추론을 수행하는 것   

그림 14.6   

CLRP 모델은 이미지와 텍스트 데이터 사이의 유사도 계산을 활용해 이미지 검색에도 활용할 수 있음   
이미지 검색 - 이미지와 텍스트의 유사도 기반으로 텍스트를 입력했을 때 유사한 이미지를 찾는 기능   

### 14.2.4 CLIP 모델 직접 활용하기
먼저 입력 데이터의 전처리를 담당하는 프로세서와 이미지와 텍스트 임베딩 모델을 가져옴   

예제 14.2   

## 14.3 텍스트로 이미지를 생성하는 모델: DALL-E
### 14.3.1 디퓨전 모델 원리
디퓨전 모델 - 물질이 농도가 높은 곳에서 낮은 곳으로 이동하는 현상인 확산 현상에서 영감을 받아 만들어진 생성 모델   

그림 14.10   

디퓨전 모델은 이미지에서 어떤 부분이 노이즈인지 예측하는 방식으로 학습하는데, 그 능력을 사용해 완전한 노이즈 상태의 이미지에서 노이즈를 예측하고 예측된 노이즈를 제거하면서 점차 완전한 노이즈에서 의미가 있는 이미지를 생성   

그림 14.11   

인코더 디코더 - 입력 데이터의 차원을 낮추는 인코딩 단계와 차원을 높이는 디코딩 단계를 통해 데이터의 의미를 압축하기 위해 사용되는 모델 구조   
U-Net - 인코더-디코더 구조를 변형해 인코딩 단계의 고차원 정보를 디코딩 단계에도 활용함으로써 이미지의 위치 정보가 손실되는 것을 막을 수 있다는 장점이 있음   

그림 14.12   

사람이 원하는 이미지를 생성하기 위해서는 디퓨전 모델에 노이즈를 넣어주면서 원하는 결과물의 형태를 텍스트 임베딩으로 변환해 디퓨전 모델에 함께 입력으로 넣어줌   

그림 14.14   

### 14.3.2 DALL-E 모델
DALL-E 모델은 앞서 살펴본 CLIP 모델을 활용해 텍스트 임베딩을 만들고 텍스트 임베딩을 활용해 두 단계를 거쳐 이미지를 생성   

그림 14.15   

프라이어(prior) 모델 - 텍스트 임베딩을 입력으로 받아 이미지 임베딩을 예측하는 디퓨전 모델   

그림 14.16   

디코더 - 이미지 임베딩을 참조해 이미지를 생성하는 디퓨전 모델   
프라이어와 디코더 모두 디퓨전 모델 구조를 사용하지만 학습되는 데이터와 입력 및 출력의 차원이 다른 모델   

그림 14.17   

굳이 프라이어를 사용해야 할까?라는 의문이 들 수 있지만 결과적으로 프라이어와 디코더를 모두 사용했을 때 원하는 결과를 얻을 확률이 높아짐   

그림 14.18   

## 14.4 LLaVA
### 14.4.1 LLaVA의 학습 데이터
LLaVA(Large Language and Visual Assistant) 모델 - 이미지를 인식하는 CLIP 모델과 LLM을 결합해 모델이 이미지를 인식하고 그 이미지에 대한 텍스트를 생성할 수 있는 모델   
데이터셋의 부족을 해결하기 위해서 GPT-4를 활용해 데이터셋을 생성   
텍스트만을 입력을 받기 때문에 이미지에 대한 설명과 위치 정보(Bounding Box)를 통해 이미지를 인식하도록 함   

그림 14.20   

GPT-4가 입력된 이미지 설명을 보고 다음 세 가지 유형의 텍스트를 생성하도록 함   

1. 대화: 사람이 이미지에 질문했을 때 어시스턴트(Assistant)가 이미지를 보고 답변하는 형식의 데이터   
2. 자세한 설명: GPT-4가 이미지 설명을 읽고 이미지에 대해 자세히 설명하도록 함   
3. 복합한 추론: 위의 두 유형이 단순히 이미지에 대한 인식과 설명이었다면, 답변을 위해 단계별 추론이 필요한 어려운 질문을 생성하고 답변하도록 함   

### 14.4.2 LLaVA 모델 구조
입력 이미지를 CLIP의 이미지 인코더를 통해 이미지 임베딩으로 만들고 간단한 선형 층을 통과해 LLM에 입력할 임베딩 토큰으로 만든 후 텍스트 지시사항은 토큰 임베딩으로 변환해 함께 입력으로 넣고 결과 생성   

그림 14.21   

### 14.4.3 LLaVA 1.5
한 층의 선형 층으로 이미지 임베딩을 토큰 임베딩으로 변환하던 구조를 2층의 MLP(Multi-Layer Perception)로 변경하는 간단한 수정만으로 성능을 대폭 끌어올림   

그림 14.22   

그림 14.23   

### 14.4.4 LLaVA NeXT
2024년 1월 30일에 발표된 LLaVA NeXT는 아래와 같은 사항들이 변경   

1. 기존 모델 대비 입력 이미지의 해상도가 4배 높아짐
2. 고품질의 지시 데이터셋을 구축해 시각적 추론 능력과 OCR 성능이 개선
3. 더 많은 시나리오에서 응답할 수 있어 다양한 애플리케이션에 활용
4. SGLangn 프레임워크를 사용해 추론 성능 향상   

표 14.1   

