{"cells":[{"cell_type":"markdown","metadata":{"id":"mr7FmYqAi6y2"},"source":["## 토큰화 코드(단어 단위)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3K1on7SMnXj","outputId":"fc6bd7ed-0d3a-426d-b2f1-7f3c399c894f","executionInfo":{"status":"ok","timestamp":1725572949638,"user_tz":-540,"elapsed":454,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n","str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n","idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n","input_ids:  [0, 1, 2, 3, 4]\n"]}],"source":["# 띄어쓰기 단위로 분리\n","input_text = \"나는 최근 파리 여행을 다녀왔다\"\n","input_text_list = input_text.split()\n","print(\"input_text_list: \", input_text_list)\n","\n","# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n","str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n","idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n","print(\"str2idx: \", str2idx)\n","print(\"idx2str: \", idx2str)\n","\n","# 토큰을 토큰 아이디로 변환\n","input_ids = [str2idx[word] for word in input_text_list]\n","print(\"input_ids: \", input_ids)"]},{"cell_type":"markdown","metadata":{"id":"CX95psBGjELL"},"source":["## 토큰 아이디에서 벡터로 변환"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCKTtOt9NvFA","outputId":"435036aa-cbc7-4795-a8b1-02f196f5c291","executionInfo":{"status":"ok","timestamp":1725572952331,"user_tz":-540,"elapsed":472,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5, 16])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","embedding_dim = 16 # 토큰 하나를 16차원의 임베딩으로 변환\n","\n","# 사전 크기가 len(str2idx)이고 embedding_dim 차원의 임베딩을 생성하는 layer\n","# nn.Embedding: 입력으로 주어진 정수 인덱스를 고정된 차원의 벡터로 변환하는 역할\n","embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n","\n","input_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n","input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16) 차원 추가: 한 개의 문장, 다섯 개의 토큰, 각 토큰 당 16차원의 임베딩\n","\n","print(input_embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"FwClYMSSjLjp"},"source":["## 절대적 위치 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ws1A-ALkjLWH","outputId":"d7d9226a-22db-4419-81f1-643fff7587df","executionInfo":{"status":"ok","timestamp":1725572955397,"user_tz":-540,"elapsed":474,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5, 16])\n","tensor([[[-1.9056,  0.4020, -0.6074,  2.6805,  3.7087, -0.6620, -1.2765,\n","           0.7662, -0.5810, -0.0544,  0.0943, -1.5781,  0.8282,  0.2502,\n","           1.8761, -2.7067],\n","         [ 1.1659, -0.9646, -0.1141,  0.8070, -0.1789, -1.8791,  0.4617,\n","          -1.0719,  0.2543, -0.5698,  0.6213, -1.4007,  0.6660, -0.0462,\n","          -1.5086, -3.6410],\n","         [ 3.1076,  1.3957,  1.5555,  0.9944,  1.4069, -2.1569, -0.5288,\n","           0.2961,  1.9556,  2.6009, -1.2951, -4.2932,  2.6212, -1.7372,\n","          -0.8859, -1.2518],\n","         [-2.4530, -1.4019, -1.1893,  2.2507,  2.3488, -2.5711,  0.7487,\n","           0.1037,  2.3612, -0.4340,  1.3498, -2.3379,  2.0035, -2.2868,\n","          -1.6020,  0.9593],\n","         [-1.3929, -0.7861,  0.9235, -2.5473,  1.9899, -1.3423,  1.8607,\n","          -0.7400, -2.9066,  1.3532,  0.4943,  1.7047,  1.0707, -0.5448,\n","          -1.0468, -0.3217]]], grad_fn=<AddBackward0>)\n"]}],"source":["# 논문과 달리 sin, cos 함수 사용X, 위치에 따른 임베딩 층을 추가해 위치 정보 학습 -> 이 방법 더 많이 씀\n","\n","embedding_dim = 16\n","max_position = 12 # 최대 토큰 수\n","\n","# 토큰 임베딩 층 생성\n","embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n","# 위치 인코딩 층 생성\n","position_embed_layer = nn.Embedding(max_position, embedding_dim) # 각 위치(0~11)에 대해 16차원의 벡터로 변환\n","\n","# input_ids의 길이 만큼 position_ids 생성하고 차원 추가\n","position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0) # tensor([[0, 1, 2, 3, 4]])\n","position_encodings = position_embed_layer(position_ids) # 각 위치에 해당하는 16차원의 임베딩 벡터 얻음\n","\n","# 토큰 임베딩 생성\n","token_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n","token_embeddings = token_embeddings.unsqueeze(0) # (1, 5, 16)\n","\n","# 토큰 임베딩과 위치 인코딩을 더해 최종 입력 임베딩 생성\n","input_embeddings = token_embeddings + position_encodings\n","\n","print(input_embeddings.shape)\n","print(input_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"0MBYHKRMkCDs"},"source":["## 쿼리, 키, 값 벡터를 만드는 nn.Linear 층"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rse5Xy6_jhok"},"outputs":[],"source":["head_dim = 16\n","\n","# 쿼리, 키, 값을 계산하기 위한 변환\n","weight_q = nn.Linear(embedding_dim, head_dim) # nn.Linear(input dim, output dim): 선형 변환 수행\n","weight_k = nn.Linear(embedding_dim, head_dim)\n","weight_v = nn.Linear(embedding_dim, head_dim)\n","\n","# 토큰(단어)마다 query, key, value 벡터 생성\n","querys = weight_q(input_embeddings) # (1, 5, 16) input_embeddings를 weight_q로 선형 변환하여 쿼리 벡터 생성\n","keys = weight_k(input_embeddings)   # (1, 5, 16) input_embeddings를 weight_k로 선형 변환하여 키 벡터 생성\n","values = weight_v(input_embeddings) # (1, 5, 16) input_embeddings를 weight_v로 선형 변환하여 값 벡터 생성"]},{"cell_type":"markdown","metadata":{"id":"wfitct-lkSP2"},"source":["## Scaled Dot-Product Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nftEA3lFkSwl"},"outputs":[],"source":["from math import sqrt\n","import torch.nn.functional as F\n","\n","def compute_attention(querys, keys, values, is_causal=False):\n","\tdim_k = querys.size(-1) # 16, key 벡터의 차원\n","\t# 내적으로 각 쿼리와 키 간의 유사도 계산, 어텐션 스코어의 분산 조정하기 위해 sqrt(dim_k)로 나눠주기\n","\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5) query와 key 내적한 후 dim_k의 제곱근으로 나누기\n","\t# 각 쿼리 벡터가 어느 키에 집중하는지 계산\n","\tweights = F.softmax(scores, dim=-1) # (1, 5, 5) 행 별로 softmax 취해\n","\treturn weights @ values # 어텐션 가중치와 value 벡터의 가중합 반환"]},{"cell_type":"markdown","metadata":{"id":"CzHY8tvlkiTl"},"source":["## 어텐션 연산의 입력과 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4evxbjRkfIi","outputId":"a2d76f0c-eb5f-4f74-f8df-317fc7eff8e2","executionInfo":{"status":"ok","timestamp":1725572964925,"user_tz":-540,"elapsed":821,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["원본 입력 형태:  torch.Size([1, 5, 16])\n","어텐션 적용 후 형태:  torch.Size([1, 5, 16])\n"]}],"source":["print(\"원본 입력 형태: \", input_embeddings.shape)\n","\n","# 주변 단어의 맥락 반영 가능\n","after_attention_embeddings = compute_attention(querys, keys, values)\n","\n","print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)\n","\n","# 입력과 동일한 형태의 출력 반환\n","# 원본 입력 형태: torch.Size([1, 5, 16])\n","# 어텐션 적용 후 형태: torch.Size([1, 5, 16]) 동일하당!"]},{"cell_type":"markdown","metadata":{"id":"OKv4X9rsknXs"},"source":["## 어텐션 연산을 수행하는 AttentionHead 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HWTZ4jukn5p"},"outputs":[],"source":["# 이제 class로 만들어보자!\n","\n","class AttentionHead(nn.Module):\n","  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n","    super().__init__()\n","    self.is_causal = is_causal\n","    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n","    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n","    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n","\n","  def forward(self, querys, keys, values):\n","    outputs = compute_attention(\n","        self.weight_q(querys),  # 쿼리 벡터\n","        self.weight_k(keys),    # 키 벡터\n","        self.weight_v(values),  # 값 벡터\n","        is_causal=self.is_causal\n","    )\n","    return outputs\n","\n","attention_head = AttentionHead(embedding_dim, embedding_dim)\n","after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"30IXVnNElE2O"},"source":["## 멀티 헤드 어텐션 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-qTbFVMlFND","outputId":"ea678aa7-40ec-4c95-8e64-1a56262f7749","executionInfo":{"status":"ok","timestamp":1725572970627,"user_tz":-540,"elapsed":482,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16])"]},"metadata":{},"execution_count":59}],"source":["class MultiheadAttention(nn.Module):\n","  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n","    super().__init__()\n","    self.n_head = n_head # head 수\n","    self.is_causal = is_causal\n","    self.weight_q = nn.Linear(token_embed_dim, d_model)\n","    self.weight_k = nn.Linear(token_embed_dim, d_model)\n","    self.weight_v = nn.Linear(token_embed_dim, d_model)\n","    self.concat_linear = nn.Linear(d_model, d_model)\n","\n","  def forward(self, querys, keys, values):\n","    B, T, C = querys.size() # B: 배치 크기(문장 수), T: 토큰 수(단어 수), C: input 임베딩 차원\n","    # view를 통해 멀티-헤드 구조로 변경\n","    # n_head만큼 연산을 수행하기 위해 쿼리, 키, 값 벡터의 차원을 n_head개로 쪼갠다\n","    # (1, 5, 16) -> (1, 4(n_head), 5, 4(C//n.head))\n","    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    # 각 헤드에서 어텐션을 계산\n","    attention = compute_attention(querys, keys, values, self.is_causal)\n","    # 그 결과를 다시 결합하여 입력과 같은 형태로 되돌리기\n","    # (1, 4, 5, 4) -> (1, 5, 4, 4) -> (1, 5, 16)\n","    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n","    # 최종적으로 선형 변환을 한 번 더 적용하여 출력 차원을 조정\n","    output = self.concat_linear(output)\n","    return output\n","\n","n_head = 4\n","mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n","after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n","after_attention_embeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"iWtHyqa_mAtB"},"source":["## Layer Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikXwtWFBl5zw","outputId":"bff704f6-dc25-451c-83de-b3dbaf356310","executionInfo":{"status":"ok","timestamp":1725573389355,"user_tz":-540,"elapsed":565,"user":{"displayName":"김도솔","userId":"17763044904108067981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5, 16])\n","tensor([[ 1.4901e-08,  4.4703e-08, -4.8429e-08,  4.4703e-08, -1.8626e-08]])\n","tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]])\n"]}],"source":["# 각 토큰 임베딩 별로 평균과 표준편차를 구해 정규화 수행\n","# 논문에서는 사후 정규화, 여기서는 사전 정규화(학습이 더 안정적)\n","norm = nn.LayerNorm(embedding_dim)\n","norm_x = norm(input_embeddings) # 입력 임베딩을 층 정규화 레이어에 통과시켜 정규화된 임베딩(norm_x)로 만들기\n","print(norm_x.shape) # torch.Size([1, 5, 16])\n","\n","# 실제로 평균과 표준편차 확인하기\n","print(norm_x.mean(dim=-1).data)\n","print(norm_x.std(dim=-1).data)"]},{"cell_type":"markdown","metadata":{"id":"nkeIvwCYnSOs"},"source":["## 피드 포워드 층 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e9702XvnSrT"},"outputs":[],"source":["# feed forward layer: 데이터의 특징을 학습하는 fully connected layer\n","# 멀티 헤드 어텐션이 단어 사이의 관계를 파악하는 역할이라면 feed forward layer는 입력 텍스트 전체를 이해하는 역할 담당\n","\n","class PreLayerNormFeedForward(nn.Module):\n","  def __init__(self, d_model, dim_feedforward, dropout):\n","    super().__init__()\n","    # d_model 차원에서 dim_feedforward 차원으로 확장했다가 다시 d_model로 변환\n","    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n","    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n","    # 드랍아웃 층을 추가하여 과적합을 방지\n","    # 드랍아웃은 학습 과정에서 무작위로 네트워크의 일부 노드를 비활성화(무시)하여 모델이 특정 노드에 의존하지 않도록 함\n","    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1\n","    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2\n","    self.activation = nn.GELU() # 활성 함수\n","    self.norm = nn.LayerNorm(d_model) # 층 정규화\n","\n","  def forward(self, src):\n","    x = self.norm(src)\n","    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n","    # 입력 x와 연산 결과를 더해 Residual Connection(잔차 연결)을 적용\n","    # Residual Connection은 정보가 소실되지 않도록, 원래 입력을 연산 결과에 더하는 역할\n","    x = self.dropout2(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"qq3eJqRInWWS"},"source":["## 인코더 층"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNCFpdVknUVa"},"outputs":[],"source":["class TransformerEncoderLayer(nn.Module):\n","  def __init__(self, d_model, nhead, dim_feedforward, dropout):\n","    super().__init__()\n","    self.attn = MultiheadAttention(d_model, d_model, nhead) # 멀티 헤드 어텐션 클래스\n","    self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n","    self.dropout1 = nn.Dropout(dropout) # 드랍아웃\n","    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드포워드\n","\n","  def forward(self, src):\n","    norm_x = self.norm1(src) # 층 정규화\n","    attn_output = self.attn(norm_x, norm_x, norm_x) # 멀티 헤드 어텐션\n","    x = src + self.dropout1(attn_output) # 잔차 연결\n","\n","    # 피드 포워드\n","    x = self.feed_forward(x) # 층 정규화, 피드 포워드, 잔차 연결 포함\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"W7acyE0lnc5L"},"source":["## 인코더 구현\n","언어를 이해하는 역할"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ty7TTF55nYDr"},"outputs":[],"source":["import copy\n","\n","# get_clone 함수는 입력한 모듈을 deepcopy를 통해 N번 반복해 모듈 리스트에 담음\n","def get_clones(module, N):\n","  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","# 인자로 전달받은 encoder_layer를 get_clones 함수를 통해 num_layers번 반복해 nn.ModuleList에 넣고,\n","# forward 메서드에서 for문을 통해 돌면서 인코더 층 연산을 반복 수행하도록 만든다.\n","class TransformerEncoder(nn.Module):\n","  def __init__(self, encoder_layer, num_layers):\n","    super().__init__()\n","    self.layers = get_clones(encoder_layer, num_layers)\n","    self.num_layers = num_layers\n","    self.norm = norm\n","\n","  def forward(self, src):\n","    output = src\n","    for mod in self.layers:\n","        output = mod(output)\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"2dJpZJGrnhMI"},"source":["## 디코더에서 어텐션 연산(마스크 어텐션)\n","학습할 때는 인코더와 디코더 모두 완성된 텍스트를 입력으로 받는다. 따라서 어텐션을 그대로 활용할 경우 작성해야 하는 텍스트를 미리 확인하게 되는 문제가 생긴다. 이를 막기 위해 특정 시점에는 그 이전에 생성된 토큰까지만 확인할 수 있도록 마스크를 추가한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2nBX5monelI"},"outputs":[],"source":["def compute_attention(querys, keys, values, is_causal=False):\n","\tdim_k = querys.size(-1) # 16\n","\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5)\n","\n","\t# is_causal이 true인 경우 마스크 연산 추가\n","\tif is_causal:\n","\t\tquery_length = querys.size(2) # 5\n","\t\tkey_length = keys.size(2) # 5\n","\t\t# 마스크 생성: torch.ones로 모두 1인 5x5 행렬에 tril 함수를 취해 대각선 포함한 아래 부분은 1, 나머지는 0\n","\t\ttemp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n","\t\t\"\"\"\n","\t\t(마스크 예시)\n","\t\t1 0 0 0 0\n","    1 1 0 0 0\n","  \t1 1 1 0 0\n","    1 1 1 1 0\n","    1 1 1 1 1\n","    \"\"\"\n","\t\t# scores에서 마스크의 false(0) 위치에 음의 무한대(-inf) 채워넣기 -> softmax 계산 시 0에 가까운 값 갖게 됨\n","\t\tscores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n","\t\t\"\"\"\n","\t\t(마스크 예시)\n","\t\t12 -inf -inf -inf -inf\n","    2    4  -inf -inf -inf\n","  \t-8   1    9  -inf -inf\n","    -10  4    6    4  -inf\n","    2    1   -6    12  -1\n","    \"\"\"\n","\n","\tweights = F.softmax(scores, dim=-1) # (1, 5, 5)\n","\treturn weights @ values # (1, 5, 16)"]},{"cell_type":"markdown","metadata":{"id":"5jxCS_lunl_7"},"source":["## 크로스 어텐션이 포함된 디코더 층"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7youbG9njnW"},"outputs":[],"source":["# 크로스 어텐션: 인코더의 결과를 디코더가 활용\n","# 이때 query는 디코더의 잠재 상태를 사용하고, key와 value는 인코더의 결과를 사용한다.\n","\n","class TransformerDecoderLayer(nn.Module):\n","  def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n","    super().__init__()\n","    self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n","    self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n","    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n","\n","    self.norm1 = nn.LayerNorm(d_model)\n","    self.norm2 = nn.LayerNorm(d_model)\n","    self.dropout1 = nn.Dropout(dropout)\n","    self.dropout2 = nn.Dropout(dropout)\n","\n","  def forward(self, tgt, encoder_output, is_causal=True):\n","    # 셀프 어텐션 연산\n","    x = self.norm1(tgt)\n","    x = x + self.dropout1(self.self_attn(x, x, x, is_causal=is_causal))\n","    # 크로스 어텐션 연산\n","    x = self.norm2(x)\n","    x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output)) # key, value에 encoder_output 사용\n","    # 피드 포워드 연산\n","    x = self.feed_forward(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"l218C0ZOnqDO"},"source":["## 디코더 구현\n","* 언어를 생성하는 역할\n","* 앞에서 생성한 토큰을 기반으로 다음 토큰 생성:  auto-regressive(자기 회귀적) 또는 causal(인과적)인 특징"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7meGa10vnnw1"},"outputs":[],"source":["import copy\n","def get_clones(module, N):\n","  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class TransformerDecoder(nn.Module):\n","  def __init__(self, decoder_layer, num_layers):\n","    super().__init__()\n","    self.layers = get_clones(decoder_layer, num_layers)\n","    self.num_layers = num_layers\n","\n","  def forward(self, tgt, src):\n","    output = tgt\n","    for mod in self.layers:\n","        output = mod(tgt, src)\n","    return output"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}