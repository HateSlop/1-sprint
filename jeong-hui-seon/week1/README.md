# LLM을 활용한 실전 AI 애플리케이션 개발

# 1부 | LLM의 기초 뼈대 세우기
# 1. LLM 지도
## 1.1 딥러닝과 언어 모델링
**LLM** : 딥러닝 기반의 언어 모델
> 사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 자연어 처리 분야 중 자연어 생성에 속하며, 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩 만들어 가는 방식으로 작동한다.
### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝
> 딥러닝은 머신러닝과 달리 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.
### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식
**임베딩** : 데이터의 의미와 특징을 포착해 숫자로 표현한 것 (ex: MBTI)
> 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분한다.
### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법
**언어 모델링** : 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식 <br><br>
**전이 학습** : 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
- **사전 학습** : 대량의 데이터로 모델을 학습시킨다.
- **미세 조정** : 특정한 문제를 해결하기 위한 데이터로 추가 학습을 시킨다.
> 언어 모델링은 자연어 처리 분야에서 **사전 학습**을 위한 과제로 사용된다.


## 1.2 언어 모델이 챗GPT가 되기까지
### 1.2.1 RNN에서 트랜스포머 아키텍처로
**RNN** : 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측한다.
> 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다.<br>

**트랜스포머 아키텍처** : 순차적인 처리 방식이 아닌, 맥락을 모두 참조하는 **어텐션** 연산을 사용한다.
> 많은 연산량이 필요하다는 단점이 있지만 성능이 좋고 병렬 처리르 통해 학습 속도를 높일 수 있다.

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계
> 모델과 학습 데이터셋의 크기만 키워도 언어 모델의 성능이 크게 높아진다.
### 1.2.3 챗GPT의 등장
- **지도 미세 조정** : **정렬**을 위한 가장 핵심적인 학습 과정, 언어 모델링으로 사전 학습한 언어 모델을 **지시 데이터셋**으로 추가 학습하는 것
  - **정렬** : LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것
  - **지시 데이터셋** : 사용자가 요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터셋
    
- **RLHF** : 사람의 피드백을 활용한 강화 학습
  - **선호 데이터셋** : 사용자가 더 선호하는 답변을 선택한 데이터셋
  > **선호 데이터셋**으로 LLM의 답변을 평가하는 **리워드 모델**을 만들고 LLM이 더 높은 점수를 받을 수 있도록 추가 학습하는데, 이때 강화학습을 사용한다.

## 1.3 LLM 애플리케이션의 시대가 열리다
### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM
> LLM은 자연어 이해와 자연어 생성 두 분야로 나눠 접근했던 기존 방식과 달리, 언어 이해와 언어 생성 능력이 모두 뛰어나다.
### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기
**sLLM** : 오픈소스 LLM에 추가 학습을 하는 경우, 모델 크기가 작으면서도 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델
### 1.3.3 더 효율적인 학습과 추론을 위한 기술
> LLM을 학습하고 추론할 때 GPU를 더 효율적으로 사용해 적은 GPU 자원으로도 LLM을 활용할 수 있도록 돕는 연구가 진행중이다.
### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 생성(RAG) 기술
- 환각 현상 : LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상
- **검색 증강 생성** : 프롬프트에 LLM이 답변할 대 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제를 줄이는 기술

## 1.4 LLM의 미래: 인식과 행동의 확장
- 멀티 모달 : 다양한 형식의 데이터를 입력/출력할 수 있는 모델
- 에이전트 : 텍스트 생성 능력을 이용해 계획을 세우거나 의사결정을 내리고 필요한 행동을 수행하는 기술
- 새로운 아키텍처 : 트랜스포머 아키텍처를 새로운 아키텍처로 변경해 입력을 효율적으로 처리하려는 연구

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기
> **LLM**은 모델 크기가 큰 딥러닝 기반의 언어 모델로, 대부분의 LLM이 **트랜스포머 아키텍처**를 기반으로 한다.
## 2.1 트랜스포머 아키텍처란
- **RNN** : 텍스트를 순차적으로 하나씩 입력하는 형태 <br>
- **트랜스포머 아키텍처**
  > 언어를 이해하는 **인코더**와 언어를 생성하는 **디코더**로 나뉜다.
  - 셀프 어텐션 : 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정한다.
  - 구성 요소 : **임베딩, 위치 인코딩, 층 정규화, 멀티 헤드 어텐션, 피드 포워드**
    
## 2.2 텍스트를 임베딩으로 변환하기
### 2.2.1 토큰화
- **토큰화** : 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것
- 어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 둔 사전을 만든다.
### 2.2.2 토큰 임베딩으로 변환하기
> 임베딩 시, 토큰을 임의의 숫자 집합으로 바꾸는데 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련을 해야한다.
### 2.2.3 위치 인코딩
> 트랜스포머는 RNN과 달리 입력을 동시에 처리하므로 순서 정보를 추가해주어야 한다.
- 위치 인코딩 : 위치에 따른 임베딩 층을 추가해 학습 데이터를 통해 학습한다.
  - **절대적 위치 인코딩** : 입력 토큰의 위치에 다라 고정된 임베딩을 더해준다.
  - **상대적 위치 인코딩**

## 2.3 어텐션 이해하기
### 2.3.1 사람이 글을 읽는 방법과 어텐션
**어텐션** : 사람이 **단어 사이의 관계**를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방한 연산
### 2.3.2 쿼리, 키, 값 이해하기
- **쿼리** : 입력하는 검색어
- **키** : 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징
- **값** : 쿼리와 관련이 깊은 키를 가진 문서
> 입력 데이터 사이의 관련도는 규칙이 아니라 데이터 자체에서 계산해야 한다.
> 가중치를 통해 토큰과 토큰 사이의 관계를 계산하는 능력을 학습시킨다. 쿼리, 키, 값을 가중치를 통해 내부적으로 토큰 사이의 관계를 계산해서 주변 맥락을 반영하는 방법을 학습한다.
### 2.3.3 코드로 보는 어텐션
### 2.3.4 멀티 헤드 어텐션
- 멀티 헤드 어텐션 : 여러 어텐션 연산을 동시에 적용해 성능을 높인 것

## 2.4 정규화와 피드 포워드 층
- 정규화 : 딥러닝 모델에서 **입력이 일정한 분포를 갖도록** 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법
### 2.4.1 층 정규화 이해하기
- 배치 정규화 : 모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행한다. 주로 이미지 처리에 사용한다.
- 층 정규화 : 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다. 주로 자연어 처리에 사용한다.
  > **배치 정규화**의 경우 정규화에 포함되는 데이터의 수가 제각각이라 정규화 효과를 보장하기 어렵다. **층 정규화**는 데이터의 수가 다르더라도 각 토큰 임베딩별로 정규화를 수행하기 때문에 정규화 효과에 차이가 없다. 
### 2.4.2 피드 포워드 층
- 피드 포워드 층 : 데이터의 특징들을 학습하는 **완전 연결 층**, 입력 텍스트 전체를 이해하는 역할을 담당한다.
  
## 2.5 인코더
> 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태이다. **잔차 연결**이 존재하는데, 이는 안정적인 학습이 가능하도록 입력을 다시 더해주는 형태로 구현한다. 인코더 블록을 반복해서 쌓아 만든다.

## 2.6 디코더
- 인코더와 달리 **마스크 멀티 헤드 어텐션**을 사용한다.
  > 디코더는 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다. (= 인과적, 자기 회귀적) 따라서 특정 시점에는 그 이전에 생성된 토큰까지만 확인할 수 있도록 **마스크**를 추가한다.
- 인코더와 달리 **크로스 어텐션**이 있다.
  > 크로스 어텐션 : 인코더의 결과를 디코더가 활용하는 것
- 디코더 층을 여러번 쌓아 만든다.
  
## 2.7 BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
### 2.7.1 인코더를 활용한 BERT
> 양방향 문맥을 이해할 수 있어 자연어 이해 작업에서 뛰어난 성능을 보인다.
### 2.7.2 디코더를 활용한 GPT
> 단방향 방식이며, 다음 토큰을 예측하는 방식으로 사전 학습을 수행한다.
### 2.7.3 인코더와 디코더를 모두 사용하는 BART, T5

## 2.8 주요 사전 학습 메커니즘
> 디코더 모델을 학습시키는 **인과적 언어 모델링**과 인코더 모델을 학습시키는 **마스크 언어 모델링**이 있다.
### 2.8.1 인과적 언어 모델링
- 문장의 시작부터 끝까지 **순차적으로** 단어를 예측하는 방식이다.
### 2.8.2 마스크 언어 모델링
- 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습시킨다.

# 3. 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리
## 3.1 허깅페이스 트랜스포머란
**허깅페이스 트랜스포머** : 다양한 트랜스포머 모델을 **통일된 인터페이스**로 사용할 수 있도록 지원하는 오픈소스 라이브러리
- transformers 라이브러리 : 트렌스포머 모델과 토크나이저를 활용할 때 사용
- datasets 라이브러리 : 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원

## 3.2 허깅페이스 허브 탐색하기
- 허브 : 다양한 사전 학습 모델과 데이터셋을 탐색하고 쉽게 불러와 사용할 수 있도록 제공하는 온라인 플랫폼
### 3.2.1 모델 허브
> 모델이 어떤 작업에 사용하는지, 어떤 언어로 학습된 모델인지 등 다양한 기준으로 분류되어 있다.
### 3.2.2 데이터셋 허브
> 모델 허브와 달리 분류 기준에 데이터셋 크기, 데이터 유형 등이 추가로 있다.
- KLUE : 한국어 언어 이해 평가의 약자로 텍스트 분류, 기계 독해, 문장 유사도 판단 등 다양한 작업에서 모델의 성능을 평가하기 위해 개발된 벤치마크 데이터셋
### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스
- 스페이스 : 사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능, 별도의 복잡한 웹 페이지 개발 없이 모델 데모를 공유할 수 있다.
> 허깅페이스는 다양한 오픈소스 LLM과 그 성능 정보를 게시하는 리더보드를 운영한다.

## 3.3 허깅페이스 라이브러리 사용법 익히기
> 모델을 학습시키거나 추론하기 위해서는 모델, 토크나이저, 데이터셋이 필요하다.
### 3.3.1 모델 활용하기
> 허깅페이스에서는 모델을 **바디와 헤드**로 구분한다. 이는 같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위함이다.
- AutoModel : 모델의 바디를 불러오는 클래스
- config.json 파일 : 허깅페이스 모델을 저장할 때 함께 저장되는 파일으로, 이를 참고해 적절한 모델과 토크나이저를 불러온다.
### 3.3.2 토크나이저 활용하기
- **토크나이저** : 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환한다. 필요한 경우 특수 토큰을 추가하는 역할도 한다.
- AutoTokenizer : 토크나이저를 불러오는 클래스
- tokenizer_config.json : 토크나이저의 종류나 설정에 대한 정보를 갖고 있다.
- tokenizer.json : 실제 어휘 사전 정보를 갖고 있다.

- input_ids : 토큰 아이디의 리스트
- attention_mask : 토큰이 실제 텍스트인지 아니면 길이를 맞추기 위해 추가한 패딩인지 알려준다.
  > 패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 특수 토큰이다.
- token_type_ids : 토큰이 속한 문장의 아이디를 알려준다.
  > 문장을 구분하는 역할을 한다. 
### 3.3.3 데이터셋 활용하기
1. 데이터셋 저장소에 있는 데이터를 불러온다. (load_dataset 함수)
2. 로컬 파일이나 파이썬 객체를 데이터셋으로 사용한다.

## 3.4 모델 학습시키기
> 모델을 학습 시키기 위해서는 사용할 데이터셋을 준비하고, 모델과 토크나이저를 불러온다.
### 3.4.1 데이터 준비
- 데이터셋을 불러와 변수에 저장한다.
- remove_columns 메서드 : 데이터셋에서 지정한 컬럼을 삭제한다.
- train_test_split 메서드 : 입력한 test_size 값에 맞춰 학습 데이터셋과 테스트 데이터셋으로 분리한다.
### 3.4.2 트레이너 API를 사용해 학습하기
- **트레이너 API** : 허깅페이스에서 제공하는 기능으로, 간편하게 모델 학습을 수행할 수 있도록 학습 과정을 추상화한 것이다. 학습에 필요한 다양한 기능을 학습 인자만으로 쉽게 활용할 수 있다.
- TrainingArguments : 학습 인자를 입력한다. (학습 에포크 수, 배치 크기, 결과를 저장할 폴더, 평가를 수행할 빈도 등)
- compute_metrics : 학습이 잘 이뤄지고 있는지 확인할 때 사용할 평가 지표를 정의한다.
- Trainer : 준비한 데이터셋과 설정을 인자로 전달하고 train() 메서드로 학습을 진행한다.
- evaluate() : 테스트 데이터셋에 대한 성능 평가를 수행한다.

- 트레이너 API의 장점 : 추상화를 통해 간편하게 사용할 수 있다.
- 트레이너 API의 단점 : 내부 동작을 파악하기 어렵다.
### 3.4.3 트레이너 API를 사용하지 않고 학습하기
- 트레이너가 내부적으로 수행하던 GPU로의 모델 이동을 직접 수행해야 한다.
- 데이터 전처리 과정 : tokenize_function (토큰화) -> rename_column -> remove_column -> DataLoader (데이터셋을 배치 데이터로)
- 학습 : train() (학습 모드 변경) -> 배치 데이터를 모델에 입력으로 전달 -> 모델 계산 -> 손실 값으로 역전파 -> step() (모델 업데이트)
- 평가 : 추론 모드 -> 모델 계산 결과로 예측 정보 찾기 -> 실제와 비교해 정확도 계산

- 트레이너를 사용하지 않는 장점 : 내부 동작을 명확히 확인할 수 있고 학습 과정을 조절할 수 있다.
### 3.4.4 학습한 모델 업로드하기
> push_to_hub() 메서드로 업로드 한다.

## 3.5 모델 추론하기
### 3.5.1 파이프라인을 활용한 추론
**pipeline** : 허깅페이스는 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 기능을 제공한다.
> 작업 종류, 모델, 설정을 입력으로 받는다.
### 3.5.2 직접 추론하기
1. 토큰화 수행 (tokenizer)
2. 모델 추론 수행
3. 가장 큰 예측 확률을 갖는 클래스 추출