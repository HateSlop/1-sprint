# LLM을 활용한 실전 AI 애플리케이션 개발

# 1부 | LLM의 기초 뼈대 세우기
# 1. LLM 지도
## 1.1 딥러닝과 언어 모델링
**LLM** : 딥러닝 기반의 언어 모델
> 사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 자연어 처리 분야 중 자연어 생성에 속하며, 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩 만들어 가는 방식으로 작동한다.
### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝
> 딥러닝은 머신러닝과 달리 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.
### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식
**임베딩** : 데이터의 의미와 특징을 포착해 숫자로 표현한 것 (ex: MBTI)
> 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분한다.
### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법
**언어 모델링** : 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식 <br><br>
**전이 학습** : 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
- **사전 학습** : 대량의 데이터로 모델을 학습시킨다.
- **미세 조정** : 특정한 문제를 해결하기 위한 데이터로 추가 학습을 시킨다.
> 언어 모델링은 자연어 처리 분야에서 **사전 학습**을 위한 과제로 사용된다.


## 1.2 언어 모델이 챗GPT가 되기까지
### 1.2.1 RNN에서 트랜스포머 아키텍처로
**RNN** : 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측한다.
> 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다.<br>

**트랜스포머 아키텍처** : 순차적인 처리 방식이 아닌, 맥락을 모두 참조하는 **어텐션** 연산을 사용한다.
> 많은 연산량이 필요하다는 단점이 있지만 성능이 좋고 병렬 처리르 통해 학습 속도를 높일 수 있다.

### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계
> 모델과 학습 데이터셋의 크기만 키워도 언어 모델의 성능이 크게 높아진다.
### 1.2.3 챗GPT의 등장
- **지도 미세 조정** : **정렬**을 위한 가장 핵심적인 학습 과정, 언어 모델링으로 사전 학습한 언어 모델을 **지시 데이터셋**으로 추가 학습하는 것
  - **정렬** : LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것
  - **지시 데이터셋** : 사용자가 요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터셋
    
- **RLHF** : 사람의 피드백을 활용한 강화 학습
  - **선호 데이터셋** : 사용자가 더 선호하는 답변을 선택한 데이터셋
  > **선호 데이터셋**으로 LLM의 답변을 평가하는 **리워드 모델**을 만들고 LLM이 더 높은 점수를 받을 수 있도록 추가 학습하는데, 이때 강화학습을 사용한다.

## 1.3 LLM 애플리케이션의 시대가 열리다
### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM
> LLM은 자연어 이해와 자연어 생성 두 분야로 나눠 접근했던 기존 방식과 달리, 언어 이해와 언어 생성 능력이 모두 뛰어나다.
### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기
**sLLM** : 오픈소스 LLM에 추가 학습을 하는 경우, 모델 크기가 작으면서도 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델
### 1.3.3 더 효율적인 학습과 추론을 위한 기술
> LLM을 학습하고 추론할 때 GPU를 더 효율적으로 사용해 적은 GPU 자원으로도 LLM을 활용할 수 있도록 돕는 연구가 진행중이다.
### 1.3.4 LLM의 환각 현상을 대처하는 검색 증강 생성(RAG) 기술
- 환각 현상 : LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상
- **검색 증강 생성** : 프롬프트에 LLM이 답변할 대 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제를 줄이는 기술

## 1.4 LLM의 미래: 인식과 행동의 확장
- 멀티 모달 : 다양한 형식의 데이터를 입력/출력할 수 있는 모델
- 에이전트 : 텍스트 생성 능력을 이용해 계획을 세우거나 의사결정을 내리고 필요한 행동을 수행하는 기술
- 새로운 아키텍처 : 트랜스포머 아키텍처를 새로운 아키텍처로 변경해 입력을 효율적으로 처리하려는 연구

# 2. LLM의 중추, 트랜스포머 아키텍처 살펴보기
> **LLM**은 모델 크기가 큰 딥러닝 기반의 언어 모델로, 대부분의 LLM이 **트랜스포머 아키텍처**를 기반으로 한다.
## 2.1 트랜스포머 아키텍처란
- **RNN** : 텍스트를 순차적으로 하나씩 입력하는 형태 <br>
- **트랜스포머 아키텍처**
  > 언어를 이해하는 **인코더**와 언어를 생성하는 **디코더**로 나뉜다.
  - 셀프 어텐션 : 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정한다.
  - 구성 요소 : **임베딩, 위치 인코딩, 층 정규화, 멀티 헤드 어텐션, 피드 포워드**
    
## 2.2 텍스트를 임베딩으로 변환하기
### 2.2.1 토큰화
- **토큰화** : 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것
- 어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 둔 사전을 만든다.
### 2.2.2 토큰 임베딩으로 변환하기
> 임베딩 시, 토큰을 임의의 숫자 집합으로 바꾸는데 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련을 해야한다.
### 2.2.3 위치 인코딩
> 트랜스포머는 RNN과 달리 입력을 동시에 처리하므로 순서 정보를 추가해주어야 한다.
- 위치 인코딩 : 위치에 따른 임베딩 층을 추가해 학습 데이터를 통해 학습한다.
  - **절대적 위치 인코딩** : 입력 토큰의 위치에 다라 고정된 임베딩을 더해준다.
  - **상대적 위치 인코딩**

## 2.3 어텐션 이해하기
### 2.3.1 사람이 글을 읽는 방법과 어텐션
**어텐션** : 사람이 **단어 사이의 관계**를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방한 연산
### 2.3.2 쿼리, 키, 값 이해하기
- **쿼리** : 입력하는 검색어
- **키** : 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징
- **값** : 쿼리와 관련이 깊은 키를 가진 문서
> 입력 데이터 사이의 관련도는 규칙이 아니라 데이터 자체에서 계산해야 한다.
> 가중치를 통해 토큰과 토큰 사이의 관계를 계산하는 능력을 학습시킨다. 쿼리, 키, 값을 가중치를 통해 내부적으로 토큰 사이의 관계를 계산해서 주변 맥락을 반영하는 방법을 학습한다.
### 2.3.3 코드로 보는 어텐션
### 2.3.4 멀티 헤드 어텐션
- 멀티 헤드 어텐션 : 여러 어텐션 연산을 동시에 적용해 성능을 높인 것

## 2.4 정규화와 피드 포워드 층
- 정규화 : 딥러닝 모델에서 **입력이 일정한 분포를 갖도록** 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법
### 2.4.1 층 정규화 이해하기
- 배치 정규화 : 모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행한다. 주로 이미지 처리에 사용한다.
- 층 정규화 : 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다. 주로 자연어 처리에 사용한다.
  > **배치 정규화**의 경우 정규화에 포함되는 데이터의 수가 제각각이라 정규화 효과를 보장하기 어렵다. **층 정규화**는 데이터의 수가 다르더라도 각 토큰 임베딩별로 정규화를 수행하기 때문에 정규화 효과에 차이가 없다. 
### 2.4.2 피드 포워드 층
- 피드 포워드 층 : 데이터의 특징들을 학습하는 **완전 연결 층**, 입력 텍스트 전체를 이해하는 역할을 담당한다.
  
## 2.5 인코더
> 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태이다. **잔차 연결**이 존재하는데, 이는 안정적인 학습이 가능하도록 입력을 다시 더해주는 형태로 구현한다. 인코더 블록을 반복해서 쌓아 만든다.

## 2.6 디코더
- 인코더와 달리 **마스크 멀티 헤드 어텐션**을 사용한다.
  > 디코더는 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다. (= 인과적, 자기 회귀적) 따라서 특정 시점에는 그 이전에 생성된 토큰까지만 확인할 수 있도록 **마스크**를 추가한다.
- 인코더와 달리 **크로스 어텐션**이 있다.
  > 크로스 어텐션 : 인코더의 결과를 디코더가 활용하는 것
- 디코더 층을 여러번 쌓아 만든다.
  
## 2.7 BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
### 2.7.1 인코더를 활용한 BERT
> 양방향 문맥을 이해할 수 있어 자연어 이해 작업에서 뛰어난 성능을 보인다.
### 2.7.2 디코더를 활용한 GPT
> 단방향 방식이며, 다음 토큰을 예측하는 방식으로 사전 학습을 수행한다.
### 2.7.3 인코더와 디코더를 모두 사용하는 BART, T5

## 2.8 주요 사전 학습 메커니즘
> 디코더 모델을 학습시키는 **인과적 언어 모델링**과 인코더 모델을 학습시키는 **마스크 언어 모델링**이 있다.
### 2.8.1 인과적 언어 모델링
- 문장의 시작부터 끝까지 **순차적으로** 단어를 예측하는 방식이다.
### 2.8.2 마스크 언어 모델링
- 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습시킨다.

# 3. 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리

