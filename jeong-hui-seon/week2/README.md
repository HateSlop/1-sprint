# LLM을 활용한 실전 AI 애플리케이션 개발

# 2부 | LLM 길들이기
# 7. 모델 가볍게 만들기
- LLM을 배포할 때 GPU 사용에 가장 많은 비용이 발생한다.
- GPU를 가능하면 적게 사용해야 비용 효율적인 서빙이 가능하다.
    - 모델의 성능을 희생하더라도 비용을 낮추는 방법 → 이번 장에서 다룰 예정
    - 모델의 성능을 유지하면서 연산 과정의 비효율을 줄이는 방법 → 8장
## 7.1 언어 모델 추론 이해하기
> GPU를 효율적으로 사용하는 방법을 알기 위해, **LLM의 추론 과정**을 살펴본다.
    > 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 동일한 연산을 최소화하기 위해 **KV 캐시**를 사용한다.
### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하며 토큰을 생성한다.
- 언어 모델이 텍스트 생성을 마치는 이유
    1. 생성 종료를 의미하는 특수 토큰을 생성하는 경우
    2. 최대 길이로 설정한 길이에 도달하는 경우
- 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적 특성을 갖는다.
- 언어 모델의 추론 과정
    1. 사전 계산 단계 : 프롬프트를 처리하는 단계, 동시에 병렬적으로 처리한다.
    2. 디코딩 단계 : 한 토큰씩 생성한다.
- 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복 수행해 비효율적이다.
- 트랜스포머 모델은 셀프 어텐션 연산을 수행한다.
    > 셀프 어텐션이란? 입력 텍스트에서 어떤 토큰이 서로 관련되는지 계산한 후 결과에 따라 토큰 임베딩을 새롭게 조정한다. 관련도 계산을 위해 토큰 임베딩을 쿼리, 키, 값 벡터로 변환한다.
- 입력 토큰에 대해 키, 값 벡터로의 변환을 반복 수행하지 않고, 계산 결과를 저장하고 있다가 다시 사용한다. (KV 캐시)
### 7.1.2 중복 연산을 줄이는 KV 캐시
- **KV 캐시** : 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 **키와 값** 결과를 메모리에 저장해 활용하는 방법
- KV 캐시 메모리 = 2byte * 2 * 레이어 수 * 토큰 임베딩 차원 * 최대 시퀀스 길이 * 배치 크기
> GPU를 효율적으로 활용하려면 한 번에 더 많은 입력을 처리해야 한다. 최적의 배치 크기를 알아보자.
### 7.1.3 GPU 구조와 최적의 배치 크기
- 서빙이 효율적인지 판단하는 기준
    1. 비용
    2. 처리량 : 시간 당 처리한 요청수
    3. 지연 시간 : 하나의 토큰을 생성하는 데 걸리는 시간
    > 효율적인 서빙을 위해서는 같은 GPU로 처리량을 높이고 지연시간을 낮춰야 한다.
- GPU의 구조
    - 여러 SM으로 구성된다.
        - 연산을 수행하는 부분
        - **SRAM** : 계산할 값을 저장 (= L1 캐시, 공유 메모리)
    - **HBM** : 고대역폭 메모리, 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어려우므로 큰 데이터를 저장한다.
- 추론 수행 시, 배치 크기 만큼의 토큰을 한번에 생성한다.
    > 계산량 : 2 * P(모델 파라미터 메모리) * 배치크기 byte
- 연산 수행을 위해서, HBM에 있는 모델 파라미터를 SRAM으로 이동시켜야 한다.

> 즉, 연산에 걸리는 시간과 모델 파라미터를 이동시키는 시간 두 가지를 고려해야 하는데, 두 시간이 같을 때가 **최적의 배치 크기**가 된다.
- 메모리 바운드 : 최적의 배치 크기(B*)보다 배치 크기가 작은 경우, 모델 파라미터를 이동시키느라 연산이 멈춘다.
- 연산 바운드 : B*보다 배치 크기가 큰 경우, 연산에 오랜 시간이 걸려 지연 시간이 길어진다.

> 그렇다면 배치 크기를 키우는 방법은 무엇이 있을까?
- 모델의 용량을 줄이는 방법 (멀티 쿼리 어텐션, 그룹 쿼리 어텐션)
- KV 캐시의 용량을 줄이는 방법 (양자화, 지식 증류)
### 7.1.4 KV 캐시 메모리 줄이기
- 멀티 헤드 어텐션 : 트랜스포머 모델이 셀프 어텐션 연산을 수행하는 방식
    > 쿼리와 키 사이 다양한 관련성을 반영하고 성능이 높다.
    > 하지만, KV 캐시 사용량이 높아져 속도가 느려진다.
- 멀티 쿼리 어텐션 : 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유한다.
    > KV 캐시를 저장하는 데 적은 메모리를 사용하지만, 성능이 떨어짐
- 그룹 쿼리 어텐션 : 멀티 헤드 어텐션보다는 키와 값의 수를 줄이고, 멀테 쿼리 어텐션보다는 늘린 절충안
    > 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 사용량을 줄일 수 있다.

## 7.2 양자화로 모델 용량 줄이기
> 모델을 저장할 때 더 적은 메모리를 사용하도록 데이터 타입을 변환하는 **양자화**를 한다.
- 양자화 : 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU를 효율적으로 사용하는 방법
> 양자화의 목표 : 더 적은 메모리를 사용하면서 최대한 원본 모델의 정보를 유지하는 것
> 모델의 크기가 커지며 파라미터 용량을 줄이려는 연구가 이어지고 있다.
- 양자화를 수행하는 시점에 따라 양자화를 분류할 수 있다.
    - 학습 후 양자화 : LLM은 학습에 많은 자원이 들어 주로 이 방식을 활용한다.
        > 비츠앤바이츠, GPTQ, AWQ
    - 학습 전 양자화
### 7.2.1 비츠앤바이츠
- 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리
1. 8비트 행렬 연산
- 입력 값 중 크기가 큰 이상치가 포함된 열을 별도로 분리해 16비트 그대로 계산한다.
- 정상 범위에 있는 열을 양자화할 대 벡터 단위로 절대 최댓값을 찾고 그 값을 기준으로 양자화를 수행한다.
- 8비트로 양자화한 정상 값 벡터끼리 행렬 곱셈을 하고 이상치 벡터끼리 행렬 곱셈을 수행해 최종 결과를 산출한다.
2. 4비트 정규 분포 양자화 : 5.5장에서 살펴본 QLoRA 방식이다.

> 비츠앤바이츠 양자화의 경우, 양자화에 별도의 시간이 걸리지 않는다. 즉 모델을 불러오면서 바로 양자화가 가능하다.
### 7.2.2 GPTQ
- GPTQ : 양자화 이전의 모델에 입력을 넣었을 때와 양자화 이후의 모델에 입력을 넣었을 때 오차가 가장 작아지도록 양자화를 수행한다.
- 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하며 양자화 이전과 유사한 결과가 나오도록 모델을 업데이트한다.
- 양자화 진행 과정에서 현재 양자화를 수행하는 열을 기준으로 왼쪽은 이미 양자화를 수행 완료한 열이고 오른쪽은 다음에 양자화를 수행할 열이다.
- 데이터 입력 결과가 이전과 최대한 가까워지도록 양자화를 수행하지 않은 부분의 파라미터를 업데이트한다.
### 7.2.3 AWQ
> 모델의 성능을 유지한다는 것은 모델이 가지고 있는 정보를 최대한 손실 없이 변환하는 것이다.
- AWQ : 모든 파라미트가 동등하게 중요하지 않고, 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있는 방법이다.
- 어떤 파라미터가 중요한지 판단할 수 있는 방법
    1. 모델 파라미터의 값이 큰 경우
    2. 입력 데이터의 활성화 값이 큰 경우
- 중요한 모델 파라미터를 찾고 해당 파라미터는 기존 모델의 데이터 타입으로 유지하고 나머지를 양자화한다.
> 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델의 성능이 유지된다.
- 모델 파라미터에 서로 다른 데이터 타입이 섞여 있는 경우 연산이 느리고 효율성이 떨어지는 문제가 발생한다.
    - 중요한 파라미터에만 1보다 큰 값을 곱하는 방식으로 해결
        > 곱해주는 값을 스케일러라고 부른다.
        > 스케일러 s가 2일 때까지는 성능이 향상되지만 넘어가는 경우 성능이 다시 하락한다.
        - 나머지 모델 파라미터의 정보에는 영향을 주지 않으면서 중요한 파라미터의 정보 소실을 막을 수 있다.
- 양자화에 많은 시간이 걸리지 않고 기존 모델의 성능을 거의 유지할 수 있다.
## 7.3 지식 증류 활용하기
> 선생 모델의 생성결과를 활용해 더 작고 효율적인 학생 모델을 학습하는 **지식 증류**를 알아본다.
- 선생 모델을 활용해 완전히 새로운 학습 데이터셋을 대규모로 구축하거나 데이터셋 구축에 사람의 판단이 필요한 부분을 선생 모델이 수행한다.
    - 제퍼 모델
        > 지시 데이터셋의 구축과 선호 데이터셋의 구축에 모두 LLM을 사용하였다.
    - 파이 모델
        > 학습 데이터로 사용할 코드를 선택하는 데 GPT-3.5를 사용하였다.