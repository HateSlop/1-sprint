# LLM을 활용한 실전 AI 애플리케이션 개발

# 2부 | LLM 길들이기
# 7. 모델 가볍게 만들기
- LLM을 배포할 때 GPU 사용에 가장 많은 비용이 발생한다.
- GPU를 가능하면 적게 사용해야 비용 효율적인 서빙이 가능하다.
    - 모델의 성능을 희생하더라도 비용을 낮추는 방법 → 이번 장에서 다룰 예정
    - 모델의 성능을 유지하면서 연산 과정의 비효율을 줄이는 방법 → 8장
## 7.1 언어 모델 추론 이해하기
> GPU를 효율적으로 사용하는 방법을 알기 위해, **LLM의 추론 과정**을 살펴본다.
    > 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 동일한 연산을 최소화하기 위해 **KV 캐시**를 사용한다.
### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하며 토큰을 생성한다.
- 언어 모델이 텍스트 생성을 마치는 이유
    1. 생성 종료를 의미하는 특수 토큰을 생성하는 경우
    2. 최대 길이로 설정한 길이에 도달하는 경우
- 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적 특성을 갖는다.
- 언어 모델의 추론 과정
    1. 사전 계산 단계 : 프롬프트를 처리하는 단계, 동시에 병렬적으로 처리한다.
    2. 디코딩 단계 : 한 토큰씩 생성한다.
- 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복 수행해 비효율적이다.
- 트랜스포머 모델은 셀프 어텐션 연산을 수행한다.
    > 셀프 어텐션이란? 입력 텍스트에서 어떤 토큰이 서로 관련되는지 계산한 후 결과에 따라 토큰 임베딩을 새롭게 조정한다. 관련도 계산을 위해 토큰 임베딩을 쿼리, 키, 값 벡터로 변환한다.
- 입력 토큰에 대해 키, 값 벡터로의 변환을 반복 수행하지 않고, 계산 결과를 저장하고 있다가 다시 사용한다. (KV 캐시)
### 7.1.2 중복 연산을 줄이는 KV 캐시
- **KV 캐시** : 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 **키와 값** 결과를 메모리에 저장해 활용하는 방법
- KV 캐시 메모리 = 2byte * 2 * 레이어 수 * 토큰 임베딩 차원 * 최대 시퀀스 길이 * 배치 크기
> GPU를 효율적으로 활용하려면 한 번에 더 많은 입력을 처리해야 한다. 최적의 배치 크기를 알아보자.
### 7.1.3 GPU 구조와 최적의 배치 크기
- 서빙이 효율적인지 판단하는 기준
    1. 비용
    2. 처리량 : 시간 당 처리한 요청수
    3. 지연 시간 : 하나의 토큰을 생성하는 데 걸리는 시간
    > 효율적인 서빙을 위해서는 같은 GPU로 처리량을 높이고 지연시간을 낮춰야 한다.
- GPU의 구조
    - 여러 SM으로 구성된다.
        - 연산을 수행하는 부분
        - **SRAM** : 계산할 값을 저장 (= L1 캐시, 공유 메모리)
    - **HBM** : 고대역폭 메모리, 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어려우므로 큰 데이터를 저장한다.
- 추론 수행 시, 배치 크기 만큼의 토큰을 한번에 생성한다.
    > 계산량 : 2 * P(모델 파라미터 메모리) * 배치크기 byte
- 연산 수행을 위해서, HBM에 있는 모델 파라미터를 SRAM으로 이동시켜야 한다.

> 즉, 연산에 걸리는 시간과 모델 파라미터를 이동시키는 시간 두 가지를 고려해야 하는데, 두 시간이 같을 때가 **최적의 배치 크기**가 된다.
- 메모리 바운드 : 최적의 배치 크기(B*)보다 배치 크기가 작은 경우, 모델 파라미터를 이동시키느라 연산이 멈춘다.
- 연산 바운드 : B*보다 배치 크기가 큰 경우, 연산에 오랜 시간이 걸려 지연 시간이 길어진다.

> 그렇다면 배치 크기를 키우는 방법은 무엇이 있을까?
- 모델의 용량을 줄이는 방법 (멀티 쿼리 어텐션, 그룹 쿼리 어텐션)
- KV 캐시의 용량을 줄이는 방법 (양자화, 지식 증류)
### 7.1.4 KV 캐시 메모리 줄이기
- 멀티 헤드 어텐션 : 트랜스포머 모델이 셀프 어텐션 연산을 수행하는 방식
    > 쿼리와 키 사이 다양한 관련성을 반영하고 성능이 높다.
    > 하지만, KV 캐시 사용량이 높아져 속도가 느려진다.
- 멀티 쿼리 어텐션 : 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유한다.
    > KV 캐시를 저장하는 데 적은 메모리를 사용하지만, 성능이 떨어짐
- 그룹 쿼리 어텐션 : 멀티 헤드 어텐션보다는 키와 값의 수를 줄이고, 멀테 쿼리 어텐션보다는 늘린 절충안
    > 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 사용량을 줄일 수 있다.

## 7.2 양자화로 모델 용량 줄이기
> 모델을 저장할 때 더 적은 메모리를 사용하도록 데이터 타입을 변환하는 **양자화**를 한다.
- 양자화 : 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU를 효율적으로 사용하는 방법
> 양자화의 목표 : 더 적은 메모리를 사용하면서 최대한 원본 모델의 정보를 유지하는 것
> 모델의 크기가 커지며 파라미터 용량을 줄이려는 연구가 이어지고 있다.
- 양자화를 수행하는 시점에 따라 양자화를 분류할 수 있다.
    - 학습 후 양자화 : LLM은 학습에 많은 자원이 들어 주로 이 방식을 활용한다.
        > 비츠앤바이츠, GPTQ, AWQ
    - 학습 전 양자화
### 7.2.1 비츠앤바이츠
- 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리
1. 8비트 행렬 연산
- 입력 값 중 크기가 큰 이상치가 포함된 열을 별도로 분리해 16비트 그대로 계산한다.
- 정상 범위에 있는 열을 양자화할 대 벡터 단위로 절대 최댓값을 찾고 그 값을 기준으로 양자화를 수행한다.
- 8비트로 양자화한 정상 값 벡터끼리 행렬 곱셈을 하고 이상치 벡터끼리 행렬 곱셈을 수행해 최종 결과를 산출한다.
2. 4비트 정규 분포 양자화 : 5.5장에서 살펴본 QLoRA 방식이다.

> 비츠앤바이츠 양자화의 경우, 양자화에 별도의 시간이 걸리지 않는다. 즉 모델을 불러오면서 바로 양자화가 가능하다.
### 7.2.2 GPTQ
- GPTQ : 양자화 이전의 모델에 입력을 넣었을 때와 양자화 이후의 모델에 입력을 넣었을 때 오차가 가장 작아지도록 양자화를 수행한다.
- 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하며 양자화 이전과 유사한 결과가 나오도록 모델을 업데이트한다.
- 양자화 진행 과정에서 현재 양자화를 수행하는 열을 기준으로 왼쪽은 이미 양자화를 수행 완료한 열이고 오른쪽은 다음에 양자화를 수행할 열이다.
- 데이터 입력 결과가 이전과 최대한 가까워지도록 양자화를 수행하지 않은 부분의 파라미터를 업데이트한다.
### 7.2.3 AWQ
> 모델의 성능을 유지한다는 것은 모델이 가지고 있는 정보를 최대한 손실 없이 변환하는 것이다.
- AWQ : 모든 파라미트가 동등하게 중요하지 않고, 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있는 방법이다.
- 어떤 파라미터가 중요한지 판단할 수 있는 방법
    1. 모델 파라미터의 값이 큰 경우
    2. 입력 데이터의 활성화 값이 큰 경우
- 중요한 모델 파라미터를 찾고 해당 파라미터는 기존 모델의 데이터 타입으로 유지하고 나머지를 양자화한다.
> 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델의 성능이 유지된다.
- 모델 파라미터에 서로 다른 데이터 타입이 섞여 있는 경우 연산이 느리고 효율성이 떨어지는 문제가 발생한다.
    - 중요한 파라미터에만 1보다 큰 값을 곱하는 방식으로 해결
        > 곱해주는 값을 스케일러라고 부른다.
        > 스케일러 s가 2일 때까지는 성능이 향상되지만 넘어가는 경우 성능이 다시 하락한다.
        - 나머지 모델 파라미터의 정보에는 영향을 주지 않으면서 중요한 파라미터의 정보 소실을 막을 수 있다.
- 양자화에 많은 시간이 걸리지 않고 기존 모델의 성능을 거의 유지할 수 있다.
## 7.3 지식 증류 활용하기
> 선생 모델의 생성결과를 활용해 더 작고 효율적인 학생 모델을 학습하는 **지식 증류**를 알아본다.
- 선생 모델을 활용해 완전히 새로운 학습 데이터셋을 대규모로 구축하거나 데이터셋 구축에 사람의 판단이 필요한 부분을 선생 모델이 수행한다.
    - 제퍼 모델
        > 지시 데이터셋의 구축과 선호 데이터셋의 구축에 모두 LLM을 사용하였다.
    - 파이 모델
        > 학습 데이터로 사용할 코드를 선택하는 데 GPT-3.5를 사용하였다.

# 8. sLLM 서빙하기
> 성능 하락 없이 추론 속도를 높이는 다양한 방법들을 살펴보자.
## 8.1 효율적인 배치 전략
> 언어 모델의 특성 상 한 번에 하나의 토큰을 생성하고, 입력에 따라 몇 개의 토큰을 생성할지 예측하기 어려우므로 배치전략을 세우는 것이 중요하다.
### 8.1.1 일반 배치(정적 배치)
- **일반 배치(정적 배치)**: 입력 데이터를 배치 처리할 때, 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식

- 두 가지 문제점
    1. 생성이 종료된 이후 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 대기하는 입력이 존재한다.
    2. 생성이 일찍 종료되는 문장이 있으면 배치 크기가 작아져 GPU를 효율적으로 사용하지 못하게 된다.
### 8.1.2 동적 배치
- **동적 배치**: 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략
- 온라인 서빙에서 배치 크기를 키워 처리량을 높일 수 있지만, 생성하는 토큰 길이 차이로 인해 처리하는 배치 크기가 점차 줄어 여전히 GPU를 비효율적으로 사용한다.
### 8.1.3 연속 배치
- **연속 배치**: 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가한다.
- 모델의 추론 과정에서 사전 연산과 디코딩은 처리 방식이 다르므로 한 번의 토큰 생성이 끝날 때마다 새로운 문장을 배치에 추가하지 않기도 한다.

## 8.2 효율적인 트랜스포머 연산
- 셀프 어텐션 연산 : 쿼리와 키 벡터 사이의 관련도를 계산해 새롭게 토큰 임베딩을 조정하므로 성능은 높지만 많은 연산 필요
    > **플래시어텐션**으로 메모리 사용량을 줄인다.
- 절대적 위치 인코딩 : 토큰의 위치 정보를 추가할 때 토큰의 위치에 따라 동일한 임베딩을 추가한다. 긴 입력 데이터에는 성능이 저하된다.
    > 토큰의 상대적 위치를 추가하는 **상대적 위치 인코딩**으로 개선한다.
### 8.2.1 플래시어텐션
- 플래시어텐션 : 긴 시퀀스를 처리하는 데 어려움이 있던 어텐션 연산 과정을 변경해 학습 과정에 필요한 메모리를 시퀀스 길이에 비례하도록 개선
    > 긴 시퀀스는 다양한 형태의 입력을 처리하는 데 중요하다.
- 트랜스포머 연산은 쿼리와 키 벡터를 곱하는 데에 많은 메모리를 사용하는데, 특히 **마스크, 소프트맥스, 드롭아웃 과정**까지의 행렬 크기가 매우 크다.
    > 따라서 그 과정에서 연산은 더 오래 걸린다.
    > GPU에서 메모리를 읽고 쓰는 데 오랜 시간이 걸리기 때문이다.
> GPU의 구조는 **SRAM**(데이터 이동 속도는 빠르나 메모리 크기가 작음), **HBM**(데이터 이동 속도 느림)가 있다.
- 플래시어텐션에서는 데이터 이동 속도가 느린 고대역폭 메모리에 큰 어텐션 행렬을 쓰고 읽으면서 걸리는 시간을 줄인다.
    1. 블록 단위로 어텐션 연산을 수행하며 전체 어텐션 행렬을 읽거나 쓰지 않는다.
    2. HBM이 아닌 SRAM에 데이터를 읽고 쓰면서 연산을 빠르게 수행한다.
- 역전파 과정에서 순전파의 전체 행렬의 값이 필요한 경우, 역전파 과정에서 다시 순전파를 계산하는 방식으로 작동한다.
> 계산량은 증가하지만, 실행시간은 감소한다.
### 8.2.2 플래시어텐션 2
- 플래시 어텐션을 개선해 2배 정도 속도를 향상했다.
1. 행렬 곱셈이 아닌 연산 줄이기
    > GPU가 행렬 연산에 최적화돼 있기 때문이다.
2. 시퀀스 길이 방향의 병렬화 추가
    - 스레드 : GPU의 가장 작은 계산 단위, 하나의 스트리밍 프로세서에서 처리된다.
    - 워프 : 32개의 스레드를 하나의 명령으로 실행하는 단위
    - 스레드 블록 : 4-8개의 워프를 모은 단위, 각각의 SM에 배정된다.
    > 배치 크기가 작거나 어텐션 헤드 수가 작은 경우 SM을 충분히 활용하지 못 하기 때문이다.
### 8.2.3 상대적 위치 인코딩
- 사인파 위치 인코딩: 사인과 코사인을 사용해 위치 인코딩을 더하는 방식
- 위치 인코딩을 학습할 수 있는 파라미터로 두고 모델 학습 과정에서 함께 학습
> 두 방법 모두 절대적 위치 인코딩으로, 학습 데이터보다 긴 입력이 들어올 경우 성능이 떨어진다.

- 상대적 위치 인코딩: 토큰과 토큰 사이의 상대적인 위치 정보를 추가한다.
1. RoPE: 각각의 토큰 임베딩을 토큰 위치에 따라 회전시켜 토큰 사이의 위치 정보가 두 임베딩 사이의 각도로 반영된다.
2. ALiBi: 쿼리와 키 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식, 학습과 추론에 별도로 처리 시간을 추가하지 않는다.

## 8.3 효율적인 추론 전략
### 8.3.1 커널 퓨전
- GPU에서 연산은 커널 단위로 이뤄진다.
- 연산을 수행하기 위해, 전후 추가적인 작업을 위한 오버헤드가 발생한다.
- **커널 퓨젼**: 반복적으로 수행하는 연산에 대해 각 커널 단위로 분리해 수행하는 것보다 연산을 하나로 묶어 오버헤드를 줄이는 것
    > ex. 플래시어텐션
### 8.3.2 페이지어텐션
> KV 캐시는 중복 연산을 줄여 연산 시간을 줄이지만, 많은 GPU 메모리를 사용한다.
- 페이지 어텐션을 사용하면 KV 캐시를 효율적으로 관리하고, 배치 크기를 키울 수 있다.
> 기존 KV 캐시는 몇 개의 토큰을 생성할 지 알지 못하므로 최대 생성 토큰 수만큼 미리 메모리를 예약한다.
- 논리적 메모리와 물리적 메모리를 연결하는 **블록 테이블**을 관리해 실제로 물리적으로 연속된 메모리를 사용하지 않으면서 논리적 메모리에서는 서로 연속적이도록 한다.
- 큰 메모리를 예약하지 않고 최대 블록크기만큼의 메모리만 배정한다.
- 병렬 샘플링에서 각기 다른 토큰을 생성한 경우 **참조 카운트**를 활용한다.
    > 참조 카운트가 1보다 클 경우 새로운 물리적 블록에 기존 토큰을 복사해 동일한 토큰은 최대한 메모리를 공유하고 새롭게 생성하는 토큰만 서로 분리해 메모리를 효율적으로 활용한다.
### 8.3.3 추측 디코딩
- 추측 디코딩: 쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식
- 드래프트 모델 : 빠르게 추론이 가능하므로 k개의 토큰을 먼저 생성한다.
- 타깃 모델: 드래프트 모델이 생성한 토큰이 타깃 모델이 추론했다면 생성했을 결과와 동일한지 계산해 승인 혹은 거절한다.
> 타깃 모델 한 번의 추론 시간과 유사한 시간동안 훨씬 많은 토큰을 생성할 수 있어 추론 속도가 빨라진다.

## 8.4 실습: LLM 서빙 프레임워크
> LLM 서빙 프레임워크 vLLM을 사용한다.
### 8.4.1 오프라인 서빙
- 오프라인 서빙: 정해진 입력 데이터에 대해 배치 추론을 수행하는 것
> 모델에 입력할 데이터가 이미 정해져 있어 배치 처리를 통해 처리량을 높일 수 있다.
> vLLM을 사용하는 경우, 허깅페이스 배치 추론 대비 추론 시간이 줄어든다.
### 8.4.2 온라인 서빙
- 온라인 서빙: 사용자의 요청이 올 때 모델 추론을 수행한다는 점에서 차이가 있다.
> vLLM은 온라인 서빙에 사용할 수 있는 API 서버를 제공한다.