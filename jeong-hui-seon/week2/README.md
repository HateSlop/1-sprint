# LLM을 활용한 실전 AI 애플리케이션 개발

# 2부 | LLM 길들이기
# 7. 모델 가볍게 만들기
- LLM을 배포할 때 GPU 사용에 가장 많은 비용이 발생한다.
- GPU를 가능하면 적게 사용해야 비용 효율적인 서빙이 가능하다.
    - 모델의 성능을 희생하더라도 비용을 낮추는 방법 → 이번 장에서 다룰 예정
    - 모델의 성능을 유지하면서 연산 과정의 비효율을 줄이는 방법 → 8장
## 7.1 언어 모델 추론 이해하기
> GPU를 효율적으로 사용하는 방법을 알기 위해, **LLM의 추론 과정**을 살펴본다.
    > 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 동일한 연산을 최소화하기 위해 **KV 캐시**를 사용한다.
### 7.1.1 언어 모델이 언어를 생성하는 방법
> 언어 모델은 입력한 텍스트 다음에 올 토큰의 확률을 계산하며 토큰을 생성한다.
- 언어 모델이 텍스트 생성을 마치는 이유
    1. 생성 종료를 의미하는 특수 토큰을 생성하는 경우
    2. 최대 길이로 설정한 길이에 도달하는 경우
- 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적 특성을 갖는다.
- 언어 모델의 추론 과정
    1. 사전 계산 단계 : 프롬프트를 처리하는 단계, 동시에 병렬적으로 처리한다.
    2. 디코딩 단계 : 한 토큰씩 생성한다.
- 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복 수행해 비효율적이다.
- 트랜스포머 모델은 셀프 어텐션 연산을 수행한다.
    > 셀프 어텐션이란? 입력 텍스트에서 어떤 토큰이 서로 관련되는지 계산한 후 결과에 따라 토큰 임베딩을 새롭게 조정한다. 관련도 계산을 위해 토큰 임베딩을 쿼리, 키, 값 벡터로 변환한다.
- 입력 토큰에 대해 키, 값 벡터로의 변환을 반복 수행하지 않고, 계산 결과를 저장하고 있다가 다시 사용한다. (KV 캐시)
### 7.1.2 중복 연산을 줄이는 KV 캐시
- **KV 캐시** : 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 **키와 값** 결과를 메모리에 저장해 활용하는 방법
- KV 캐시 메모리 = 2byte * 2 * 레이어 수 * 토큰 임베딩 차원 * 최대 시퀀스 길이 * 배치 크기
> GPU를 효율적으로 활용하려면 한 번에 더 많은 입력을 처리해야 한다. 최적의 배치 크기를 알아보자.
### 7.1.3 GPU 구조와 최적의 배치 크기
- 서빙이 효율적인지 판단하는 기준
    1. 비용
    2. 처리량 : 시간 당 처리한 요청수
    3. 지연 시간 : 하나의 토큰을 생성하는 데 걸리는 시간
    > 효율적인 서빙을 위해서는 같은 GPU로 처리량을 높이고 지연시간을 낮춰야 한다.
- GPU의 구조
    - 여러 SM으로 구성된다.
        - 연산을 수행하는 부분
        - **SRAM** : 계산할 값을 저장 (= L1 캐시, 공유 메모리)
    - **HBM** : 고대역폭 메모리, 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어려우므로 큰 데이터를 저장한다.
- 추론 수행 시, 배치 크기 만큼의 토큰을 한번에 생성한다.
    > 계산량 : 2 * P(모델 파라미터 메모리) * 배치크기 byte
- 연산 수행을 위해서, HBM에 있는 모델 파라미터를 SRAM으로 이동시켜야 한다.

> 즉, 연산에 걸리는 시간과 모델 파라미터를 이동시키는 시간 두 가지를 고려해야 하는데, 두 시간이 같을 때가 **최적의 배치 크기**가 된다.
- 메모리 바운드 : 최적의 배치 크기(B*)보다 배치 크기가 작은 경우, 모델 파라미터를 이동시키느라 연산이 멈춘다.
- 연산 바운드 : B*보다 배치 크기가 큰 경우, 연산에 오랜 시간이 걸려 지연 시간이 길어진다.

> 그렇다면 배치 크기를 키우는 방법은 무엇이 있을까?
- 모델의 용량을 줄이는 방법 (멀티 쿼리 어텐션, 그룹 쿼리 어텐션)
- KV 캐시의 용량을 줄이는 방법 (양자화, 지식 증류)
### 7.1.4 KV 캐시 메모리 줄이기
- 멀티 헤드 어텐션 : 트랜스포머 모델이 셀프 어텐션 연산을 수행하는 방식
    > 쿼리와 키 사이 다양한 관련성을 반영하고 성능이 높다.
    > 하지만, KV 캐시 사용량이 높아져 속도가 느려진다.
- 멀티 쿼리 어텐션 : 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유한다.
    > KV 캐시를 저장하는 데 적은 메모리를 사용하지만, 성능이 떨어짐
- 그룹 쿼리 어텐션 : 멀티 헤드 어텐션보다는 키와 값의 수를 줄이고, 멀테 쿼리 어텐션보다는 늘린 절충안
    > 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 사용량을 줄일 수 있다.

## 7.2 양자화로 모델 용량 줄이기
> 모델을 저장할 때 더 적은 메모리를 사용하도록 데이터 타입을 변환하는 **양자화**를 한다.
- 양자화 : 부동소수점 데이터를 더 적은 메모리를 사용하는 정수 형식으로 변환해 GPU를 효율적으로 사용하는 방법
> 양자화의 목표 : 더 적은 메모리를 사용하면서 최대한 원본 모델의 정보를 유지하는 것
> 모델의 크기가 커지며 파라미터 용량을 줄이려는 연구가 이어지고 있다.
- 양자화를 수행하는 시점에 따라 양자화를 분류할 수 있다.
    - 학습 후 양자화 : LLM은 학습에 많은 자원이 들어 주로 이 방식을 활용한다.
        > 비츠앤바이츠, GPTQ, AWQ
    - 학습 전 양자화
### 7.2.1 비츠앤바이츠
- 양자화 방식을 쉽게 사용할 수 있도록 제공하는 양자화 라이브러리
1. 8비트 행렬 연산
- 입력 값 중 크기가 큰 이상치가 포함된 열을 별도로 분리해 16비트 그대로 계산한다.
- 정상 범위에 있는 열을 양자화할 대 벡터 단위로 절대 최댓값을 찾고 그 값을 기준으로 양자화를 수행한다.
- 8비트로 양자화한 정상 값 벡터끼리 행렬 곱셈을 하고 이상치 벡터끼리 행렬 곱셈을 수행해 최종 결과를 산출한다.
2. 4비트 정규 분포 양자화 : 5.5장에서 살펴본 QLoRA 방식이다.

> 비츠앤바이츠 양자화의 경우, 양자화에 별도의 시간이 걸리지 않는다. 즉 모델을 불러오면서 바로 양자화가 가능하다.
### 7.2.2 GPTQ
- GPTQ : 양자화 이전의 모델에 입력을 넣었을 때와 양자화 이후의 모델에 입력을 넣었을 때 오차가 가장 작아지도록 양자화를 수행한다.
- 작은 데이터셋을 준비하고 그 데이터셋을 활용해 모델 연산을 수행하며 양자화 이전과 유사한 결과가 나오도록 모델을 업데이트한다.
- 양자화 진행 과정에서 현재 양자화를 수행하는 열을 기준으로 왼쪽은 이미 양자화를 수행 완료한 열이고 오른쪽은 다음에 양자화를 수행할 열이다.
- 데이터 입력 결과가 이전과 최대한 가까워지도록 양자화를 수행하지 않은 부분의 파라미터를 업데이트한다.
### 7.2.3 AWQ
> 모델의 성능을 유지한다는 것은 모델이 가지고 있는 정보를 최대한 손실 없이 변환하는 것이다.
- AWQ : 모든 파라미트가 동등하게 중요하지 않고, 특별히 중요한 파라미터의 정보를 유지하면 양자화를 수행하면서도 성능 저하를 막을 수 있는 방법이다.
- 어떤 파라미터가 중요한지 판단할 수 있는 방법
    1. 모델 파라미터의 값이 큰 경우
    2. 입력 데이터의 활성화 값이 큰 경우
- 중요한 모델 파라미터를 찾고 해당 파라미터는 기존 모델의 데이터 타입으로 유지하고 나머지를 양자화한다.
> 활성화 값을 기준으로 중요한 1% 파라미터의 정보만 지키면 모델의 성능이 유지된다.
- 모델 파라미터에 서로 다른 데이터 타입이 섞여 있는 경우 연산이 느리고 효율성이 떨어지는 문제가 발생한다.
    - 중요한 파라미터에만 1보다 큰 값을 곱하는 방식으로 해결
        > 곱해주는 값을 스케일러라고 부른다.
        > 스케일러 s가 2일 때까지는 성능이 향상되지만 넘어가는 경우 성능이 다시 하락한다.
        - 나머지 모델 파라미터의 정보에는 영향을 주지 않으면서 중요한 파라미터의 정보 소실을 막을 수 있다.
- 양자화에 많은 시간이 걸리지 않고 기존 모델의 성능을 거의 유지할 수 있다.
## 7.3 지식 증류 활용하기
> 선생 모델의 생성결과를 활용해 더 작고 효율적인 학생 모델을 학습하는 **지식 증류**를 알아본다.
- 선생 모델을 활용해 완전히 새로운 학습 데이터셋을 대규모로 구축하거나 데이터셋 구축에 사람의 판단이 필요한 부분을 선생 모델이 수행한다.
    - 제퍼 모델
        > 지시 데이터셋의 구축과 선호 데이터셋의 구축에 모두 LLM을 사용하였다.
    - 파이 모델
        > 학습 데이터로 사용할 코드를 선택하는 데 GPT-3.5를 사용하였다.

# 8. sLLM 서빙하기
> 성능 하락 없이 추론 속도를 높이는 다양한 방법들을 살펴보자.
## 8.1 효율적인 배치 전략
> 언어 모델의 특성 상 한 번에 하나의 토큰을 생성하고, 입력에 따라 몇 개의 토큰을 생성할지 예측하기 어려우므로 배치전략을 세우는 것이 중요하다.
### 8.1.1 일반 배치(정적 배치)
- **일반 배치(정적 배치)**: 입력 데이터를 배치 처리할 때, 한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식

- 두 가지 문제점
    1. 생성이 종료된 이후 다른 데이터의 추론을 기다리느라 결과를 반환하지 못하고 대기하는 입력이 존재한다.
    2. 생성이 일찍 종료되는 문장이 있으면 배치 크기가 작아져 GPU를 효율적으로 사용하지 못하게 된다.
### 8.1.2 동적 배치
- **동적 배치**: 비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략
- 온라인 서빙에서 배치 크기를 키워 처리량을 높일 수 있지만, 생성하는 토큰 길이 차이로 인해 처리하는 배치 크기가 점차 줄어 여전히 GPU를 비효율적으로 사용한다.
### 8.1.3 연속 배치
- **연속 배치**: 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가한다.
- 모델의 추론 과정에서 사전 연산과 디코딩은 처리 방식이 다르므로 한 번의 토큰 생성이 끝날 때마다 새로운 문장을 배치에 추가하지 않기도 한다.

## 8.2 효율적인 트랜스포머 연산
- 셀프 어텐션 연산 : 쿼리와 키 벡터 사이의 관련도를 계산해 새롭게 토큰 임베딩을 조정하므로 성능은 높지만 많은 연산 필요
    > **플래시어텐션**으로 메모리 사용량을 줄인다.
- 절대적 위치 인코딩 : 토큰의 위치 정보를 추가할 때 토큰의 위치에 따라 동일한 임베딩을 추가한다. 긴 입력 데이터에는 성능이 저하된다.
    > 토큰의 상대적 위치를 추가하는 **상대적 위치 인코딩**으로 개선한다.
### 8.2.1 플래시어텐션
- 플래시어텐션 : 긴 시퀀스를 처리하는 데 어려움이 있던 어텐션 연산 과정을 변경해 학습 과정에 필요한 메모리를 시퀀스 길이에 비례하도록 개선
    > 긴 시퀀스는 다양한 형태의 입력을 처리하는 데 중요하다.
- 트랜스포머 연산은 쿼리와 키 벡터를 곱하는 데에 많은 메모리를 사용하는데, 특히 **마스크, 소프트맥스, 드롭아웃 과정**까지의 행렬 크기가 매우 크다.
    > 따라서 그 과정에서 연산은 더 오래 걸린다.
    > GPU에서 메모리를 읽고 쓰는 데 오랜 시간이 걸리기 때문이다.
> GPU의 구조는 **SRAM**(데이터 이동 속도는 빠르나 메모리 크기가 작음), **HBM**(데이터 이동 속도 느림)가 있다.
- 플래시어텐션에서는 데이터 이동 속도가 느린 고대역폭 메모리에 큰 어텐션 행렬을 쓰고 읽으면서 걸리는 시간을 줄인다.
    1. 블록 단위로 어텐션 연산을 수행하며 전체 어텐션 행렬을 읽거나 쓰지 않는다.
    2. HBM이 아닌 SRAM에 데이터를 읽고 쓰면서 연산을 빠르게 수행한다.
- 역전파 과정에서 순전파의 전체 행렬의 값이 필요한 경우, 역전파 과정에서 다시 순전파를 계산하는 방식으로 작동한다.
> 계산량은 증가하지만, 실행시간은 감소한다.
### 8.2.2 플래시어텐션 2
- 플래시 어텐션을 개선해 2배 정도 속도를 향상했다.
1. 행렬 곱셈이 아닌 연산 줄이기
    > GPU가 행렬 연산에 최적화돼 있기 때문이다.
2. 시퀀스 길이 방향의 병렬화 추가
    - 스레드 : GPU의 가장 작은 계산 단위, 하나의 스트리밍 프로세서에서 처리된다.
    - 워프 : 32개의 스레드를 하나의 명령으로 실행하는 단위
    - 스레드 블록 : 4-8개의 워프를 모은 단위, 각각의 SM에 배정된다.
    > 배치 크기가 작거나 어텐션 헤드 수가 작은 경우 SM을 충분히 활용하지 못 하기 때문이다.
### 8.2.3 상대적 위치 인코딩
- 사인파 위치 인코딩: 사인과 코사인을 사용해 위치 인코딩을 더하는 방식
- 위치 인코딩을 학습할 수 있는 파라미터로 두고 모델 학습 과정에서 함께 학습
> 두 방법 모두 절대적 위치 인코딩으로, 학습 데이터보다 긴 입력이 들어올 경우 성능이 떨어진다.

- 상대적 위치 인코딩: 토큰과 토큰 사이의 상대적인 위치 정보를 추가한다.
1. RoPE: 각각의 토큰 임베딩을 토큰 위치에 따라 회전시켜 토큰 사이의 위치 정보가 두 임베딩 사이의 각도로 반영된다.
2. ALiBi: 쿼리와 키 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식, 학습과 추론에 별도로 처리 시간을 추가하지 않는다.

## 8.3 효율적인 추론 전략
### 8.3.1 커널 퓨전
- GPU에서 연산은 커널 단위로 이뤄진다.
- 연산을 수행하기 위해, 전후 추가적인 작업을 위한 오버헤드가 발생한다.
- **커널 퓨젼**: 반복적으로 수행하는 연산에 대해 각 커널 단위로 분리해 수행하는 것보다 연산을 하나로 묶어 오버헤드를 줄이는 것
    > ex. 플래시어텐션
### 8.3.2 페이지어텐션
> KV 캐시는 중복 연산을 줄여 연산 시간을 줄이지만, 많은 GPU 메모리를 사용한다.
- 페이지 어텐션을 사용하면 KV 캐시를 효율적으로 관리하고, 배치 크기를 키울 수 있다.
> 기존 KV 캐시는 몇 개의 토큰을 생성할 지 알지 못하므로 최대 생성 토큰 수만큼 미리 메모리를 예약한다.
- 논리적 메모리와 물리적 메모리를 연결하는 **블록 테이블**을 관리해 실제로 물리적으로 연속된 메모리를 사용하지 않으면서 논리적 메모리에서는 서로 연속적이도록 한다.
- 큰 메모리를 예약하지 않고 최대 블록크기만큼의 메모리만 배정한다.
- 병렬 샘플링에서 각기 다른 토큰을 생성한 경우 **참조 카운트**를 활용한다.
    > 참조 카운트가 1보다 클 경우 새로운 물리적 블록에 기존 토큰을 복사해 동일한 토큰은 최대한 메모리를 공유하고 새롭게 생성하는 토큰만 서로 분리해 메모리를 효율적으로 활용한다.
### 8.3.3 추측 디코딩
- 추측 디코딩: 쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식
- 드래프트 모델 : 빠르게 추론이 가능하므로 k개의 토큰을 먼저 생성한다.
- 타깃 모델: 드래프트 모델이 생성한 토큰이 타깃 모델이 추론했다면 생성했을 결과와 동일한지 계산해 승인 혹은 거절한다.
> 타깃 모델 한 번의 추론 시간과 유사한 시간동안 훨씬 많은 토큰을 생성할 수 있어 추론 속도가 빨라진다.

## 8.4 실습: LLM 서빙 프레임워크
> LLM 서빙 프레임워크 vLLM을 사용한다.
### 8.4.1 오프라인 서빙
- 오프라인 서빙: 정해진 입력 데이터에 대해 배치 추론을 수행하는 것
> 모델에 입력할 데이터가 이미 정해져 있어 배치 처리를 통해 처리량을 높일 수 있다.
> vLLM을 사용하는 경우, 허깅페이스 배치 추론 대비 추론 시간이 줄어든다.
### 8.4.2 온라인 서빙
- 온라인 서빙: 사용자의 요청이 올 때 모델 추론을 수행한다는 점에서 차이가 있다.
> vLLM은 온라인 서빙에 사용할 수 있는 API 서버를 제공한다.

# 3부 | LLM을 활용한 실전 애플리케이션 개발
# 9. LLM 애플리케이션 개발하기
## 9.1 검색 증강 생성(RAG)
> LLM을 학습시킬 때 상당한 시간과 비용이 들고, LLM의 답변은 부정확한 정보를 지어내는 환각 현상이 존재한다.
- RAG 기법: LLM에게 단순히 질문이나 요청만 전달하고 생성하는 것이 아니라 답변에 필요한 충분한 정보와 맥락을 제공하고 답변하도록 하는 방법
### 9.1.1 데이터 저장
- 데이터 소스: 비정형 데이터가 저장된 데이터 저장소
- 임베딩 모델: 데이터 소스를 임베딩 벡터로 변환
- 벡터 데이터베이스: 벡터 사이 거리를 기준으로 검색하는 데이터베이스
    > 특정 검색 쿼리로 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리도 벡터로 변환해 벡터 데이터베이스에서 위치를 찾고 쿼리 임베딩과 가가운 벡터를 찾는다.
### 9.1.2 프롬프트에 검색 결과 통합
> 사용자의 요청을 임베딩 모델을 통해 임베딩 벡터로 변환하고, 벡터 데이터베이스에서 검색 임베딩 벡터와 가까운 벡터를 찾아 검색 결과를 반환받는다. 검색 결과는 프롬프트 모듈에서 사용자의 요청과 하나로 통합된다.
1. 질문과 관련된 맥락 정보를 임베딩 모델을 통해 문서 임베딩으로 변환 후 벡터 데이터베이스에 저장한다.
2. 사용자의 질문을 쿼리 임베딩으로 변환해 검색 후 두 임베딩이 가까워 검색 결과로 반환되면, 자동으로 관련 정보를 찾아 프롬프트에 추가한다.
### 9.1.3 실습: 라마인덱스로 RAG 구현하기
- 라마 인덱스: 대표적인 LLM 오케스트레이션 라이브러리
> 라마 인덱스를 사용하면 간단한 코드만으로 RAG를 수행할 수 있다.
1. VectorIndexRetriever 클래스로 벡터 데이터베이스에서 검색하는 retriever를 만든다.
2. 검색 결과를 사용자의 요청과 통합하기 위해 get_response_synthesizer() 함수를 사용해 response_synthesizer를 만든다.
3. RetrieverQueryEngine 클래스에 앞서 생성한 retriever와 response_synthesizer를 전달해 RAG를 한 번에 수행하는 query_engine을 생성한다.

## 9.2 LLM 캐시
- LLM 캐시: LLM 추론을 수행할 때 사용자의 요청과 생성 결과를 기록하고 이후에 동일하거나 비슷한 요청이 들어오면 새롭게 텍스트를 생성하지 않고 이전의 생성 결과를 가져와 바로 응답함으로써 LLM 생성 요청을 줄인다.
### 9.2.1 LLM 캐시 작동 원리
- LLM 캐시의 종류
1. 일치 캐시: 요청이 완전히 일치하는 경우 저장된 응답을 반환한다.
    > 파이썬의 딕셔너리와 같은 자료구조로 구현한다.
2. 유사 검색 캐시: 유사한 요청이 있었는지 확인한다.
    > 임베딩 벡터를 비교해 구현한다.
### 9.2.2 실습: OpenAI API 캐시 구현
> 파이썬 딕셔너리와 오픈소스 벡터 데이터베이스 크로마를 사용한다.

- 파이썬 딕셔너리를 활용한 일치 캐시 구현
    1. 파이썬 딕셔너리로 LLM 캐시를 생성한다.
    2. 입력으로 받은 prompt가 self.cache에 없다면 새롭게 텍스트를 생성한다.
    3. 생성 후 결과를 활용할 수 있도록 self.cache에 저장한다.
    4. 캐시에 동일한 프롬프트가 있다면 캐시에 저장된 응답을 반환한다.
- 유사 검색 캐시 구현
    > self.semantic_cache를 추가한다.
    1. generate 메서드는 일치 캐시를 확인한 후 없다면 유사 검색 캐시를 확인한다.
    2. 크로마 벡터 데이터베이스의 query 메서드에 query_texts를 입력하면 벡터 데이터베이스에 등록된 임베딩 모델을 사용해 텍스트를 임베딩 벡터로 변환하고 검색을 수행한다.
    3. 검색 결과가 존재하는지, 검색 결과 문서 사이의 거리가 충분히 가까운지 확인하고 조건을 만족하면 반환한다.

## 9.3 데이터 검증
### 9.3.1 데이터 검증 방식
- 데이터 검증: 벡터 검색 결과나 LLM 생성 결과에 포함되지 않아야 하는 데이터를 필터링하고 답변을 피해야 하는 요청을 선별하는 것
1. 규칙 기반: 문자열 매칭이나 정규 표현식을 활용해 데이터를 확인하는 방식
2. 분류 또는 회귀 모델: 긍부정 분류 모델을 만들어 부정 스코어를 통해 다시 생성하도록 하는 방식
3. 임베딩 유사도 기반: 특정 내용과 관련된 텍스트를 임베딩 벡터로 만들어 유사한 경우 답변을 피하도록 하는 방식
4. LLM 활용
### 9.3.2 데이터 검증 실습
> NeMo-Guardrails 라이브러리를 활용한다.
- LLM이 정해진 방식으로 응답하도록 하는 방식
    1. 사용자의 요청에 따른 상황을 정의한다.
    2. 그에 따른 응답을 정의한다.
    3. 사용자의 요청과 봇의 응답을 하나로 묶어 어떤 요청에 어떤 응답을 반환할지 정의한다.
- LLM이 정해진 방식으로 응답하지 않도록 하는 방식
    > 새로운 요청과 응답 정의를 생성하는 것이다.
- LLM에게 직접 입출력이 특정 조건을 만족하는지 확인하는 방식

## 9.4 데이터 로깅
- 데이터 로깅: 사용자의 입력과 LLM이 생성한 출력을 기록하는 것
> W&B를 예시로 살펴본다.
### 9.4.1 OpenAI API 로깅
1. 시스템 프롬프트와 사용자의 질문을 전달해 텍스트 생성
2. Trace 클래스에 입력, 생성 결과, 생성 성공 여부 등 정보들을 입력한다.
3. log 메서드를 사용해 로그를 W&B에 전달한다.
### 9.4.2 라마인덱스 로깅
- 라마인덱스에서 제공하는 set_global_handler 함수로 라마인덱스 내부에서 W&B에 로그를 전송하도록 설정한다.

# 10. 임베딩 모델로 데이터 의미 압축하기
## 10.1 텍스트 임베딩 이해하기
- 텍스트 임베딩 (문장 임베딩): 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식
### 10.1.1 문장 임베딩 방식의 장점
> 단어나 문장 사이의 관계를 계산할 수 있다.
### 10.1.2 원핫 인코딩
> 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지하지만, 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다.
### 10.1.3 백오브워즈
> 비슷한 단어가 많이 나오면 비슷한 문장 또는 문서라는 가정을 활용해 문서를 숫자로 변환한다.
> 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는 데 크게 도움이 되지 않는 경우가 존재한다.
### 10.1.4 TF-IDF
> 어느 문서에나 나오는 단어에 관한 문제, 즉 백오브워즈의 문제를 보완한 방식으로, **많은 문서에 등장하는 단어의 중요도를 작게 만든다.**
### 10.1.5 워드투벡
> 단어가 함께 등장하는 빈도 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법이다.
- CBOW: 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식
- 스킵그램: 가운데 단어 정보롤 주변의 단어를 예측하는 방식

## 10.2 문장 임베딩 방식
### 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법
- 바이 인코더 방식: 각각의 문장을 독립적으로 BERT 모델에 입력으로 넣고, 모델의 출력 결과인 문장 임베딩 벡터 사이의 유사도를 구한다.
    > 각 문장의 독립적인 임베딩을 결과로 반환하기 때문에 유사도를 계산하고 싶은 문장이 바뀌어도 추가 연산이 필요없다.
- 교차 인코더 방식: 두 문장을 함께 BERT 모델에 입력으로 넣고, 모델이 직접 두 문장 사이의 관계를 0-1 사이 값으로 출력한다.
    > 계산량이 많으나, 두 텍스트의 유사도를 정확히 계산 가능
### 10.2.2 바이 인코더 모델 구조
- BERT 모델은 입력 토큰마다 출력 임베딩을 생성한다.
- 풀링 층을 사용해 문장을 대표하는 1개의 임베딩으로 통합한다.
> Sentence-Transformer 라이브러리를 사용하면 쉽게 바이 인코더를 사용할 수 있다.
- 풀링 모드: 언어 모델이 출력한 결과 임베딩을 고정된 크기의 문장 임베딩으로 통합할 때 통합하는 방식
    1. 클래스 모드: 첫번째 토큰의 출력 임베딩을 문장 임베딩으로 사용
    2. 평균 모드: 모든 입력 토큰의 출력 임베딩을 평균한 값을 문장 임베딩으로 사용
    3. 최대 모드: 모든 입력 토큰의 출력 임베딩에서 문장 길이 방향에서 최댓값을 찾아 문장 임베딩으로 사용
### 10.2.3 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해 보기
- 텍스트 모델 예시
    1. 한국어 문장 임베딩 모델을 불러온다.
    2. 각 문장을 임베딩 모델에 입력해 encode 메서드를 통해 문장 임베딩으로 변환한다.
    3. util.cos_sim 함수로 각 문장 사이 유사도를 계산한다.
- 이미지 모델 예시
    1. clip 모델을 사용한다.
    2. encode 메서드를 사용해 이미지를 이미지 임베딩으로, 텍스트를 텍스트 임베딩으로 변환한다.
    3. 이미지 임베딩과 텍스트 임베딩 사이의 유사도를 계산한다.
### 10.2.4 오픈소스와 상업용 임베딩 모델 비교하기
- 오픈소스 임베딩 모델: Sentence-Transformer 라이브러리
    - 사용자 특화 임베딩 모델을 만들 수 있다.
- 상업용 임베딩 모델
    - 대량의 데이터로 학습되어 성능이 뛰어나다.
    - 비용이 적게 든다.

## 10.3 실습: 의미 검색 구현하기
> Sentence-Transformer 라이브러리와 faiss 라이브러리를 활용한다.
- 의미 검색: 단순히 키워드 매칭을 통한 검색이 아니라 밀집 임베딩을 이용해 문장이나 문서의 의미를 고려한 검색을 수행하는 것
### 10.3.1 의미 검색 구현하기
> KLUE 데이터 셋의 MRC 데이터를 활용한다.
- 검색 쿼리 문장을 문장 임베딩 모델의 encode 메서드를 사용해 문장 임베딩으로 변환한다.
- 인덱스의 search 메서드로 검색한다.

> 의미 검색은 키워드가 동일하지 않아도 의미가 유사하면 찾을 수 있다는 장점이 있지만, 관련성이 떨어지는 검색 결과가 나오기도 한다는 단점이 있다.
### 10.3.2 라마인덱스에서 Sentence-Transformers 모델 사용하기
> 라마인덱스는 내부적으로 임베딩 API를 호출해 변환을 수행하는데, Sentence-Transformer 라이브러리의 임베딩 모델을 통합할 수 있는 기능을 지원한다.

## 10.4 검색 방식을 조합해 성능 높이기
> 의미 검색과 키워드 검색의 단점을 보완하기 위해 두 방식을 조합한 하이브리드 검색이 있다.
### 10.4.1 키워드 검색 방식: BM25
- BM25: 통계 기반 스코어링 방법으로, TF-IDF에 문서의 길이에 대한 가중치를 추가한 알고리즘
    - IDF 부분: 여러 문서에 등장하는 단어는 덜 중요한 단어이므로 값이 작아진다.
    - TF 부분: 특정 문서에 토큰의 등장이 아무리 커져도 값이 무한정 커지지 않아 단어 빈도에 대한 포화 효과를 나타낸다.
    - 문서 길이 가중치: 짧은 문서에 토큰이 등장한 경우 더 중요도를 높게 판단한다.
### 10.4.2 상호 순위 조합 이해하기
- 상호 순위 조합: 통계 기반 점수와 임베딩 유사도 점수 각각에서의 순위를 활용해 점수를 산출하는 방식

## 10.5 실습: 하이브리드 검색 구현하기
### 10.5.1 BM25 구현하기
- BM25 클래스 구현
    1. 각 토큰이 몇 개의 문서에 등장하는지 집계한다.
    2. 각 토큰이 각 문서 내에서 몇 번 등장하는지 집계한다.
    3. 검색하려는 쿼리와 각 문서 사이의 점수를 계산한다.
    4. 쿼리와 문서 사이의 점수가 가장 높은 k개 문서의 인덱스와 점수를 반환한다.
### 10.5.2 상호 순위 조합 구현하기
- 상호 순위 조합 함수 구현
    1. 여러 검색 방식에서 정해진 유사 문서의 인덱스 리스트를 입력으로 받는다.
    2. 각각의 순위 리스트를 순회하며 각각 문서 인덱스에 점수를 더한다.
    3. 점수를 종합한 딕셔너리를 정렬해 반환한다.
### 10.5.3 하이브리드 검색 구현하기
- 하이브리드 검색 함수 구현
    1. 의미 검색에서 반복적으로 수행한 검색 쿼리 문장 임베딩 변환과 인덱스 검색 부분을 한번에 수행하도록 정의한다.
    2. 검색 쿼리 문장과 상호 순위 조합에 사용할 파라미터 k를 입력받는다.
    3. 입력받은 쿼리 문장으로 의미 검색과 키워드 검색을 수행한다.
    4. 상호 순위 조합을 사용해 결과를 반환한다.

# 11. 자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기
## 11.1 검색 성능을 높이기 위한 두 가지 방법
## 11.2 언어 모델을 임베딩 모델로 만들기
## 11.3 임베딩 모델 미세 조정하기
## 11.4 검색 품질을 높이는 순위 재정렬
## 11.5 바이 인코더와 교차 인코더로 개선된 RAG 구현하기